{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient\n",
    "\n",
    "This code is an example using Policy Gradient Method. The code is from [this website](https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63).\n",
    "The example uses the gym [CartPole system](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py).\n",
    "Here we use [pyTorch](https://pytorch.org/) rather than [tensorFlow](https://www.tensorflow.org/). \n",
    "- `torch.nn` is used to inherit and use the neural network class.\n",
    "- `torch.nn.functional` is used to apply ReLU or softmax.\n",
    "- `torch.optim` is used to apply [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/), a famous method for stochastic gradient descent.\n",
    "- `torch.autograd`'s `Variable` is known to be deprecated [[REF]](https://stackoverflow.com/questions/57580202/whats-the-purpose-of-torch-autograd-variable), but since the original code uses it we will leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np  \n",
    "\n",
    "import torch  \n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "from   torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot   as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Policy Network\n",
    "\n",
    "The policy neural network has three layers:\n",
    "- [Layer 1] Denoted as $l_1$ with $n_s$ Neurons. The input is the state of the current cart-and-pole.\n",
    "- [Layer 2] Denoted as $l_2$ with $n_{h}$ Neurons, where subscript $h$ stands for \"hidden\".\n",
    "- [Layer 3] Denoted as $l_3$ with $n_a$ Neurons\n",
    "\n",
    "From $l_1$ to $l_2$ and $l_2$ to $l_3$, the network uses a linear combination, i.e., using $l_1$, $l_2$ and $l_3$ as an array of values of the corresponding neurons:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    l_2 &= W_{12} \\cdot l_1 + b_{12} \\\\\n",
    "    l_3 &= W_{23} \\cdot l_2 + b_{23}\n",
    "\\end{align*}    \n",
    "$$\n",
    "where $W$ and $b$ are weight matrix and bias array, respectively.\n",
    "Note that the values are additionally fed into a nonlinear function.\n",
    "$l_2$ is fed into a ReLU (Rectified Linear Unit) function, and $l_3$ is fed into a softmax function to make it as a probability distribution function (i.e., all the values are non-negative and it sums to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyNetwork( nn.Module ):\n",
    "    def __init__( self, num_inputs, num_actions, hidden_size, learning_rate = 3e-4):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.linear1     = nn.Linear( num_inputs, hidden_size  )\n",
    "        self.linear2     = nn.Linear( hidden_size, num_actions )\n",
    "        self.optimizer   = optim.Adam( self.parameters(), lr = learning_rate )\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu( self.linear1( state ) )\n",
    "        x = F.softmax( self.linear2( x ), dim = 1 )\n",
    "        return x \n",
    "    \n",
    "    def get_action( self, state ):\n",
    "        state = torch.from_numpy( state ).float().unsqueeze( 0 )\n",
    "\n",
    "        # Forward the neural network and return the prbability distribution function\n",
    "        probs = self.forward( Variable( state ) )\n",
    "\n",
    "        # Choosing the action based on the output policy\n",
    "        highest_prob_action = np.random.choice( self.num_actions, p = np.squeeze( probs.detach( ).numpy( ) ) )\n",
    "\n",
    "        # The log-value of the probability just for the sake of policy gradient\n",
    "        log_prob            = torch.log( probs.squeeze( 0 )[ highest_prob_action ] )\n",
    "        return highest_prob_action, log_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Goal of Policy Gradient Method!\n",
    "\n",
    "Once we parameterize the policy with a neural network $\\pi_{\\theta}(a|s)$, we need an objective function to conduct the gradient ascent (recall that we are optimizing rewards not costs!).\n",
    "Given a trajectory $\\tau$ of Markov Decision Process with length $T$ (starts from 0, hence $T+1$ states):\n",
    "$$\n",
    "    \\tau: s_0, a_0, r_1, s_1, a_1, \\cdots, s_{T-1}, a_{T-1}, r_T, s_T\n",
    "$$\n",
    "\n",
    "The objective function $J(\\theta)$ that we are planning to maximize is the expected value of the reward from a given trajectory:\n",
    "$$\n",
    "    J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Big[R(\\tau)\\Big], \\;\\; \\text{where} \\;\\;     R(\\tau) =\\sum_{i=1}^{T}r_i\n",
    "$$\n",
    "Note that the $R(\\cdot)$ function is simply the sum of all rewards of a given trajectory, and subscript $\\tau \\sim \\pi_{\\theta}$ means that the trajectory is generated following policy $\\pi_{\\theta}$.\n",
    "The gradient of the objective function is simply:\n",
    "$$\n",
    "    \\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta}\\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Big[R(\\tau)\\Big] = \\nabla_{\\theta}\\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\bigg[ \\sum_{i=1}^{T}r_i \\bigg]\n",
    "$$\n",
    "Since it is the expectation over all possible trajectories, we use a simple trick to simplify the gradient:\n",
    "$$\n",
    "    \\nabla_{\\theta}\\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\bigg[ R(\\tau) \\bigg] = \\nabla_{\\theta}\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) = \\sum_{\\tau} R(\\tau) \\nabla_{\\theta} p_{\\theta}(\\tau) = \\sum_{\\tau} R(\\tau) \\frac{\\nabla_{\\theta} p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)}p_{\\theta}(\\tau) = \\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla_{\\theta}\\log{p_{\\theta}(\\tau)} = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Big[ R(\\tau)\\nabla_{\\theta}\\log{p_{\\theta}(\\tau)} \\Big]\n",
    "$$\n",
    "Meaning, the gradient with respect to parameter $\\theta$ goes into the expectation operator and we are taking the log of the probability of the trajectory. \n",
    "Our next step is to conduct a calculation of $p_{\\theta}(\\tau)$, $\\log{p_{\\theta}(\\tau)}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    p_{\\theta}(\\tau)       & = \\text{Pr}\\Big[ S_0 = s_0, A_0 = a_0, \\cdots, S_{T-1}=s_{T-1}, A_{T-1}=a_{T-1} \\Big] \\\\\n",
    "                           & =  p(s_0)\\cdot\\pi_{\\theta}(a_0|s_0)\\cdot p(r_1, s_1 | a_0, s_0) \\cdot \\pi_{\\theta}(a_1 | s_1) \\cdot p(r_2, s_2 | a_1, s_1) \\cdots \\pi_{\\theta}(a_{T-1}|s_{T-1})p(r_T, s_T | a_{T-1}, s_{T-1}) \\\\\n",
    "                           \\\\\n",
    "    \\log{p_{\\theta}(\\tau)} & = \\log{p(s_0)} + \\log{\\pi_{\\theta}(a_0|s_0)} + \\log{p(r_1, s_1 | a_0, s_0)} + \\cdots + \\log{\\pi_{\\theta}(a_{T-1}|s_{T-1})} + \\log{p(r_T, s_T | a_{T-1}, s_{T-1})}  \\\\\n",
    "    \\\\\n",
    "    \\nabla_{\\theta}\\log{p_{\\theta}(\\tau)} & = \\nabla_{\\theta} \\bigg( \\sum_{i=0}^{T-1} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg) \n",
    "\\end{align*}    \n",
    "$$\n",
    "The great point about policy gradients is the fact that only the policy terms $\\pi_{\\theta}(a_i|s_i)$ survive for the gradient. Summarizing, \n",
    "$$\n",
    "    \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Big[ R(\\tau)\\nabla_{\\theta}\\log{p_{\\theta}(\\tau)} \\Big] \n",
    "    = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ R(\\tau)\\nabla_{\\theta} \\bigg( \\sum_{i=0}^{T-1} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg)  \\Bigg] \n",
    "    = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\bigg( \\sum_{i=1}^{T}r_i \\bigg) \\nabla_{\\theta} \\bigg( \\sum_{i=0}^{T-1} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg)  \\Bigg]\n",
    "$$\n",
    "And using some math tricks for the simplification [[REF]](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/):\n",
    "$$\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\bigg( \\sum_{i=1}^{T}r_i \\bigg) \\nabla_{\\theta} \\bigg( \\sum_{i=0}^{T-1} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg)  \\Bigg] = \\cdots = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\sum_{i=0}^{T-1} \\nabla_{\\theta} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg(\\sum_{j=i+1}^{T}r_j\\bigg) \\Bigg]\n",
    "$$\n",
    "Note that the last term is simply the undiscounted sum of rewards, i.e., $G_t$ with $\\gamma = 1$. Meaning, we can simply generalize this equation for cases with $\\gamma \\in [0,1)$.\n",
    "\n",
    "# Policy update via Gradient\n",
    "\n",
    "Since we parameterized the policy as a neural network, the map from state to action is now parameterized and therefore the gradient can be calculated. We denote the parameterized policy as $\\pi_{\\theta}(a|s)$, where $\\theta$ stands for the weights and biases of the 3-layer neural network.\n",
    "\n",
    "\n",
    " \n",
    "The policy gradient formula is as follows:\n",
    "$$\n",
    "    \\Delta_\\theta J(\\theta) = \\sum_{t=0}^{T-1} \\Delta_{\\theta} \\log \\pi_{\\theta} (a_t | s_t) G_t\n",
    "$$\n",
    "where $G_t$ is a discounted reward:\n",
    "$$\n",
    "    G_t = \\mathbb{E}[R_{t+1}+\\gamma R_{t+2}+\\cdots | S_t = s]\n",
    "$$\n",
    "This method is called the REINFORCE algorithm, which is a Monte-Carlo variant of policy gradients. Note that Monte-Carlo methods is simply a hard way of saying \"methods using random samples\".\n",
    "The Monte-Carlo method is required since we cannot calculate actual \"expectations\" in real life $\\mathbb{E}[\\cdot]$. We need to somehow take samples and average it to calculate the expectations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_policy( policy_network, rewards, log_probs ):\n",
    "\n",
    "    # This is the discount rate, it should be smaller than 1 for convergence. \n",
    "    GAMMA = 0.9\n",
    "\n",
    "    discounted_rewards = [ ]\n",
    "\n",
    "    # Given the trajectory (or arrays) of rewards, we calculate the discounted reward. \n",
    "    for t in range( len( rewards ) ) :\n",
    "\n",
    "        # Initialization of Gt\n",
    "        Gt = 0 \n",
    "\n",
    "        # The power for the calculation\n",
    "        pw = 0\n",
    "        \n",
    "        for r in rewards[ t: ] :\n",
    "            Gt += GAMMA ** pw * r\n",
    "            pw += + 1\n",
    "        discounted_rewards.append( Gt )\n",
    "        \n",
    "    discounted_rewards = torch.tensor( discounted_rewards )\n",
    "    discounted_rewards = ( discounted_rewards - discounted_rewards.mean( ) ) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
    "\n",
    "    policy_gradient = [ ]\n",
    "\n",
    "    for log_prob, Gt in zip( log_probs, discounted_rewards ):\n",
    "        policy_gradient.append( -log_prob * Gt )\n",
    "    \n",
    "    # Initialization of the gradient value\n",
    "    policy_network.optimizer.zero_grad( )\n",
    "    \n",
    "    policy_gradient = torch.stack( policy_gradient ).sum( )\n",
    "\n",
    "    # Backpropagation (or simply, chain rule!)\n",
    "    policy_gradient.backward( )\n",
    "    policy_network.optimizer.step()        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A bit more Explanation\n",
    "\n",
    "### Part 1 - The discount rate\n",
    "In the code, `GAMMA` variable is a discount rate $\\gamma$. Recall that $G_t$ is:\n",
    "$$\n",
    "    G_t = \\mathbb{E}[R_{t+1}+\\gamma R_{t+2}+\\cdots | S_t = s]\n",
    "$$ \n",
    "Choosing $\\gamma \\in [0, 1)$ is necessary for the iteration to converge. The convergence proof uses \"contraction\" of mapping, and $\\gamma$ being smaller than 1 is crucial for the proof. For people who are interested, this simplified proof for Q-learning might be useful [[REF]](http://users.isr.ist.utl.pt/~mtjspaan/readingGroup/ProofQlearning.pdf).\n",
    "\n",
    "\n",
    "\n",
    "### Part 2 - Trajectory \n",
    "For each trial, we get a sequence of:\n",
    "$$\n",
    "    S_0, A_0, R_1, S_1, A_1, \\cdots, S_{T-1}, A_{T-1}, R_T, S_T\n",
    "$$\n",
    "where $T$ is the final time-step of the simulation. From this, we iteratively calculate $G_t$ for $t\\in \\{ 1,2, \\cdots, T\\}$. \n",
    "$$\n",
    "    G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t-1} R_{T}\n",
    "$$\n",
    "Note that the expectation operator is dropped off since we are approximating it with Monte-Carlo Method.\n",
    "\n",
    "### Part 3 - Normalization of $G_t$?\n",
    "This is the (personally) most confusing part, but I believe it is due to some practical reason. \n",
    "```\n",
    "    discounted_rewards = ( discounted_rewards - discounted_rewards.mean( ) ) / (discounted_rewards.std() + 1e-9) # normalize discounted rewards\n",
    "```\n",
    "[This post](https://datascience.stackexchange.com/questions/20098/why-do-we-normalize-the-discounted-rewards-when-doing-policy-gradient-reinforcem) will be helpful. \n",
    "The bottom line is to simply stabilize the learning process by normalization, which keeps the $G_t$ value in some reasonable value range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Cart-and-Pole System\n",
    "\n",
    "The "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total reward: 9.0, average_reward: 9.0, length: 8\n",
      "episode: 1, total reward: 29.0, average_reward: 19.0, length: 28\n",
      "episode: 2, total reward: 19.0, average_reward: 19.0, length: 18\n",
      "episode: 3, total reward: 26.0, average_reward: 20.75, length: 25\n",
      "episode: 4, total reward: 16.0, average_reward: 19.8, length: 15\n",
      "episode: 5, total reward: 32.0, average_reward: 21.833, length: 31\n",
      "episode: 6, total reward: 17.0, average_reward: 21.143, length: 16\n",
      "episode: 7, total reward: 16.0, average_reward: 20.5, length: 15\n",
      "episode: 8, total reward: 21.0, average_reward: 20.556, length: 20\n",
      "episode: 9, total reward: 50.0, average_reward: 23.5, length: 49\n",
      "episode: 10, total reward: 34.0, average_reward: 26.0, length: 33\n",
      "episode: 11, total reward: 25.0, average_reward: 25.6, length: 24\n",
      "episode: 12, total reward: 23.0, average_reward: 26.0, length: 22\n",
      "episode: 13, total reward: 30.0, average_reward: 26.4, length: 29\n",
      "episode: 14, total reward: 16.0, average_reward: 26.4, length: 15\n",
      "episode: 15, total reward: 31.0, average_reward: 26.3, length: 30\n",
      "episode: 16, total reward: 9.0, average_reward: 25.5, length: 8\n",
      "episode: 17, total reward: 29.0, average_reward: 26.8, length: 28\n",
      "episode: 18, total reward: 25.0, average_reward: 27.2, length: 24\n",
      "episode: 19, total reward: 28.0, average_reward: 25.0, length: 27\n",
      "episode: 20, total reward: 52.0, average_reward: 26.8, length: 51\n",
      "episode: 21, total reward: 19.0, average_reward: 26.2, length: 18\n",
      "episode: 22, total reward: 11.0, average_reward: 25.0, length: 10\n",
      "episode: 23, total reward: 19.0, average_reward: 23.9, length: 18\n",
      "episode: 24, total reward: 26.0, average_reward: 24.9, length: 25\n",
      "episode: 25, total reward: 12.0, average_reward: 23.0, length: 11\n",
      "episode: 26, total reward: 15.0, average_reward: 23.6, length: 14\n",
      "episode: 27, total reward: 19.0, average_reward: 22.6, length: 18\n",
      "episode: 28, total reward: 31.0, average_reward: 23.2, length: 30\n",
      "episode: 29, total reward: 14.0, average_reward: 21.8, length: 13\n",
      "episode: 30, total reward: 37.0, average_reward: 20.3, length: 36\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mosesnah/Documents/projects/Q-learning-tutorial/policy_gradient.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/policy_gradient.ipynb#ch0000006?line=35'>36</a>\u001b[0m \u001b[39m# Run a single trial\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/policy_gradient.ipynb#ch0000006?line=36'>37</a>\u001b[0m \u001b[39mfor\u001b[39;00m steps \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m( max_steps ):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/policy_gradient.ipynb#ch0000006?line=38'>39</a>\u001b[0m     env\u001b[39m.\u001b[39;49mrender( )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/policy_gradient.ipynb#ch0000006?line=40'>41</a>\u001b[0m     \u001b[39m# Get the choice of action and the pi( a_t | s_t ) for the gradient calculation\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/policy_gradient.ipynb#ch0000006?line=41'>42</a>\u001b[0m     action, log_prob \u001b[39m=\u001b[39m policy_net\u001b[39m.\u001b[39mget_action( state )\n",
      "File \u001b[0;32m~/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/core.py:328\u001b[0m, in \u001b[0;36mWrapper.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/core.py?line=325'>326</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrender\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/core.py?line=326'>327</a>\u001b[0m     \u001b[39m\"\"\"Renders the environment with kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/core.py?line=327'>328</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_disable_render_order_enforcing \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[1;32m     <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=46'>47</a>\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\n\u001b[1;32m     <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=47'>48</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=48'>49</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=49'>50</a>\u001b[0m     )\n\u001b[0;32m---> <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py?line=50'>51</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mrender(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py:273\u001b[0m, in \u001b[0;36mCartPoleEnv.render\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=270'>271</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhuman\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=271'>272</a>\u001b[0m     pygame\u001b[39m.\u001b[39mevent\u001b[39m.\u001b[39mpump()\n\u001b[0;32m--> <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=272'>273</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclock\u001b[39m.\u001b[39;49mtick(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetadata[\u001b[39m\"\u001b[39;49m\u001b[39mrender_fps\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=273'>274</a>\u001b[0m     pygame\u001b[39m.\u001b[39mdisplay\u001b[39m.\u001b[39mflip()\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/gym/envs/classic_control/cartpole.py?line=275'>276</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrgb_array\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate the gym of Cart-and-Pole\n",
    "env = gym.make( 'CartPole-v1' )\n",
    "\n",
    "# The number of states and actions are +4 and +2\n",
    "ns  = env.observation_space.shape[ 0 ]\n",
    "na  = env.action_space.n\n",
    "\n",
    "# Generate the 3-layer Policy Network.\n",
    "policy_net = PolicyNetwork( ns, na, 128 ) \n",
    "\n",
    "# We conduct max_episode_num trails (or episodes)\n",
    "max_episode_num = 500\n",
    "\n",
    "# For each trial, we run (e.g.,) 1000 steps, i.e., T = 1000 and the trajectory will be as follows:\n",
    "# S0, A0, R1, S1, A1, ... S999 A999 R1000, S1000\n",
    "# .... in case if the simulation does not reach the terminal state\n",
    "max_steps       = 2000\n",
    "\n",
    "# Saving the number of steps for each trial\n",
    "numsteps        = []\n",
    "avg_numsteps    = []\n",
    "\n",
    "# Saving the sum of rewards of a single trial\n",
    "all_rewards     = []\n",
    "\n",
    "for episode in range( max_episode_num ):\n",
    "\n",
    "    # gym initialization\n",
    "    state     = env.reset()\n",
    "\n",
    "    # To run gradient ascent, we need to save the array of log_probs and rewards. \n",
    "    # In detail, it is the \"discounted\" rewards, but the \"update_policy\" method executes that calculation internally. \n",
    "    log_probs = []\n",
    "    rewards   = []\n",
    "\n",
    "    # Run a single trial\n",
    "    for steps in range( max_steps ):\n",
    "\n",
    "        env.render( )\n",
    "\n",
    "        # Get the choice of action and the pi( a_t | s_t ) for the gradient calculation\n",
    "        action, log_prob = policy_net.get_action( state )\n",
    "\n",
    "        # The 4th argument is \"info\", which is some sort of additional information that we don't use for this example.\n",
    "        new_state, reward, done, _ = env.step( action )\n",
    "\n",
    "        log_probs.append( log_prob )\n",
    "        rewards.append( reward )\n",
    "\n",
    "        # If the trail encounters the terminal state\n",
    "        if done: \n",
    "            update_policy( policy_net, rewards, log_probs )\n",
    "\n",
    "            numsteps.append( steps )\n",
    "\n",
    "            # Taking the average of the number of steps for the learning process\n",
    "            avg_numsteps.append( np.mean( numsteps[ -10: ] ) )\n",
    "\n",
    "            # The rewards of the whole process. \n",
    "            all_rewards.append( np.sum( rewards ))\n",
    "\n",
    "\n",
    "            if episode % 1 == 0:\n",
    "                sys.stdout.write( \"episode: {}, total reward: {}, average_reward: {}, length: {}\\n\".format(episode, np.round(np.sum(rewards), decimals = 3),  np.round(np.mean(all_rewards[-10:]), decimals = 3), steps))\n",
    "            break\n",
    "        \n",
    "        state = new_state\n",
    "    \n",
    "env.close( )\n",
    "plt.plot( numsteps )\n",
    "plt.plot( avg_numsteps )\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the optimal policy (i.e., optimal neural network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "- [1] [Great post on Github](https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/README.md)\n",
    "- [2] [Policy Gradient with Stephan Curry](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146)\n",
    "- [3] [Policy Gradient in a Nutshell](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d)\n",
    "- [4] [Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)\n",
    "- [5] [Great post from Dr. Daniel Seita](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd23c35c013382817769049cdac90792e6589b2481e2321ea0fa01482e4dfef6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
