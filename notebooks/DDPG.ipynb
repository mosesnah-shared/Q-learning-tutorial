{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "We introduce a python code for Deep Deterministic Policy Gradient (DDPG). Compared to methods using stochastic policy, where from given state $s$ the policy is defined as a probability distribution $\\pi(\\cdot|s)$ and the algorithm chooses one of the action based on that distribution, deterministic policy $\\mu(s)$ provides a specific value from the given state $s$, hence called **deterministic**.  The paper can be found [here](https://arxiv.org/abs/1509.02971), and the code is modified from [this](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b) and [this](https://github.com/sfujim/TD3/blob/master/OurDDPG.py) websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim         as optim\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import moviepy.editor      as mpy\n",
    "\n",
    "# Check whether GPU computation (i.e., CUDA) is available.\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available( ) else \"cpu\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor and Critic Networks\n",
    "\n",
    "For the DDPG algorithm, we split the Actor and Critic networks. Both networks used [ADAM optimizer](https://arxiv.org/pdf/1412.6980.pdf). \n",
    "\n",
    "## Actor\n",
    "The actor network also has four layers, and it gets state vector as input and returns an action $a=\\mu(s)$.\n",
    "The learning rate of the [ADAM optimizer](https://arxiv.org/pdf/1412.6980.pdf) is $10^{-4}$, which is identical to the [original paper](https://arxiv.org/pdf/1509.02971.pdf). [Details in Supplementary Material, Section 7 - Experiment Details](https://arxiv.org/pdf/1509.02971.pdf). \n",
    "The output of the actor network is ranged from $-1$ to $+1$, hence normalization or rescalaing the output to the actual action range of the environment is later required.\n",
    "\n",
    "## Critic \n",
    "The critic has four layers, and the critic learns the Q-function, $Q(s,a)$, hence the input is a concatenation of state and action, with a scalar output.\n",
    "We have slightly modified the neural network architecture of the [original paper](https://arxiv.org/pdf/1509.02971.pdf). For the [original paper](https://arxiv.org/pdf/1509.02971.pdf), actions $a_t$ were not included until the 2nd hidden layer of the Critic Network. [Details in Supplementary Material, Section 7 - Experiment Details](https://arxiv.org/pdf/1509.02971.pdf).\n",
    "However for us, we simply concatenate the state and action tuples and feedfoward that to the 1st layer of the Critic Network. \n",
    "The learning rate of the [ADAM optimizer](https://arxiv.org/pdf/1412.6980.pdf) is $10^{-3}$, which is identical to the [original paper](https://arxiv.org/pdf/1509.02971.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor( nn.Module ):\n",
    "    \"\"\"\n",
    "        Learning the a = mu(s) mapping, which is a deterministic function.\n",
    "    \"\"\"\n",
    "    def __init__( self, n_state: int, n_action: int, n_hidden: int = 256, max_action: float = 1.0 ):\n",
    "\n",
    "        # Class inheritance. \n",
    "        super( Actor, self ).__init__( )\n",
    "\n",
    "        # Save the maximum action value \n",
    "        assert max_action >= 0\n",
    "        self.max_action = max_action\n",
    "\n",
    "        # First Layer, changes array  with size N x ( n_state  ) to N x ( n_hidden )\n",
    "        self.l1 = nn.Linear(  n_state, n_hidden )\n",
    "\n",
    "        # Second Layer, changes array with size N x ( n_hidden ) to N x ( n_hidden )\n",
    "        self.l2 = nn.Linear( n_hidden, n_hidden )\n",
    "\n",
    "        # Third Layer, changes array  with size N x ( n_hidden ) to N x ( n_action )\n",
    "        self.l3 = nn.Linear( n_hidden, n_action )\n",
    "        \n",
    "    def forward( self, state ):\n",
    "        \n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l1( state ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l2( x ) )\n",
    "\n",
    "        # Applying to tanh, which ranges the value from -1 to +1\n",
    "        x = torch.tanh( self.l3( x ) ) \n",
    "\n",
    "        # Since the x value is from -1 to +1, we change the range to -max_action to +max_action.\n",
    "        return x * self.max_action\n",
    "\n",
    "class Critic( nn.Module ):\n",
    "    \"\"\"\n",
    "        Learning the Q(s,a) function, which is the \"Quality\" function. Hence, input is a concatenation of state, action and the output is a scalar. \n",
    "    \"\"\"\n",
    "    def __init__( self, n_state, n_action, n_hidden = 256 ):\n",
    "\n",
    "        # Class inheritance. \n",
    "        super( Critic, self ).__init__()\n",
    "\n",
    "        # First Layer, changes array with size N x ( n_state + n_action ) to N x ( n_hidden )\n",
    "        self.l1 = nn.Linear( n_state + n_action, n_hidden )\n",
    "\n",
    "        # Second Layer, changes array with size N x ( n_hidden ) to N x ( n_hidden )\n",
    "        self.l2 = nn.Linear( n_hidden, n_hidden )\n",
    "\n",
    "        # Third Layer, changes array with size N x ( n_hidden ) to N x ( 1 ), since Q is a scalar function. \n",
    "        self.l3 = nn.Linear( n_hidden, 1 )\n",
    "\n",
    "    \n",
    "    def forward( self, state, action ):\n",
    "\n",
    "        # Concatenation of state and action vector.\n",
    "        # The state  is assumed to be a 2D array with size N x n_s, where N is the number of samples\n",
    "        # The action is assumed to be a 2D array with size N x n_a, where N is the number of samples\n",
    "        # As a result of torch.cat( [ state, action ] along axis 1, ), we have size N x ( n_s + n_a ), and the dim = 0 must have the same size\n",
    "        x = torch.cat( [ state, action ], dim = 1 )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l1( x ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l2( x ) )\n",
    "\n",
    "        # A simple Ax + b combination \n",
    "        x = self.l3( x )\n",
    "\n",
    "        # The output is a N x 1 array. \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "As in Deep Q-Network (DQN), we construct a replay buffer. As mentioned in [this paper](https://arxiv.org/abs/1509.02971), optimization algorithms assume that the samples are independently and identically distributed, and the replay buffer addresses that problem. Replay buffer is simply a collection of state-action-reward-next state pairs, $(S_t, A_t, R_{t+1}, S_{t+1})$. \n",
    "The code is borrowed from [this repository](https://github.com/sfujim/TD3/blob/master/utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer( object ):\n",
    "\n",
    "    def __init__( self, n_state, n_action, max_size = 100000 ):\n",
    "\n",
    "        # Save the dimension of state, dimension of action and the maximum size of the replay buffer\n",
    "        self.n_state  = n_state\n",
    "        self.n_action = n_action\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # Defining the current size of the replay buffer, just to make the sampling easy. \n",
    "        self.current_size = 0\n",
    "\n",
    "        # Defining the Index Pointer (ptr) of the replaybuffer. \n",
    "        # This is required for \"adding\" the experiences on the replaybuffer. \n",
    "        self.idx_ptr      = 0\n",
    "\n",
    "        # Defining the 2D arrays of the ReplayBuffer\n",
    "        # 2D array definition is necessary to forward the Neural Network\n",
    "        self.states      = np.zeros( ( max_size, n_state   ) )\n",
    "        self.actions     = np.zeros( ( max_size, n_action  ) )\n",
    "        self.rewards     = np.zeros( ( max_size, 1         ) )\n",
    "        self.next_states = np.zeros( ( max_size, n_state   ) )\n",
    "        self.is_done     = np.zeros( ( max_size, 1         ) )\n",
    "\n",
    "\n",
    "    def add( self, state, action, reward, next_state, is_done ):\n",
    "        \"\"\"\n",
    "            Adding a state-action-reward-next_state pair into the ReplayBuffer. \n",
    "        \"\"\"\n",
    "\n",
    "        self.states[      self.idx_ptr ] = state\n",
    "        self.actions[     self.idx_ptr ] = action\n",
    "        self.rewards[     self.idx_ptr ] = reward\n",
    "        self.next_states[ self.idx_ptr ] = next_state\n",
    "        self.is_done[     self.idx_ptr ] = is_done\n",
    "\n",
    "        # Update our index pointer. Note that the \"oldest\" experiences are overwritten.\n",
    "        self.idx_ptr = ( self.idx_ptr + 1 ) % self.max_size\n",
    "\n",
    "        # Update the current size of the replay buffer\n",
    "        self.current_size = min( self.current_size + 1, self.max_size )\n",
    "\n",
    "    def sample( self, n_batch_size ):\n",
    "        \"\"\"\n",
    "            Collect \"n_batch_size\" samples from the replay buffer and return it as a batch.\n",
    "        \"\"\"\n",
    "        idx = np.random.randint( 0, self.current_size, size = n_batch_size )\n",
    "\n",
    "        # Returning the 2D numpy array as a 2D torch array.\n",
    "\n",
    "        return ( \n",
    "            torch.FloatTensor(      self.states[ idx ]  ).to( device ) ,\n",
    "            torch.FloatTensor(     self.actions[ idx ]  ).to( device ) , \n",
    "            torch.FloatTensor(     self.rewards[ idx ]  ).to( device ) ,\n",
    "            torch.FloatTensor( self.next_states[ idx ]  ).to( device ) ,\n",
    "            torch.FloatTensor(     self.is_done[ idx ]  ).to( device )   \n",
    "        )\n",
    "\n",
    "\n",
    "    def reset( self ):\n",
    "        \"\"\"\n",
    "            Reset all the replay buffers to zeros\n",
    "        \"\"\"\n",
    "        self.states      = np.zeros( ( self.max_size, self.n_state   ) )\n",
    "        self.actions     = np.zeros( ( self.max_size, self.n_action  ) )\n",
    "        self.rewards     = np.zeros( ( self.max_size, 1              ) )\n",
    "        self.next_states = np.zeros( ( self.max_size, self.n_state   ) )\n",
    "        self.is_done     = np.zeros( ( self.max_size, 1              ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ornstein-Uhlenbeck Process\n",
    "\n",
    "The Ornstein-Uhlenbeck Process generates noise that is correlated with the previous noise, as to prevent the noise from canceling out or “freezing” the overall dynamics. Adding this noise is mentioned in the [original paper](https://arxiv.org/abs/1509.02971). [Wikipedia](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) provides a thorough explanation of the Ornstein-Uhlenbeck Process. The source code is from [this](https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py). The Ornstein-Uhlenbeck process with an additional drift term can be described as:\n",
    "$$\n",
    "    dx_t = \\theta (\\mu - x_t)dt + \\sigma d W_t\n",
    "$$\n",
    "where $W_t$ denotes the [Wiener process](https://en.wikipedia.org/wiki/Wiener_process). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OUNoise( object ):\n",
    "    \n",
    "    def __init__( self, action_space, mu = 0.0, theta = 0.15, max_sigma = 0.3, min_sigma = 0.3, decay_period = 100000 ):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[ 0 ]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset( )\n",
    "        \n",
    "    def reset( self ):\n",
    "        # Caution! The state here is not the \"pendulum\"'s state, but the \"noise\" itself. \n",
    "        self.state = np.ones( self.action_dim ) * self.mu\n",
    "        \n",
    "    def step( self, t = 0 ):\n",
    "        \n",
    "        # Call the current noise value. \n",
    "        x  = self.state\n",
    "\n",
    "        # randn returns a sample from the standard (i.e., normal) distribution\n",
    "        dx = self.theta * ( self.mu - x ) + self.sigma * np.random.randn( self.action_dim )\n",
    "\n",
    "        # For our case, we simply set the max_sigma and min_sigma the same, hence the sigma value is constant for us\n",
    "        self.sigma = self.max_sigma - ( self.max_sigma - self.min_sigma ) * min( 1.0, t / self.decay_period )\n",
    "\n",
    "        # Time-increment. x_{n+1} = x_{n} + dx\n",
    "        self.state = x + dx\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    def add_noise2action( self, action, t = 0 ): \n",
    "        \n",
    "        # Calculate the noise with respect to the given time. \n",
    "        ou_noise   = self.step( t )\n",
    "\n",
    "        # Adding ou noise onto the action and then clipping it.\n",
    "        return np.clip( action + ou_noise, self.low, self.high )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Agent\n",
    "Note that when we develop the actor and critic networks, we also make the copy of those networks, called actor-target and critic-target networks. The details are again, explained in [this paper](https://arxiv.org/abs/1509.02971).\n",
    "\n",
    "The critic (or value) network is updated similarly as is done in Q-learning, where the target networks are employed. \n",
    "The loss function that are used for the policy gradient are defined as follows:\n",
    "$$\n",
    "    \\text{Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\big( y_i - Q( s_i, a_i | \\theta^{Q}) \\big )^2\n",
    "$$\n",
    "where $N$ is the sampled batch from the replay buffer. $y_i$ is defined as follows:\n",
    "$$\n",
    "    y_i = r_i + \\gamma Q'\\big( \\; s_{i+1}, \\; \\mu'(s_{i+1} | \\theta^{\\mu'} ) | \\theta^{Q'} \\big)\n",
    "$$\n",
    "where comma superscript stands for the target network parameters. \n",
    "\n",
    "The actor network is updated with the following equation:\n",
    "$$\n",
    "    J( \\theta ) = \\mathbb{E} \\big[ Q(s,a)|_{s=s_t, a = \\mu(s_t)} \\big]\n",
    "$$\n",
    "\n",
    "For the target networks, the parameters are \"soft\" updated with the following equations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\theta^{Q'}   & \\longleftarrow \\tau \\theta^{Q}   + ( 1 - \\tau ) \\theta^{Q'}   \\\\\n",
    "    \\theta^{\\mu'} & \\longleftarrow \\tau \\theta^{\\mu} + ( 1 - \\tau ) \\theta^{\\mu'} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\tau \\in [0,1)$ and we should choose a small value (for our example, $\\tau=0.01$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDPGagent( object ):\n",
    "\n",
    "    def __init__( self, n_state, n_action, max_action = 1., gamma = 0.99, tau = 0.005 ):\n",
    "\n",
    "        # Actor Network , its target (copy) Network, and the ADAM optimizer.\n",
    "        self.actor             = Actor( n_state, n_action, max_action = max_action ).to( device )\n",
    "        self.actor_target      = copy.deepcopy( self.actor )\n",
    "        self.actor_optimizer   = optim.Adam(  self.actor.parameters( ), lr = 1e-4 )\n",
    "\n",
    "        # Critic Network, its target (copy) Network, and the ADAM optimizer.\n",
    "        self.critic            = Critic( n_state, n_action )\n",
    "        self.critic_target     = copy.deepcopy( self.critic )\n",
    "        self.critic_optimizer  = optim.Adam(  self.critic.parameters( ), lr = 1e-3 )\n",
    "\n",
    "        # The discount factor gamma and the soft-update gain tau\n",
    "        self.gamma = gamma\n",
    "        self.tau   = tau\n",
    "\n",
    "        # The maximum action.\n",
    "        self.max_action = max_action\n",
    "\n",
    "    \n",
    "    def get_action( self, state ):\n",
    "\n",
    "        # Conduct the a = mu(s), where mu is a \"deterministic function\"\n",
    "        # Unsqueeze makes an 1 x n_s array of state. \n",
    "        state  = torch.from_numpy( state ).float( ).unsqueeze( 0 )\n",
    "\n",
    "        # Returns an 1 x n_a array of state\n",
    "        # forward method can be omitted\n",
    "        action = self.actor( state )\n",
    "\n",
    "        # Change action from Torch to Numpy.\n",
    "        # Since n_a is 1 for this case, action is simply an 1x1 array.\n",
    "        # Hence, flattening the data. \n",
    "        action = action.cpu( ).data.numpy( ).flatten( )\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update( self, replay_buffer, batch_size = 256 ):\n",
    "        \"\"\"\n",
    "            Mini-batch update. \n",
    "        \"\"\"\n",
    "        # Randomly sample batch_size numbers of S A R S.\n",
    "        states, actions, rewards, next_states, is_done = replay_buffer.sample( batch_size )\n",
    "\n",
    "        # ====================================================== #\n",
    "        # ================ Critic Optimizer Part =============== #\n",
    "        # ====================================================== #\n",
    "        # Keep in mind that the original paper first optimizes the Critic network\n",
    "\n",
    "        # Critic loss \n",
    "        Qprime  = self.critic_target( next_states, self.actor_target( next_states ) )\n",
    "        Qprime  = rewards + ( ( 1. - is_done ) * self.gamma * Qprime ).detach( )\n",
    "        Q       = self.critic( states,actions )\n",
    "\n",
    "        critic_loss  = F.mse_loss( Q, Qprime )\n",
    "\n",
    "        # Update Critic network\n",
    "        self.critic_optimizer.zero_grad( )\n",
    "        critic_loss.backward( ) \n",
    "        self.critic_optimizer.step( )\n",
    "\n",
    "        # ====================================================== #\n",
    "        # ================ Actor Optimizer Part ================ #\n",
    "        # ====================================================== #\n",
    "        # Actor loss, it is simply the mean of the Q function \n",
    "        # The Q function value Q( s, a ) is actually Q( s, mu( s ) ), hence the Q function is described as the parameters of mu (actor).\n",
    "        # Since a is a continuous function, we can compute its gradient.   \n",
    "        actor_loss = - self.critic( states, self.actor( states ) ).mean( )\n",
    "        \n",
    "        # Update (Optimize) Actor network\n",
    "        self.actor_optimizer.zero_grad( )\n",
    "        actor_loss.backward( )\n",
    "        self.actor_optimizer.step( )\n",
    "\n",
    "\n",
    "        # The \"soft\" update of target networks\n",
    "        for target_param, param in zip( self.actor_target.parameters( ), self.actor.parameters( ) ):\n",
    "            target_param.data.copy_( param.data * self.tau + target_param.data * ( 1.0 - self.tau ) )\n",
    "       \n",
    "        for target_param, param in zip( self.critic_target.parameters( ), self.critic.parameters( ) ):\n",
    "            target_param.data.copy_( param.data * self.tau + target_param.data * ( 1.0 - self.tau ) )\n",
    "\n",
    "\n",
    "    def save( self, filename ):\n",
    "        \n",
    "        torch.save( self.critic.state_dict( )           , filename + \"_critic\"              )\n",
    "        torch.save( self.critic_optimizer.state_dict( ) , filename + \"_critic_optimizer\"    )\n",
    "        \n",
    "        torch.save( self.actor.state_dict( )            , filename + \"_actor\"               )\n",
    "        torch.save( self.actor_optimizer.state_dict( )  , filename + \"_actor_optimizer\"     )\n",
    "\n",
    "\n",
    "    def load( self, filename ):\n",
    "\n",
    "        # Load Critic\n",
    "        self.critic.load_state_dict(            torch.load( filename + \"_critic\"           )  )\n",
    "        self.critic_optimizer.load_state_dict(  torch.load( filename + \"_critic_optimizer\" )  )\n",
    "        self.critic_target = copy.deepcopy( self.critic )\n",
    "\n",
    "        # Load Actor\n",
    "        self.actor.load_state_dict(             torch.load( filename + \"_actor\"            )  )\n",
    "        self.actor_optimizer.load_state_dict(   torch.load( filename + \"_actor_optimizer\"  )  )\n",
    "        self.actor_target = copy.deepcopy( self.actor )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: -1391.07, average_reward: -1391.0723476594578 \n",
      "episode: 1, reward: -1324.53, average_reward: -1357.800843714047 \n",
      "episode: 2, reward: -1295.76, average_reward: -1337.1216119955363 \n",
      "episode: 3, reward: -1458.58, average_reward: -1367.4850768503402 \n",
      "episode: 4, reward: -1691.9, average_reward: -1432.3689281665663 \n",
      "episode: 5, reward: -1498.38, average_reward: -1443.3701303717562 \n",
      "episode: 6, reward: -1619.02, average_reward: -1468.4622784144287 \n",
      "episode: 7, reward: -1623.29, average_reward: -1487.81635054323 \n",
      "episode: 8, reward: -1467.29, average_reward: -1485.5356557718726 \n",
      "episode: 9, reward: -1579.13, average_reward: -1494.8953569541325 \n",
      "episode: 10, reward: -1549.0, average_reward: -1510.6884473606897 \n",
      "episode: 11, reward: -1484.16, average_reward: -1526.6519545871433 \n",
      "episode: 12, reward: -1167.25, average_reward: -1513.8003002615894 \n",
      "episode: 13, reward: -938.56, average_reward: -1461.798265941035 \n",
      "episode: 14, reward: -674.29, average_reward: -1360.0364893775375 \n",
      "episode: 15, reward: -911.52, average_reward: -1301.3508308328355 \n",
      "episode: 16, reward: -757.57, average_reward: -1215.2064457276133 \n",
      "episode: 17, reward: -643.21, average_reward: -1117.1978250540055 \n",
      "episode: 18, reward: -970.06, average_reward: -1067.4748799085514 \n",
      "episode: 19, reward: -644.94, average_reward: -974.0554320560265 \n",
      "episode: 20, reward: -632.26, average_reward: -882.3810996965445 \n",
      "episode: 21, reward: -969.9, average_reward: -830.9546143327204 \n",
      "episode: 22, reward: -380.12, average_reward: -752.2423731934789 \n",
      "episode: 23, reward: -634.79, average_reward: -721.8660630102811 \n",
      "episode: 24, reward: -252.71, average_reward: -679.7087062596872 \n",
      "episode: 25, reward: -364.53, average_reward: -625.0102048384241 \n",
      "episode: 26, reward: -249.53, average_reward: -574.2056421847739 \n",
      "episode: 27, reward: -253.02, average_reward: -535.1870569487412 \n",
      "episode: 28, reward: -249.92, average_reward: -463.1733028661474 \n",
      "episode: 29, reward: -256.06, average_reward: -424.2852952817123 \n",
      "episode: 30, reward: -258.26, average_reward: -386.8849062524124 \n",
      "episode: 31, reward: -281.36, average_reward: -318.0313453573007 \n",
      "episode: 32, reward: -253.63, average_reward: -305.38228575147195 \n",
      "episode: 33, reward: -497.85, average_reward: -291.6877999008896 \n",
      "episode: 34, reward: -384.27, average_reward: -304.8434876400094 \n",
      "episode: 35, reward: -256.94, average_reward: -294.084355891881 \n",
      "episode: 36, reward: -513.0, average_reward: -320.4319954596062 \n",
      "episode: 37, reward: -123.98, average_reward: -307.52767651566444 \n",
      "episode: 38, reward: -123.7, average_reward: -294.9052739440546 \n",
      "episode: 39, reward: -251.85, average_reward: -294.4843553428497 \n",
      "episode: 40, reward: -3.88, average_reward: -269.04640229547766 \n",
      "episode: 41, reward: -801.67, average_reward: -321.0774082270242 \n",
      "episode: 42, reward: -395.14, average_reward: -335.22844915675523 \n",
      "episode: 43, reward: -4.56, average_reward: -285.89944488800944 \n",
      "episode: 44, reward: -252.79, average_reward: -272.7518731280896 \n",
      "episode: 45, reward: -116.81, average_reward: -258.7384600662198 \n",
      "episode: 46, reward: -250.81, average_reward: -232.5196949900324 \n",
      "episode: 47, reward: -6.45, average_reward: -220.76691332251252 \n",
      "episode: 48, reward: -117.92, average_reward: -220.18865598254456 \n",
      "episode: 49, reward: -121.95, average_reward: -207.19892607179844 \n",
      "episode: 50, reward: -747.73, average_reward: -281.5840477892423 \n",
      "episode: 51, reward: -124.76, average_reward: -213.89259504348652 \n",
      "episode: 52, reward: -620.23, average_reward: -236.40080530722713 \n",
      "episode: 53, reward: -347.21, average_reward: -270.66599016812404 \n",
      "episode: 54, reward: -2.97, average_reward: -245.68405238225355 \n",
      "episode: 55, reward: -119.37, average_reward: -245.940270741199 \n",
      "episode: 56, reward: -246.21, average_reward: -245.47984224516236 \n",
      "episode: 57, reward: -116.12, average_reward: -256.4462145152075 \n",
      "episode: 58, reward: -248.28, average_reward: -269.48252504918344 \n",
      "episode: 59, reward: -581.42, average_reward: -315.4297894994239 \n",
      "episode: 60, reward: -122.7, average_reward: -252.9267679722972 \n",
      "episode: 61, reward: -897.27, average_reward: -330.17786587415014 \n",
      "episode: 62, reward: -127.13, average_reward: -280.8682021715749 \n",
      "episode: 63, reward: -445.17, average_reward: -290.66467693891633 \n",
      "episode: 64, reward: -250.75, average_reward: -315.44245469710677 \n",
      "episode: 65, reward: -339.14, average_reward: -337.4198025611025 \n",
      "episode: 66, reward: -241.13, average_reward: -336.91201042680865 \n",
      "episode: 67, reward: -236.52, average_reward: -348.9523223165977 \n",
      "episode: 68, reward: -125.49, average_reward: -336.67380487387254 \n",
      "episode: 69, reward: -311.32, average_reward: -309.66304875439164 \n",
      "episode: 70, reward: -365.83, average_reward: -333.9765900677356 \n",
      "episode: 71, reward: -119.99, average_reward: -256.24812092360617 \n",
      "episode: 72, reward: -489.94, average_reward: -292.5288739113776 \n",
      "episode: 73, reward: -0.44, average_reward: -248.05581030541575 \n",
      "episode: 74, reward: -127.01, average_reward: -235.68146624397664 \n",
      "episode: 75, reward: -237.02, average_reward: -225.4691148016037 \n",
      "episode: 76, reward: -123.05, average_reward: -213.66076689116431 \n",
      "episode: 77, reward: -265.74, average_reward: -216.5826242582178 \n",
      "episode: 78, reward: -363.12, average_reward: -240.34535059852615 \n",
      "episode: 79, reward: -119.43, average_reward: -221.15660520767423 \n",
      "episode: 80, reward: -384.61, average_reward: -223.03439058264652 \n",
      "episode: 81, reward: -234.05, average_reward: -234.44099113231968 \n",
      "episode: 82, reward: -125.43, average_reward: -197.98991389922224 \n",
      "episode: 83, reward: -127.45, average_reward: -210.69074034376794 \n",
      "episode: 84, reward: -124.2, average_reward: -210.4098530185528 \n",
      "episode: 85, reward: -125.02, average_reward: -199.20971764682298 \n",
      "episode: 86, reward: -122.77, average_reward: -199.1813673743644 \n",
      "episode: 87, reward: -237.51, average_reward: -196.3587137744608 \n",
      "episode: 88, reward: -242.82, average_reward: -184.32860208019184 \n",
      "episode: 89, reward: -233.45, average_reward: -195.73103267605066 \n",
      "episode: 90, reward: -126.57, average_reward: -169.92735877165734 \n",
      "episode: 91, reward: -247.21, average_reward: -171.24349595941038 \n",
      "episode: 92, reward: -125.6, average_reward: -171.2609564972708 \n",
      "episode: 93, reward: -121.35, average_reward: -170.65078105834348 \n",
      "episode: 94, reward: -120.64, average_reward: -170.29468500401498 \n",
      "episode: 95, reward: -118.01, average_reward: -169.59394106273183 \n",
      "episode: 96, reward: -122.26, average_reward: -169.5439051468403 \n",
      "episode: 97, reward: -0.83, average_reward: -145.87600686080054 \n",
      "episode: 98, reward: -123.5, average_reward: -133.94412309872143 \n",
      "episode: 99, reward: -255.91, average_reward: -136.18965651036808 \n",
      "episode: 100, reward: -0.32, average_reward: -123.56465615026548 \n",
      "episode: 101, reward: -372.01, average_reward: -136.044247350567 \n",
      "episode: 102, reward: -0.95, average_reward: -123.57936997264389 \n",
      "episode: 103, reward: -124.45, average_reward: -123.88961383194001 \n",
      "episode: 104, reward: -515.1, average_reward: -163.33581916835118 \n",
      "episode: 105, reward: -117.9, average_reward: -163.3241554721422 \n",
      "episode: 106, reward: -371.74, average_reward: -188.27170502957694 \n",
      "episode: 107, reward: -234.78, average_reward: -211.66657750332996 \n",
      "episode: 108, reward: -391.18, average_reward: -238.43468033854288 \n",
      "episode: 109, reward: -123.59, average_reward: -225.20316139354958 \n",
      "episode: 110, reward: -227.15, average_reward: -247.88563351987705 \n",
      "episode: 111, reward: -126.95, average_reward: -223.37962879766005 \n",
      "episode: 112, reward: -246.08, average_reward: -247.89276540055448 \n",
      "episode: 113, reward: -240.87, average_reward: -259.5344467533939 \n",
      "episode: 114, reward: -120.75, average_reward: -220.09940010208214 \n",
      "episode: 115, reward: -125.97, average_reward: -220.90729692065256 \n",
      "episode: 116, reward: -125.18, average_reward: -196.25103152520003 \n",
      "episode: 117, reward: -480.65, average_reward: -220.8384608095283 \n",
      "episode: 118, reward: -250.63, average_reward: -206.7833978567265 \n",
      "episode: 119, reward: -120.21, average_reward: -206.44529505362394 \n",
      "episode: 120, reward: -490.52, average_reward: -232.78212380619826 \n",
      "episode: 121, reward: -125.71, average_reward: -232.6587068815853 \n",
      "episode: 122, reward: -246.27, average_reward: -232.67718289974215 \n",
      "episode: 123, reward: -240.27, average_reward: -232.61731491496903 \n",
      "episode: 124, reward: -0.98, average_reward: -220.6400505211885 \n",
      "episode: 125, reward: -125.81, average_reward: -220.62343742497802 \n",
      "episode: 126, reward: -365.08, average_reward: -244.6139351451841 \n",
      "episode: 127, reward: -127.07, average_reward: -209.25530376512197 \n",
      "episode: 128, reward: -124.54, average_reward: -196.6461480817159 \n",
      "episode: 129, reward: -306.37, average_reward: -215.2621290154695 \n",
      "episode: 130, reward: -126.67, average_reward: -178.87760702181717 \n",
      "episode: 131, reward: -2.97, average_reward: -166.60340443957122 \n",
      "episode: 132, reward: -360.54, average_reward: -178.03026945833636 \n",
      "episode: 133, reward: -239.76, average_reward: -177.97970264182007 \n",
      "episode: 134, reward: -366.01, average_reward: -214.4825564068211 \n",
      "episode: 135, reward: -130.52, average_reward: -214.95338524076882 \n",
      "episode: 136, reward: -413.7, average_reward: -219.8150323658433 \n",
      "episode: 137, reward: -242.57, average_reward: -231.36508409460484 \n",
      "episode: 138, reward: -3.05, average_reward: -219.2162444116571 \n",
      "episode: 139, reward: -411.62, average_reward: -229.74088903376378 \n",
      "episode: 140, reward: -132.11, average_reward: -230.28481330368504 \n",
      "episode: 141, reward: -249.02, average_reward: -254.8899557681175 \n",
      "episode: 142, reward: -254.21, average_reward: -244.25759811822985 \n",
      "episode: 143, reward: -254.49, average_reward: -245.73031876144515 \n",
      "episode: 144, reward: -256.5, average_reward: -234.78003248125387 \n",
      "episode: 145, reward: -490.68, average_reward: -270.7963265416421 \n",
      "episode: 146, reward: -248.08, average_reward: -254.2347934564791 \n",
      "episode: 147, reward: -377.69, average_reward: -267.7472337143374 \n",
      "episode: 148, reward: -383.59, average_reward: -305.8009201096379 \n",
      "episode: 149, reward: -242.26, average_reward: -288.865440951514 \n",
      "episode: 150, reward: -373.42, average_reward: -312.99609872094743 \n",
      "episode: 151, reward: -360.65, average_reward: -324.1585700057427 \n",
      "episode: 152, reward: -502.13, average_reward: -348.95026718495114 \n",
      "episode: 153, reward: -567.36, average_reward: -380.2375641233066 \n",
      "episode: 154, reward: -387.19, average_reward: -393.30578281151054 \n",
      "episode: 155, reward: -494.67, average_reward: -393.704784779159 \n",
      "episode: 156, reward: -256.28, average_reward: -394.5245801244906 \n",
      "episode: 157, reward: -365.76, average_reward: -393.33155416833023 \n",
      "episode: 158, reward: -372.37, average_reward: -392.2098179924119 \n",
      "episode: 159, reward: -254.75, average_reward: -393.45852388020455 \n",
      "episode: 160, reward: -238.89, average_reward: -380.0055689725003 \n",
      "episode: 161, reward: -506.93, average_reward: -394.63385369631897 \n",
      "episode: 162, reward: -500.44, average_reward: -394.4652742594409 \n",
      "episode: 163, reward: -369.61, average_reward: -374.68972519521293 \n",
      "episode: 164, reward: -376.0, average_reward: -373.57103501385166 \n",
      "episode: 165, reward: -125.41, average_reward: -336.64540570848027 \n",
      "episode: 166, reward: -232.56, average_reward: -334.2733092935465 \n",
      "episode: 167, reward: -125.19, average_reward: -310.215614940391 \n",
      "episode: 168, reward: -488.58, average_reward: -321.8364974940714 \n",
      "episode: 169, reward: -147.39, average_reward: -311.1004451801946 \n",
      "episode: 170, reward: -4.65, average_reward: -287.67627305651115 \n",
      "episode: 171, reward: -4.72, average_reward: -237.45505367191763 \n",
      "episode: 172, reward: -245.19, average_reward: -211.93007878603234 \n",
      "episode: 173, reward: -254.07, average_reward: -200.37599472521828 \n",
      "episode: 174, reward: -86.42, average_reward: -171.41856546642288 \n",
      "episode: 175, reward: -251.78, average_reward: -184.0555201970059 \n",
      "episode: 176, reward: -119.39, average_reward: -172.738772483617 \n",
      "episode: 177, reward: -116.09, average_reward: -171.8291260529824 \n",
      "episode: 178, reward: -125.07, average_reward: -135.47848188182363 \n",
      "episode: 179, reward: -3.4, average_reward: -121.07971019432755 \n",
      "episode: 180, reward: -239.4, average_reward: -144.55463691271476 \n",
      "episode: 181, reward: -243.73, average_reward: -168.45545269080168 \n",
      "episode: 182, reward: -248.38, average_reward: -168.77404315675219 \n",
      "episode: 183, reward: -3.24, average_reward: -143.69126444344516 \n",
      "episode: 184, reward: -251.85, average_reward: -160.2334914779458 \n",
      "episode: 185, reward: -2.12, average_reward: -135.26740998009691 \n",
      "episode: 186, reward: -130.34, average_reward: -136.36193639228668 \n",
      "episode: 187, reward: -132.18, average_reward: -137.97067599926092 \n",
      "episode: 188, reward: -123.23, average_reward: -137.786181487031 \n",
      "episode: 189, reward: -117.02, average_reward: -149.1474485875192 \n",
      "episode: 190, reward: -130.03, average_reward: -138.21045367921502 \n",
      "episode: 191, reward: -3.07, average_reward: -114.14496970884898 \n",
      "episode: 192, reward: -379.81, average_reward: -127.28775165466978 \n",
      "episode: 193, reward: -123.74, average_reward: -139.3380290600913 \n",
      "episode: 194, reward: -249.62, average_reward: -139.11503788245858 \n",
      "episode: 195, reward: -124.95, average_reward: -151.39772686085402 \n",
      "episode: 196, reward: -259.97, average_reward: -164.3609252500972 \n",
      "episode: 197, reward: -378.07, average_reward: -188.95036722345654 \n",
      "episode: 198, reward: -129.14, average_reward: -189.54194210570935 \n",
      "episode: 199, reward: -254.64, average_reward: -203.30482082964085 \n",
      "episode: 200, reward: -495.72, average_reward: -239.87389509766018 \n",
      "episode: 201, reward: -492.78, average_reward: -288.8446321564831 \n",
      "episode: 202, reward: -252.87, average_reward: -276.15116323567355 \n",
      "episode: 203, reward: -377.68, average_reward: -301.5452362453004 \n",
      "episode: 204, reward: -382.19, average_reward: -314.802475528759 \n",
      "episode: 205, reward: -254.78, average_reward: -327.7852415979879 \n",
      "episode: 206, reward: -254.64, average_reward: -327.2523703611922 \n",
      "episode: 207, reward: -382.01, average_reward: -327.6460128251724 \n",
      "episode: 208, reward: -260.66, average_reward: -340.7971967626199 \n",
      "episode: 209, reward: -250.57, average_reward: -340.389731630503 \n",
      "episode: 210, reward: -256.06, average_reward: -316.4242856675434 \n",
      "episode: 211, reward: -491.19, average_reward: -316.2648243740193 \n",
      "episode: 212, reward: -249.71, average_reward: -315.9488773462286 \n",
      "episode: 213, reward: -232.94, average_reward: -301.4742251590301 \n",
      "episode: 214, reward: -370.02, average_reward: -300.2568033732632 \n",
      "episode: 215, reward: -133.21, average_reward: -288.0996409575057 \n",
      "episode: 216, reward: -367.76, average_reward: -299.4112632438383 \n",
      "episode: 217, reward: -8.79, average_reward: -262.0894256545529 \n",
      "episode: 218, reward: -388.23, average_reward: -274.8469539653964 \n",
      "episode: 219, reward: -250.89, average_reward: -274.87916283673246 \n",
      "episode: 220, reward: -129.55, average_reward: -262.2284633849 \n",
      "episode: 221, reward: -506.96, average_reward: -263.8054511776848 \n",
      "episode: 222, reward: -251.9, average_reward: -264.0237306898597 \n",
      "episode: 223, reward: -255.89, average_reward: -266.31946479895373 \n",
      "episode: 224, reward: -606.57, average_reward: -289.9752333126297 \n",
      "episode: 225, reward: -10.08, average_reward: -277.6624124914103 \n",
      "episode: 226, reward: -371.42, average_reward: -278.0286379370972 \n",
      "episode: 227, reward: -498.27, average_reward: -326.9765108375249 \n",
      "episode: 228, reward: -355.8, average_reward: -323.73377975851446 \n",
      "episode: 229, reward: -376.02, average_reward: -336.2465606962938 \n",
      "episode: 230, reward: -257.27, average_reward: -349.01764326083105 \n",
      "episode: 231, reward: -256.68, average_reward: -323.9897949232481 \n",
      "episode: 232, reward: -253.57, average_reward: -324.1567933808365 \n",
      "episode: 233, reward: -130.28, average_reward: -311.5956818082332 \n",
      "episode: 234, reward: -253.3, average_reward: -276.26880574528894 \n",
      "episode: 235, reward: -361.38, average_reward: -311.3988191131224 \n",
      "episode: 236, reward: -134.53, average_reward: -287.709939773253 \n",
      "episode: 237, reward: -530.82, average_reward: -290.96504085315263 \n",
      "episode: 238, reward: -506.79, average_reward: -306.0638034401491 \n",
      "episode: 239, reward: -260.54, average_reward: -294.5158132424779 \n",
      "episode: 240, reward: -187.06, average_reward: -287.4956160528125 \n",
      "episode: 241, reward: -13.65, average_reward: -263.19331220925426 \n",
      "episode: 242, reward: -416.09, average_reward: -279.44536421948436 \n",
      "episode: 243, reward: -493.99, average_reward: -315.8161471723165 \n",
      "episode: 244, reward: -493.98, average_reward: -339.8839648643659 \n",
      "episode: 245, reward: -384.92, average_reward: -342.23833257725505 \n",
      "episode: 246, reward: -130.08, average_reward: -341.79288181488937 \n",
      "episode: 247, reward: -255.04, average_reward: -314.21541222327426 \n",
      "episode: 248, reward: -381.46, average_reward: -301.68185865753514 \n",
      "episode: 249, reward: -129.39, average_reward: -288.5664162978184 \n",
      "episode: 250, reward: -615.3, average_reward: -331.3903632984774 \n",
      "episode: 251, reward: -256.19, average_reward: -355.6441489846801 \n",
      "episode: 252, reward: -380.58, average_reward: -352.0938692307087 \n",
      "episode: 253, reward: -382.18, average_reward: -340.913192809281 \n",
      "episode: 254, reward: -273.38, average_reward: -318.8527914933471 \n",
      "episode: 255, reward: -377.01, average_reward: -318.0621420632453 \n",
      "episode: 256, reward: -133.87, average_reward: -318.44173632134937 \n",
      "episode: 257, reward: -130.18, average_reward: -305.9549347124993 \n",
      "episode: 258, reward: -237.0, average_reward: -291.50961994485453 \n",
      "episode: 259, reward: -135.47, average_reward: -292.11777933861333 \n",
      "episode: 260, reward: -475.21, average_reward: -278.1082186741367 \n",
      "episode: 261, reward: -372.27, average_reward: -289.7161010274898 \n",
      "episode: 262, reward: -134.0, average_reward: -265.0576376123631 \n",
      "episode: 263, reward: -356.94, average_reward: -262.5336527108434 \n",
      "episode: 264, reward: -132.36, average_reward: -248.4317249262027 \n",
      "episode: 265, reward: -190.15, average_reward: -229.7457248842145 \n",
      "episode: 266, reward: -251.11, average_reward: -241.46987420880123 \n",
      "episode: 267, reward: -381.08, average_reward: -266.55985508294907 \n",
      "episode: 268, reward: -367.02, average_reward: -279.5618596574957 \n",
      "episode: 269, reward: -339.38, average_reward: -299.95318912754607 \n",
      "episode: 270, reward: -132.93, average_reward: -265.7252321447988 \n",
      "episode: 271, reward: -621.8, average_reward: -290.6780357129634 \n",
      "episode: 272, reward: -395.54, average_reward: -316.8316736336737 \n",
      "episode: 273, reward: -374.26, average_reward: -318.56303173481564 \n",
      "episode: 274, reward: -364.4, average_reward: -341.7671262004961 \n",
      "episode: 275, reward: -373.91, average_reward: -360.1424620080356 \n",
      "episode: 276, reward: -481.58, average_reward: -383.1894736442949 \n",
      "episode: 277, reward: -133.75, average_reward: -358.4572842671602 \n",
      "episode: 278, reward: -249.56, average_reward: -346.71108494951403 \n",
      "episode: 279, reward: -615.06, average_reward: -374.2786564603883 \n",
      "episode: 280, reward: -375.42, average_reward: -398.5283342618841 \n",
      "episode: 281, reward: -419.44, average_reward: -378.2924290144571 \n",
      "episode: 282, reward: -130.22, average_reward: -351.7603504157822 \n",
      "episode: 283, reward: -497.92, average_reward: -364.12649143821034 \n",
      "episode: 284, reward: -350.4, average_reward: -362.72665789528736 \n",
      "episode: 285, reward: -382.59, average_reward: -363.59497701854883 \n",
      "episode: 286, reward: -371.01, average_reward: -352.5371487107622 \n",
      "episode: 287, reward: -500.24, average_reward: -389.18583827057034 \n",
      "episode: 288, reward: -513.58, average_reward: -415.58808020751604 \n",
      "episode: 289, reward: -253.04, average_reward: -379.3869852113729 \n",
      "episode: 290, reward: -127.08, average_reward: -354.5528508837911 \n",
      "episode: 291, reward: -372.14, average_reward: -349.8224326539736 \n",
      "episode: 292, reward: -378.47, average_reward: -374.64823064033186 \n",
      "episode: 293, reward: -385.38, average_reward: -363.39394536432735 \n",
      "episode: 294, reward: -500.56, average_reward: -378.4098196357236 \n",
      "episode: 295, reward: -770.76, average_reward: -417.22641017834394 \n",
      "episode: 296, reward: -377.42, average_reward: -417.86823986228285 \n",
      "episode: 297, reward: -382.11, average_reward: -406.05503711655354 \n",
      "episode: 298, reward: -129.31, average_reward: -367.62731212079456 \n",
      "episode: 299, reward: -142.17, average_reward: -356.53973603030386 \n",
      "episode: 300, reward: -122.81, average_reward: -356.1119100234563 \n",
      "episode: 301, reward: -123.35, average_reward: -331.2336184672039 \n",
      "episode: 302, reward: -356.83, average_reward: -329.069263502049 \n",
      "episode: 303, reward: -129.23, average_reward: -303.45480271236727 \n",
      "episode: 304, reward: -126.61, average_reward: -266.05977928056257 \n",
      "episode: 305, reward: -128.99, average_reward: -201.8828055384559 \n",
      "episode: 306, reward: -352.56, average_reward: -199.39629399874016 \n",
      "episode: 307, reward: -117.78, average_reward: -172.9628645902574 \n",
      "episode: 308, reward: -134.8, average_reward: -173.51265740587414 \n",
      "episode: 309, reward: -119.51, average_reward: -171.24656805873366 \n",
      "episode: 310, reward: -250.28, average_reward: -183.99368821432003 \n",
      "episode: 311, reward: -499.94, average_reward: -221.6529033380034 \n",
      "episode: 312, reward: -255.75, average_reward: -211.5451979604979 \n",
      "episode: 313, reward: -385.0, average_reward: -237.1223424610142 \n",
      "episode: 314, reward: -375.42, average_reward: -262.00358031548797 \n",
      "episode: 315, reward: -372.93, average_reward: -286.3980370590524 \n",
      "episode: 316, reward: -235.39, average_reward: -274.68152983772563 \n",
      "episode: 317, reward: -474.51, average_reward: -310.354814944695 \n",
      "episode: 318, reward: -512.92, average_reward: -348.1668345048891 \n",
      "episode: 319, reward: -270.25, average_reward: -363.24149824898547 \n",
      "episode: 320, reward: -127.46, average_reward: -350.9593604287471 \n",
      "episode: 321, reward: -381.28, average_reward: -339.09293154714567 \n",
      "episode: 322, reward: -496.24, average_reward: -363.1414481888296 \n",
      "episode: 323, reward: -130.65, average_reward: -337.7058226594579 \n",
      "episode: 324, reward: -494.36, average_reward: -349.5996804152205 \n",
      "episode: 325, reward: -253.97, average_reward: -337.7039008253106 \n",
      "episode: 326, reward: -297.35, average_reward: -343.89934901264667 \n",
      "episode: 327, reward: -380.24, average_reward: -334.47218937311084 \n",
      "episode: 328, reward: -418.22, average_reward: -325.00180507757995 \n",
      "episode: 329, reward: -125.68, average_reward: -310.5438595585535 \n",
      "episode: 330, reward: -131.33, average_reward: -310.931302071848 \n",
      "episode: 331, reward: -125.5, average_reward: -285.35309543617603 \n",
      "episode: 332, reward: -253.61, average_reward: -261.08987353665344 \n",
      "episode: 333, reward: -125.64, average_reward: -260.5887262924198 \n",
      "episode: 334, reward: -373.0, average_reward: -248.45276612600043 \n",
      "episode: 335, reward: -122.98, average_reward: -235.3535265459936 \n",
      "episode: 336, reward: -4.49, average_reward: -206.0673693528658 \n",
      "episode: 337, reward: -130.42, average_reward: -181.08584500607873 \n",
      "episode: 338, reward: -284.88, average_reward: -167.7521651686093 \n",
      "episode: 339, reward: -266.95, average_reward: -181.87973550574878 \n",
      "episode: 340, reward: -376.85, average_reward: -206.43207128892087 \n",
      "episode: 341, reward: -130.15, average_reward: -206.8974354121097 \n",
      "episode: 342, reward: -139.07, average_reward: -195.44347701197213 \n",
      "episode: 343, reward: -379.42, average_reward: -220.82219651947122 \n",
      "episode: 344, reward: -3.04, average_reward: -183.8259452806813 \n",
      "episode: 345, reward: -364.27, average_reward: -207.9552089326456 \n",
      "episode: 346, reward: -252.52, average_reward: -232.75889690378852 \n",
      "episode: 347, reward: -306.8, average_reward: -250.39680440476383 \n",
      "episode: 348, reward: -248.0, average_reward: -246.70798841185578 \n",
      "episode: 349, reward: -119.72, average_reward: -231.98489412161257 \n",
      "episode: 350, reward: -121.87, average_reward: -206.48703259737394 \n",
      "episode: 351, reward: -3.3, average_reward: -193.80137456445962 \n",
      "episode: 352, reward: -368.98, average_reward: -216.79229266448937 \n",
      "episode: 353, reward: -249.86, average_reward: -203.83608926047015 \n",
      "episode: 354, reward: -250.2, average_reward: -228.55189995280517 \n",
      "episode: 355, reward: -375.34, average_reward: -229.6585617703958 \n",
      "episode: 356, reward: -240.16, average_reward: -228.42247726782102 \n",
      "episode: 357, reward: -5.65, average_reward: -198.30712687142187 \n",
      "episode: 358, reward: -230.83, average_reward: -196.59014657748895 \n",
      "episode: 359, reward: -118.85, average_reward: -196.50315119662267 \n",
      "episode: 360, reward: -238.41, average_reward: -208.15646546690556 \n",
      "episode: 361, reward: -373.11, average_reward: -245.1378119765625 \n",
      "episode: 362, reward: -373.54, average_reward: -245.59451199259516 \n",
      "episode: 363, reward: -138.92, average_reward: -234.50024097684076 \n",
      "episode: 364, reward: -247.28, average_reward: -234.20812293664616 \n",
      "episode: 365, reward: -7.52, average_reward: -197.4262949772495 \n",
      "episode: 366, reward: -370.03, average_reward: -210.41261096793104 \n",
      "episode: 367, reward: -368.82, average_reward: -246.73041658875871 \n",
      "episode: 368, reward: -360.25, average_reward: -259.6728909462363 \n",
      "episode: 369, reward: -241.84, average_reward: -271.9717451763375 \n",
      "episode: 370, reward: -123.04, average_reward: -260.4350101018967 \n",
      "episode: 371, reward: -120.79, average_reward: -235.20299045461343 \n",
      "episode: 372, reward: -166.39, average_reward: -214.48746461831723 \n",
      "episode: 373, reward: -120.19, average_reward: -212.61447314971346 \n",
      "episode: 374, reward: -366.13, average_reward: -224.50030522048283 \n",
      "episode: 375, reward: -501.82, average_reward: -273.9301091656964 \n",
      "episode: 376, reward: -133.67, average_reward: -250.29419097512928 \n",
      "episode: 377, reward: -252.22, average_reward: -238.63391249861132 \n",
      "episode: 378, reward: -243.69, average_reward: -226.9782646900873 \n",
      "episode: 379, reward: -254.14, average_reward: -228.20869874964714 \n",
      "episode: 380, reward: -373.58, average_reward: -253.2627478115606 \n",
      "episode: 381, reward: -255.82, average_reward: -266.76589391684615 \n",
      "episode: 382, reward: -126.56, average_reward: -262.7828262061035 \n",
      "episode: 383, reward: -251.74, average_reward: -275.93779274967784 \n",
      "episode: 384, reward: -4.29, average_reward: -239.75369753102026 \n",
      "episode: 385, reward: -253.6, average_reward: -214.93194942775864 \n",
      "episode: 386, reward: -268.8, average_reward: -228.44492170027456 \n",
      "episode: 387, reward: -3.52, average_reward: -203.57477596705172 \n",
      "episode: 388, reward: -3.08, average_reward: -179.5130612897278 \n",
      "episode: 389, reward: -261.51, average_reward: -180.24955624311866 \n",
      "episode: 390, reward: -130.71, average_reward: -155.962481885148 \n",
      "episode: 391, reward: -252.99, average_reward: -155.67897371345055 \n",
      "episode: 392, reward: -251.9, average_reward: -168.21310963357783 \n",
      "episode: 393, reward: -249.66, average_reward: -168.00548576615648 \n",
      "episode: 394, reward: -253.03, average_reward: -192.87936302032773 \n",
      "episode: 395, reward: -128.81, average_reward: -180.39993618950353 \n",
      "episode: 396, reward: -238.84, average_reward: -177.40406020419672 \n",
      "episode: 397, reward: -246.84, average_reward: -201.73620604505925 \n",
      "episode: 398, reward: -121.05, average_reward: -213.53317216836308 \n",
      "episode: 399, reward: -123.9, average_reward: -199.77278242690628 \n",
      "episode: 400, reward: -245.08, average_reward: -211.20988332746455 \n",
      "episode: 401, reward: -364.83, average_reward: -222.39426001966612 \n",
      "episode: 402, reward: -227.48, average_reward: -219.9520966323242 \n",
      "episode: 403, reward: -127.08, average_reward: -207.69398548822292 \n",
      "episode: 404, reward: -239.11, average_reward: -206.30204722035597 \n",
      "episode: 405, reward: -246.3, average_reward: -218.05140330441196 \n",
      "episode: 406, reward: -250.53, average_reward: -219.220744248038 \n",
      "episode: 407, reward: -612.22, average_reward: -255.75820663432742 \n",
      "episode: 408, reward: -248.72, average_reward: -268.52541958901594 \n",
      "episode: 409, reward: -229.38, average_reward: -279.0726453078803 \n",
      "episode: 410, reward: -613.89, average_reward: -315.9533599543168 \n",
      "episode: 411, reward: -378.82, average_reward: -317.35258578606846 \n",
      "episode: 412, reward: -256.79, average_reward: -320.28385727543235 \n",
      "episode: 413, reward: -253.71, average_reward: -332.946863908284 \n",
      "episode: 414, reward: -120.58, average_reward: -321.093623232451 \n",
      "episode: 415, reward: -258.26, average_reward: -322.2889949649146 \n",
      "episode: 416, reward: -396.56, average_reward: -336.8917586889552 \n",
      "episode: 417, reward: -254.27, average_reward: -301.0974156932897 \n",
      "episode: 418, reward: -503.12, average_reward: -326.5373519116505 \n",
      "episode: 419, reward: -124.47, average_reward: -316.0465887035035 \n",
      "episode: 420, reward: -124.99, average_reward: -267.15696534416185 \n",
      "episode: 421, reward: -251.3, average_reward: -254.40477901828754 \n",
      "episode: 422, reward: -500.85, average_reward: -278.8106570646025 \n",
      "episode: 423, reward: -365.41, average_reward: -289.98085069820127 \n",
      "episode: 424, reward: -496.87, average_reward: -327.61022411351126 \n",
      "episode: 425, reward: -499.4, average_reward: -351.7250921018391 \n",
      "episode: 426, reward: -605.14, average_reward: -372.58287028993743 \n",
      "episode: 427, reward: -622.2, average_reward: -409.37542607156456 \n",
      "episode: 428, reward: -502.65, average_reward: -409.3289021196921 \n",
      "episode: 429, reward: -498.61, average_reward: -446.7434373446411 \n",
      "episode: 430, reward: -384.15, average_reward: -472.65974198922675 \n",
      "episode: 431, reward: -254.69, average_reward: -472.9987289307519 \n",
      "episode: 432, reward: -361.81, average_reward: -459.09460739525167 \n",
      "episode: 433, reward: -128.61, average_reward: -435.4138428204301 \n",
      "episode: 434, reward: -117.61, average_reward: -397.48760548396933 \n",
      "episode: 435, reward: -130.44, average_reward: -360.590948438301 \n",
      "episode: 436, reward: -229.54, average_reward: -323.0312202351115 \n",
      "episode: 437, reward: -236.79, average_reward: -284.4904346118084 \n",
      "episode: 438, reward: -413.32, average_reward: -275.5571061745361 \n",
      "episode: 439, reward: -250.42, average_reward: -250.73777105076323 \n",
      "episode: 440, reward: -2.61, average_reward: -212.58345318345764 \n",
      "episode: 441, reward: -121.12, average_reward: -199.2263057659648 \n",
      "episode: 442, reward: -123.16, average_reward: -175.3613079399043 \n",
      "episode: 443, reward: -377.11, average_reward: -200.2115815004778 \n",
      "episode: 444, reward: -126.54, average_reward: -201.10405605420033 \n",
      "episode: 445, reward: -132.95, average_reward: -201.35506707176603 \n",
      "episode: 446, reward: -130.09, average_reward: -191.41027644088823 \n",
      "episode: 447, reward: -119.54, average_reward: -179.68565642773518 \n",
      "episode: 448, reward: -0.4, average_reward: -138.39394501830344 \n",
      "episode: 449, reward: -373.28, average_reward: -150.6796780850848 \n",
      "episode: 450, reward: -124.35, average_reward: -162.8539196739525 \n",
      "episode: 451, reward: -225.82, average_reward: -173.32374058076493 \n",
      "episode: 452, reward: -5.18, average_reward: -161.52557703579126 \n",
      "episode: 453, reward: -128.35, average_reward: -136.64964489058988 \n",
      "episode: 454, reward: -236.5, average_reward: -147.6456953832238 \n",
      "episode: 455, reward: -442.79, average_reward: -178.62954371820248 \n",
      "episode: 456, reward: -120.54, average_reward: -177.6742694804152 \n",
      "episode: 457, reward: -252.94, average_reward: -191.01372650544363 \n",
      "episode: 458, reward: -128.98, average_reward: -203.87191535624757 \n",
      "episode: 459, reward: -132.7, average_reward: -179.81376422323157 \n",
      "episode: 460, reward: -131.66, average_reward: -180.54401876757473 \n",
      "episode: 461, reward: -131.9, average_reward: -171.15271323239062 \n",
      "episode: 462, reward: -368.79, average_reward: -207.51456125996006 \n",
      "episode: 463, reward: -125.52, average_reward: -207.23163505800744 \n",
      "episode: 464, reward: -126.76, average_reward: -196.25744697470475 \n",
      "episode: 465, reward: -127.19, average_reward: -164.69765059609682 \n",
      "episode: 466, reward: -398.5, average_reward: -192.49394667661483 \n",
      "episode: 467, reward: -6.05, average_reward: -167.8048266769696 \n",
      "episode: 468, reward: -375.76, average_reward: -192.48230041435608 \n",
      "episode: 469, reward: -486.59, average_reward: -227.87205461283503 \n",
      "episode: 470, reward: -232.24, average_reward: -237.93003698182838 \n",
      "episode: 471, reward: -237.04, average_reward: -248.44392514437814 \n",
      "episode: 472, reward: -321.66, average_reward: -243.73055276084793 \n",
      "episode: 473, reward: -132.99, average_reward: -244.47774789636205 \n",
      "episode: 474, reward: -126.84, average_reward: -244.48576741033327 \n",
      "episode: 475, reward: -123.82, average_reward: -244.14865847858374 \n",
      "episode: 476, reward: -260.65, average_reward: -230.36367992709629 \n",
      "episode: 477, reward: -6.54, average_reward: -230.41334531672993 \n",
      "episode: 478, reward: -128.25, average_reward: -205.66232849192957 \n",
      "episode: 479, reward: -129.76, average_reward: -169.97918712958756 \n",
      "episode: 480, reward: -249.79, average_reward: -171.7345764609599 \n",
      "episode: 481, reward: -127.28, average_reward: -160.7579666943843 \n",
      "episode: 482, reward: -476.59, average_reward: -176.25133908265207 \n",
      "episode: 483, reward: -250.86, average_reward: -188.03819251287376 \n",
      "episode: 484, reward: -253.81, average_reward: -200.73563048072123 \n",
      "episode: 485, reward: -131.96, average_reward: -201.55032414660855 \n",
      "episode: 486, reward: -358.18, average_reward: -211.3032839669438 \n",
      "episode: 487, reward: -5.97, average_reward: -211.24622585099868 \n",
      "episode: 488, reward: -124.06, average_reward: -210.82748718009992 \n",
      "episode: 489, reward: -240.54, average_reward: -221.90543602852495 \n",
      "episode: 490, reward: -1024.03, average_reward: -299.32959262280593 \n",
      "episode: 491, reward: -124.23, average_reward: -299.0253822212583 \n",
      "episode: 492, reward: -127.06, average_reward: -264.0723126956827 \n",
      "episode: 493, reward: -380.97, average_reward: -277.08305101572057 \n",
      "episode: 494, reward: -360.52, average_reward: -287.7545180287058 \n",
      "episode: 495, reward: -128.37, average_reward: -287.3952359820886 \n",
      "episode: 496, reward: -121.31, average_reward: -263.7077118452388 \n",
      "episode: 497, reward: -128.37, average_reward: -275.94693275444996 \n",
      "episode: 498, reward: -121.29, average_reward: -275.6701638805264 \n",
      "episode: 499, reward: -238.05, average_reward: -275.4210481199555 \n"
     ]
    }
   ],
   "source": [
    "# Define instances of the environment, DDPG agent and the OU noise.\n",
    "env   = gym.make( \"Pendulum-v1\" ) \n",
    "\n",
    "# Set the random seeds\n",
    "env.seed(              round( time.time( ) ) )\n",
    "env.action_space.seed( round( time.time( ) ) )\n",
    "torch.manual_seed(     round( time.time( ) ) )\n",
    "np.random.seed(        round( time.time( ) ) )\n",
    "\n",
    "# Get the dimension of states and actions, and also the \n",
    "# [WARNING] This is for environments where we assume the mean of action is 0. \n",
    "n_state    = env.observation_space.shape[ 0 ] \n",
    "n_action   = env.action_space.shape[ 0 ]\n",
    "max_action = float( env.action_space.high  )\n",
    "\n",
    "# Define the agent, noise and replay buffers\n",
    "agent         = DDPGagent( n_state, n_action, max_action )\n",
    "OUnoise       = OUNoise( env.action_space )\n",
    "replay_buffer = ReplayBuffer( n_state, n_action )\n",
    "\n",
    "# The number of \"batch\" that will be sampled from the replay buffer will be \"batch_size\" \n",
    "n_batch_size  = 256\n",
    "\n",
    "# Saving these values to plot the performance at the end.\n",
    "frames        = [ ]\n",
    "whole_rewards = [ ]\n",
    "\n",
    "# Flags for turning on or off the render.\n",
    "is_save_video = False\n",
    "is_save_model = True\n",
    "\n",
    "# For the pendulum model the best reward is 0, hence saving a -infinity value. \n",
    "best_model_val = -np.inf\n",
    "\n",
    "rewards       = [ ]\n",
    "avg_rewards   = [ ]\n",
    "\n",
    "for episode in range( 500 ):\n",
    "\n",
    "    # Initialize the gym environment and OU noise \n",
    "    state = env.reset()\n",
    "    OUnoise.reset( )\n",
    "\n",
    "    # Initialize the episode's reward\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # For pendulum v1 gym, a single simulation is maximum 200-steps long. \n",
    "    # [REF] https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py\n",
    "    for step in range( 200 ):\n",
    "\n",
    "        # Get the action value from the deterministic policy Actor network.\n",
    "        action = agent.get_action( state )\n",
    "\n",
    "        if is_save_video : frames.append( env.render( mode = 'rgb_array' ) )\n",
    "\n",
    "        # Apply the OU noise on this action\n",
    "        action = OUnoise.add_noise2action( action, step )\n",
    "\n",
    "        # Run a single step of simulation\n",
    "        new_state, reward, done, _ = env.step( action )  \n",
    "\n",
    "        # Add this to our replay buffer, note that push simply generates the tuple and add \n",
    "        replay_buffer.add( state, action, reward, new_state, done )\n",
    "        \n",
    "        # Once the agent memory is full, then update the policy via replay buffer.\n",
    "        if replay_buffer.current_size > n_batch_size: agent.update( replay_buffer, batch_size = n_batch_size )        \n",
    "        \n",
    "        # Update the state and reward value \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if best_model_val <= episode_reward:\n",
    "        best_model_val = episode_reward \n",
    "\n",
    "        # If this policy has a good result, save it \n",
    "        if is_save_model: agent.save( \"../models/DDPG_best_model\" ) \n",
    "\n",
    "    # Once a single simulation is done, append the values that will be plotted later\n",
    "    rewards.append( episode_reward )\n",
    "    avg_rewards.append( np.mean( rewards[ -10 : ] ) )\n",
    "\n",
    "    sys.stdout.write(\"episode: {}, reward: {}, average_reward: {} \\n\".format( episode, np.round( episode_reward, decimals = 2 ), avg_rewards[ -1 ] ) ) \n",
    "\n",
    "whole_rewards.append(  rewards  )\n",
    "\n",
    "env.close( )\n",
    "\n",
    "if is_save_video:\n",
    "    clip = mpy.ImageSequenceClip( frames, fps = 30 )\n",
    "    clip.write_gif( \"../videos/DDPG.gif\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.plot( rewards     )\n",
    "plt.plot( avg_rewards )\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file ../videos/circle.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  76%|███████▌  | 151/200 [00:04<00:01, 35.12it/s, now=None]"
     ]
    }
   ],
   "source": [
    "is_save_video = True\n",
    "\n",
    "env   = gym.make( \"Pendulum-v1\" )  \n",
    "\n",
    "# Set the random seeds\n",
    "env.seed(              round( time.time( ) ) )\n",
    "env.action_space.seed( round( time.time( ) ) )\n",
    "\n",
    "\n",
    "# Get the dimension of states and actions, and also the \n",
    "# [WARNING] This is for environments where we assume the mean of action is 0. \n",
    "n_state    = env.observation_space.shape[ 0 ] \n",
    "n_action   = env.action_space.shape[ 0 ]\n",
    "max_action = float( env.action_space.high  )\n",
    "\n",
    "model_path = \"../models/DDPG_best_model\"\n",
    "agent      = DDPGagent( n_state, n_action, max_action )\n",
    "agent.load( model_path )\n",
    "\n",
    "# Run trial\n",
    "state = env.reset()   \n",
    "\n",
    "frames = []\n",
    "# The maximum trial is 200 for the pendulum \n",
    "for _ in range( 200 ):\n",
    "\n",
    "    if is_save_video: frames.append( env.render( mode = \"rgb_array\")  ) \n",
    "    env.render( )\n",
    "\n",
    "    # Get the choice of action and the pi( a_t | s_t ) for the gradient calculation\n",
    "    action = agent.get_action( state )\n",
    "    new_state, _, done, _ = env.step( action )\n",
    "\n",
    "    # If the trail encounters the terminal state\n",
    "    if done: \n",
    "        break\n",
    "    state = new_state\n",
    "\n",
    "# Save Video\n",
    "env.close( )\n",
    "\n",
    "if is_save_video:\n",
    "    clip = mpy.ImageSequenceClip( frames, fps = 30 )\n",
    "    clip.write_gif( \"../videos/DDPG.gif\" )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742fb12a3d29bb1ab201ca06dfc1cb9996440041728c355377b69362e9fdf7ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
