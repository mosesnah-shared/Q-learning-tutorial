{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "We introduce a python code for Deep Deterministic Policy Gradient (DDPG). Compared to methods using stochastic policy, where from given state $s$ the policy is defined as a probability distribution $\\pi(\\cdot|s)$ and the algorithm chooses one of the action based on that distribution, deterministic policy $\\mu(s)$ provides a specific value from the given state $s$, hence called **deterministic**.  The paper can be found [here](https://arxiv.org/abs/1509.02971), and the code is modified from [This website](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim         as optim\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor and Critic Network (Separate)\n",
    "\n",
    "For the DDPG algorithm, we split the Actor and Critic networks\n",
    "\n",
    "## Critic \n",
    "The critic has four layers, and the critic learns the Q-function, $Q(s,a)$, hence the input is a concatenation of state and action, with a scalar output. \n",
    "\n",
    "## Actor\n",
    "The actor network also has four layers, and it gets state vector as input and returns an action $a=\\mu(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic( nn.Module ):\n",
    "\n",
    "    def __init__( self, input_size, hidden_size, output_size ):\n",
    "\n",
    "        super( Critic, self ).__init__()\n",
    "\n",
    "        # First Layer\n",
    "        self.linear1 = nn.Linear(  input_size, hidden_size )\n",
    "\n",
    "        # Second Layer\n",
    "        self.linear2 = nn.Linear( hidden_size, hidden_size )\n",
    "\n",
    "        # Third Layer\n",
    "        self.linear3 = nn.Linear( hidden_size, output_size )\n",
    "\n",
    "    \n",
    "    def forward( self, state, action ):\n",
    "\n",
    "        # Concatenation of state and action vector\n",
    "        x = torch.cat( [ state, action ] , 1 )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.linear1( x ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.linear2( x ) )\n",
    "\n",
    "        # A simple Ax + b combination \n",
    "        x = self.linear3( x )\n",
    "\n",
    "        return x\n",
    "\n",
    "class Actor( nn.Module ):\n",
    "\n",
    "    def __init__( self, input_size, hidden_size, output_size ):\n",
    "\n",
    "        super( Actor, self ).__init__( )\n",
    "\n",
    "        # First Layer\n",
    "        self.linear1 = nn.Linear(  input_size, hidden_size )\n",
    "\n",
    "        # Second Layer\n",
    "        self.linear2 = nn.Linear( hidden_size, hidden_size )\n",
    "\n",
    "        # Third Layer\n",
    "        self.linear3 = nn.Linear( hidden_size, output_size )\n",
    "        \n",
    "    def forward( self, state ):\n",
    "        \n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.linear1( state ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.linear2( x ) )\n",
    "\n",
    "        # Applying to tanh, which ranges the value from -1 to +1\n",
    "        x = torch.tanh( self.linear3( x ) ) \n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "As in Deep Q-Network (DQN), we construct a replay buffer. As mentioned in [this paper](https://arxiv.org/abs/1509.02971), optimization algorithms assume that the samples are independently and identically distributed, and the replay buffer addresses that problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "\n",
    "    def __init__( self, max_size ):\n",
    "        self.buffer = deque( maxlen = max_size )\n",
    "    \n",
    "    def push( self, state, action, reward, next_state, done ):\n",
    "        \n",
    "        experience = ( state, action, np.array( [ reward ] ), next_state, done )\n",
    "\n",
    "        # Insert the tuple of (S A R S) into to the right end of the buffer deque.\n",
    "        self.buffer.append( experience )\n",
    "\n",
    "    def sample( self, batch_size ):\n",
    "        \n",
    "        # The sample is simply an array of \n",
    "        # Sn An Rn+1 Sn+1\n",
    "        # Where S, A and R are variables that are self-explanatory.  \n",
    "        state_batch      = [ ]\n",
    "        action_batch     = [ ]\n",
    "        reward_batch     = [ ] \n",
    "        next_state_batch = [ ]\n",
    "        done_batch       = [ ]\n",
    "\n",
    "        # Sample batch_size amount of list from the buffer\n",
    "        batch = random.sample( self.buffer, batch_size )\n",
    "\n",
    "        for experience in batch:\n",
    "\n",
    "            state, action, reward, next_state, done = experience\n",
    "\n",
    "            state_batch.append(       state      )\n",
    "            action_batch.append(      action     )\n",
    "            reward_batch.append(      reward     )\n",
    "            next_state_batch.append(  next_state )\n",
    "            done_batch.append(        done       )\n",
    "        \n",
    "        return state_batch, action_batch, reward_batch, next_state_batch, done_batch\n",
    "\n",
    "    def __len__( self ):\n",
    "        return len( self.buffer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ornstein-Uhlenbeck Process\n",
    "\n",
    "The Ornstein-Uhlenbeck Process generates noise that is correlated with the previous noise, as to prevent the noise from canceling out or “freezing” the overall dynamics. Adding this noise is mentioned in the [original paper](https://arxiv.org/abs/1509.02971). [Wikipedia](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) provides a thorough explanation of the Ornstein-Uhlenbeck Process. The source code is from [this](https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py). The Ornstein-Uhlenbeck process with an additional drift term can be described as:\n",
    "$$\n",
    "    dx_t = \\theta (\\mu - x_t)dt + \\sigma d W_t\n",
    "$$\n",
    "where $W_t$ denotes the [Wiener process](https://en.wikipedia.org/wiki/Wiener_process). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OUNoise( object ):\n",
    "    def __init__( self, action_space, mu = 0.0, theta = 0.15, max_sigma = 0.3, min_sigma = 0.3, decay_period = 100000 ):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[ 0 ]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset( self ):\n",
    "        self.state = np.ones( self.action_dim ) * self.mu\n",
    "        \n",
    "    def evolve_state( self ):\n",
    "        \n",
    "        # State re-definition\n",
    "        x  = self.state\n",
    "\n",
    "        # randn returns a sample from the standard (i.e., normal) distribution\n",
    "        dx = self.theta * ( self.mu - x ) + self.sigma * np.random.randn( self.action_dim )\n",
    "\n",
    "        # Time-increment. x_{n+1} = x_{n} + dx\n",
    "        self.state = x + dx\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    def get_action( self, action, t = 0 ): \n",
    "        \n",
    "        ou_state   = self.evolve_state( )\n",
    "\n",
    "        # For our case, we simply set the max_sigma and min_sigma the same, hence the sigma value is constant for us\n",
    "        self.sigma = self.max_sigma - ( self.max_sigma - self.min_sigma ) * min( 1.0, t / self.decay_period )\n",
    "\n",
    "        # Adding ou noise onto the action and then clipping it.\n",
    "        return np.clip( action + ou_state, self.low, self.high )\n",
    "\n",
    "\n",
    "class NormalizedEnv( gym.ActionWrapper ):\n",
    "    \"\"\" \n",
    "        The pendulum v1's action min/max are -2/+2, respectively. \n",
    "        But the action output of tanh is -1 to +1, hence we need scale the action values between range [-2, +2] and [-1, +1]\n",
    "\n",
    "        [REF] https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py\n",
    "        [REF] https://github.com/openai/gym/blob/master/gym/core.py\n",
    "        [REF] https://www.gymlibrary.ml/content/wrappers/\n",
    "    \"\"\"\n",
    " \n",
    "    def action( self, act ):\n",
    "        \"\"\"\n",
    "            Action range must change from [-1, +1] (The actor network's output) to [-2, +2], which will be the input to the gym.\n",
    "            Hence, simply multiply 2. Note that this method does not work for other gym environments, where the range differs. \n",
    "        \"\"\"\n",
    "\n",
    "        return 2 * act\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Agent\n",
    "Note that when we develop the actor and critic networks, we also make the copy of those networks, called actor-target and critic-target networks. The details are again, explained in [this paper](https://arxiv.org/abs/1509.02971)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDPGagent( object ):\n",
    "    def __init__( self, env, hidden_size=256, actor_learning_rate = 1e-4, critic_learning_rate = 1e-3, gamma = 0.99, tau = 1e-2, max_memory_size = 50000 ):\n",
    "\n",
    "        # Params\n",
    "        self.num_states  = env.observation_space.shape[ 0 ]\n",
    "        self.num_actions = env.action_space.shape[ 0 ]\n",
    "\n",
    "        # Actor Networks\n",
    "        self.actor        = Actor( self.num_states, hidden_size, self.num_actions )\n",
    "        self.actor_target = Actor( self.num_states, hidden_size, self.num_actions )\n",
    "\n",
    "        # Critic Networks\n",
    "        self.critic        = Critic( self.num_states + self.num_actions, hidden_size, self.num_actions )\n",
    "        self.critic_target = Critic( self.num_states + self.num_actions, hidden_size, self.num_actions )\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau   = tau\n",
    "\n",
    "        for target_param, param in zip( self.actor_target.parameters( ), self.actor.parameters( ) ):\n",
    "\n",
    "            # Copy the parameters of the actor parameters to the target parameters\n",
    "            target_param.data.copy_( param.data )\n",
    "\n",
    "        for target_param, param in zip( self.critic_target.parameters( ), self.critic.parameters( ) ):\n",
    "\n",
    "            # Copy the parameters of the actor parameters to the target parameters\n",
    "            target_param.data.copy_(param.data)\n",
    "        \n",
    "        # Construct the replay-buffer. The memory size is \n",
    "        self.memory = Memory( max_memory_size )      \n",
    "\n",
    "        self.critic_criterion  = nn.MSELoss()\n",
    "        self.actor_optimizer   = optim.Adam(  self.actor.parameters( ), lr =  actor_learning_rate )\n",
    "        self.critic_optimizer  = optim.Adam( self.critic.parameters( ), lr = critic_learning_rate )\n",
    "    \n",
    "    def get_action( self, state ):\n",
    "        state  = torch.from_numpy( state ).float( ).unsqueeze( 0 )\n",
    "        action = self.actor.forward( state )\n",
    "        action = action.detach( ).numpy( )[ 0,0 ]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update( self, batch_size ):\n",
    "\n",
    "        states, actions, rewards, next_states, _ = self.memory.sample( batch_size )\n",
    "\n",
    "        states      = torch.FloatTensor( states      )\n",
    "        actions     = torch.FloatTensor( actions     )\n",
    "        rewards     = torch.FloatTensor( rewards     )\n",
    "        next_states = torch.FloatTensor( next_states )\n",
    "    \n",
    "        # Critic loss        \n",
    "        Qvals        = self.critic.forward( states, actions )\n",
    "        next_actions = self.actor_target.forward( next_states )\n",
    "        next_Q       = self.critic_target.forward( next_states, next_actions.detach( ) )\n",
    "        Qprime       = rewards + self.gamma * next_Q\n",
    "        critic_loss  = self.critic_criterion( Qvals, Qprime )\n",
    "\n",
    "        # Actor loss\n",
    "        policy_loss = - self.critic.forward( states, self.actor.forward( states ) ).mean( )\n",
    "        \n",
    "        # Update Actor network\n",
    "        self.actor_optimizer.zero_grad( )\n",
    "        policy_loss.backward( )\n",
    "        self.actor_optimizer.step( )\n",
    "\n",
    "        # Update Critic network\n",
    "        self.critic_optimizer.zero_grad( )\n",
    "        critic_loss.backward( ) \n",
    "        self.critic_optimizer.step( )\n",
    "\n",
    "        # Update target networks \n",
    "        for target_param, param in zip( self.actor_target.parameters( ), self.actor.parameters( ) ):\n",
    "            target_param.data.copy_( param.data * self.tau + target_param.data * ( 1.0 - self.tau ) )\n",
    "       \n",
    "        for target_param, param in zip( self.critic_target.parameters( ), self.critic.parameters( ) ):\n",
    "            target_param.data.copy_( param.data * self.tau + target_param.data * ( 1.0 - self.tau ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env = NormalizedEnv( gym.make( \"Pendulum-v1\" )  )\n",
    "\n",
    "\n",
    "agent = DDPGagent( env )\n",
    "noise = OUNoise(env.action_space)\n",
    "batch_size = 128\n",
    "rewards = []\n",
    "avg_rewards = []\n",
    "\n",
    "for episode in range(50):\n",
    "    state = env.reset()\n",
    "    noise.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = agent.get_action(state)\n",
    "        action = noise.get_action(action, step)\n",
    "        new_state, reward, done, _ = env.step(action) \n",
    "        agent.memory.push(state, action, reward, new_state, done)\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.update(batch_size)        \n",
    "        \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            sys.stdout.write(\"episode: {}, reward: {}, average _reward: {} \\n\".format(episode, np.round(episode_reward, decimals=2), np.mean(rewards[-10:])))\n",
    "            break\n",
    "\n",
    "    rewards.append(episode_reward)\n",
    "    avg_rewards.append(np.mean(rewards[-10:]))\n",
    "\n",
    "plt.plot(rewards)\n",
    "plt.plot(avg_rewards)\n",
    "plt.plot()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742fb12a3d29bb1ab201ca06dfc1cb9996440041728c355377b69362e9fdf7ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
