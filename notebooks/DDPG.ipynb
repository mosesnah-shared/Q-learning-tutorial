{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "We introduce a python code for Deep Deterministic Policy Gradient (DDPG). Compared to methods using stochastic policy, where from given state $s$ the policy is defined as a probability distribution $\\pi(\\cdot|s)$ and the algorithm chooses one of the action based on that distribution, deterministic policy $\\mu(s)$ provides a specific value from the given state $s$, hence called **deterministic**.  The paper can be found [here](https://arxiv.org/abs/1509.02971), and the code is modified from [this](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b) and [this](https://github.com/sfujim/TD3/blob/master/OurDDPG.py) websites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim         as optim\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import moviepy.editor      as mpy\n",
    "\n",
    "# Check whether GPU computation (i.e., CUDA) is available.\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available( ) else \"cpu\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor and Critic Networks\n",
    "\n",
    "For the DDPG algorithm, we split the Actor and Critic networks. Both networks used [ADAM optimizer](https://arxiv.org/pdf/1412.6980.pdf). \n",
    "\n",
    "## Actor\n",
    "The actor network also has four layers, and it gets state vector as input and returns an action $a=\\mu(s)$.\n",
    "The learning rate of the [ADAM optimizer](https://arxiv.org/pdf/1412.6980.pdf) is $10^{-4}$, which is identical to the [original paper](https://arxiv.org/pdf/1509.02971.pdf). [Details in Supplementary Material, Section 7 - Experiment Details](https://arxiv.org/pdf/1509.02971.pdf). \n",
    "The output of the actor network is ranged from $-1$ to $+1$, hence normalization or rescalaing the output to the actual action range of the environment is later required.\n",
    "\n",
    "## Critic \n",
    "The critic has four layers, and the critic learns the Q-function, $Q(s,a)$, hence the input is a concatenation of state and action, with a scalar output.\n",
    "We have slightly modified the neural network architecture of the [original paper](https://arxiv.org/pdf/1509.02971.pdf). For the [original paper](https://arxiv.org/pdf/1509.02971.pdf), actions $a_t$ were not included until the 2nd hidden layer of the Critic Network. [Details in Supplementary Material, Section 7 - Experiment Details](https://arxiv.org/pdf/1509.02971.pdf).\n",
    "However for us, we simply concatenate the state and action tuples and feedfoward that to the 1st layer of the Critic Network. \n",
    "The learning rate of the [ADAM optimizer](https://arxiv.org/pdf/1412.6980.pdf) is $10^{-3}$, which is identical to the [original paper](https://arxiv.org/pdf/1509.02971.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Actor( nn.Module ):\n",
    "    \"\"\"\n",
    "        Learning the a = mu(s) mapping, which is a deterministic function.\n",
    "    \"\"\"\n",
    "    def __init__( self, n_state: int, n_action: int, n_hidden: int = 256, max_action: float = 1.0 ):\n",
    "\n",
    "        # Class inheritance. \n",
    "        super( Actor, self ).__init__( )\n",
    "\n",
    "        # Save the maximum action value \n",
    "        assert max_action >= 0\n",
    "        self.max_action = max_action\n",
    "\n",
    "        # First Layer, changes array  with size N x ( n_state  ) to N x ( n_hidden )\n",
    "        self.l1 = nn.Linear(  n_state, n_hidden )\n",
    "\n",
    "        # Second Layer, changes array with size N x ( n_hidden ) to N x ( n_hidden )\n",
    "        self.l2 = nn.Linear( n_hidden, n_hidden )\n",
    "\n",
    "        # Third Layer, changes array  with size N x ( n_hidden ) to N x ( n_action )\n",
    "        self.l3 = nn.Linear( n_hidden, n_action )\n",
    "        \n",
    "    def forward( self, state ):\n",
    "        \n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l1( state ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l2( x ) )\n",
    "\n",
    "        # Applying to tanh, which ranges the value from -1 to +1\n",
    "        x = torch.tanh( self.l3( x ) ) \n",
    "\n",
    "        # Since the x value is from -1 to +1, we change the range to -max_action to +max_action.\n",
    "        return x * self.max_action\n",
    "\n",
    "class Critic( nn.Module ):\n",
    "    \"\"\"\n",
    "        Learning the Q(s,a) function, which is the \"Quality\" function. Hence, input is a concatenation of state, action and the output is a scalar. \n",
    "    \"\"\"\n",
    "    def __init__( self, n_state, n_action, n_hidden = 256 ):\n",
    "\n",
    "        # Class inheritance. \n",
    "        super( Critic, self ).__init__()\n",
    "\n",
    "        # First Layer, changes array with size N x ( n_state + n_action ) to N x ( n_hidden )\n",
    "        self.l1 = nn.Linear( n_state + n_action, n_hidden )\n",
    "\n",
    "        # Second Layer, changes array with size N x ( n_hidden ) to N x ( n_hidden )\n",
    "        self.l2 = nn.Linear( n_hidden, n_hidden )\n",
    "\n",
    "        # Third Layer, changes array with size N x ( n_hidden ) to N x ( 1 ), since Q is a scalar function. \n",
    "        self.l3 = nn.Linear( n_hidden, 1 )\n",
    "\n",
    "    \n",
    "    def forward( self, state, action ):\n",
    "\n",
    "        # Concatenation of state and action vector.\n",
    "        # The state  is assumed to be a 2D array with size N x n_s, where N is the number of samples\n",
    "        # The action is assumed to be a 2D array with size N x n_a, where N is the number of samples\n",
    "        # As a result of torch.cat( [ state, action ] along axis 1, ), we have size N x ( n_s + n_a ), and the dim = 0 must have the same size\n",
    "        x = torch.cat( [ state, action ], dim = 1 )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l1( x ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l2( x ) )\n",
    "\n",
    "        # A simple Ax + b combination \n",
    "        x = self.l3( x )\n",
    "\n",
    "        # The output is a N x 1 array. \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "As in Deep Q-Network (DQN), we construct a replay buffer. As mentioned in [this paper](https://arxiv.org/abs/1509.02971), optimization algorithms assume that the samples are independently and identically distributed, and the replay buffer addresses that problem. Replay buffer is simply a collection of state-action-reward-next state pairs, $(S_t, A_t, R_{t+1}, S_{t+1})$. \n",
    "The code is borrowed from [this repository](https://github.com/sfujim/TD3/blob/master/utils.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer( object ):\n",
    "\n",
    "    def __init__( self, n_state, n_action, max_size = 100000 ):\n",
    "\n",
    "        # Save the dimension of state, dimension of action and the maximum size of the replay buffer\n",
    "        self.n_state  = n_state\n",
    "        self.n_action = n_action\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # Defining the current size of the replay buffer, just to make the sampling easy. \n",
    "        self.current_size = 0\n",
    "\n",
    "        # Defining the Index Pointer (ptr) of the replaybuffer. \n",
    "        # This is required for \"adding\" the experiences on the replaybuffer. \n",
    "        self.idx_ptr      = 0\n",
    "\n",
    "        # Defining the 2D arrays of the ReplayBuffer\n",
    "        # 2D array definition is necessary to forward the Neural Network\n",
    "        self.states      = np.zeros( ( max_size, n_state   ) )\n",
    "        self.actions     = np.zeros( ( max_size, n_action  ) )\n",
    "        self.rewards     = np.zeros( ( max_size, 1         ) )\n",
    "        self.next_states = np.zeros( ( max_size, n_state   ) )\n",
    "        self.is_done     = np.zeros( ( max_size, 1         ) )\n",
    "\n",
    "\n",
    "    def add( self, state, action, reward, next_state, is_done ):\n",
    "        \"\"\"\n",
    "            Adding a state-action-reward-next_state pair into the ReplayBuffer. \n",
    "        \"\"\"\n",
    "\n",
    "        self.states[      self.idx_ptr ] = state\n",
    "        self.actions[     self.idx_ptr ] = action\n",
    "        self.rewards[     self.idx_ptr ] = reward\n",
    "        self.next_states[ self.idx_ptr ] = next_state\n",
    "        self.is_done[     self.idx_ptr ] = is_done\n",
    "\n",
    "        # Update our index pointer. Note that the \"oldest\" experiences are overwritten.\n",
    "        self.idx_ptr = ( self.idx_ptr + 1 ) % self.max_size\n",
    "\n",
    "        # Update the current size of the replay buffer\n",
    "        self.current_size = min( self.current_size + 1, self.max_size )\n",
    "\n",
    "    def sample( self, n_batch_size ):\n",
    "        \"\"\"\n",
    "            Collect \"n_batch_size\" samples from the replay buffer and return it as a batch.\n",
    "        \"\"\"\n",
    "        idx = np.random.randint( 0, self.current_size, size = n_batch_size )\n",
    "\n",
    "        # Returning the 2D numpy array as a 2D torch array.\n",
    "\n",
    "        return ( \n",
    "            torch.FloatTensor(      self.states[ idx ]  ).to( device ) ,\n",
    "            torch.FloatTensor(     self.actions[ idx ]  ).to( device ) , \n",
    "            torch.FloatTensor(     self.rewards[ idx ]  ).to( device ) ,\n",
    "            torch.FloatTensor( self.next_states[ idx ]  ).to( device ) ,\n",
    "            torch.FloatTensor(     self.is_done[ idx ]  ).to( device )   \n",
    "        )\n",
    "\n",
    "\n",
    "    def reset( self ):\n",
    "        \"\"\"\n",
    "            Reset all the replay buffers to zeros\n",
    "        \"\"\"\n",
    "        self.states      = np.zeros( ( self.max_size, self.n_state   ) )\n",
    "        self.actions     = np.zeros( ( self.max_size, self.n_action  ) )\n",
    "        self.rewards     = np.zeros( ( self.max_size, 1              ) )\n",
    "        self.next_states = np.zeros( ( self.max_size, self.n_state   ) )\n",
    "        self.is_done     = np.zeros( ( self.max_size, 1              ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ornstein-Uhlenbeck Process\n",
    "\n",
    "The Ornstein-Uhlenbeck Process generates noise that is correlated with the previous noise, as to prevent the noise from canceling out or “freezing” the overall dynamics. Adding this noise is mentioned in the [original paper](https://arxiv.org/abs/1509.02971). [Wikipedia](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) provides a thorough explanation of the Ornstein-Uhlenbeck Process. The source code is from [this](https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py). The Ornstein-Uhlenbeck process with an additional drift term can be described as:\n",
    "$$\n",
    "    dx_t = \\theta (\\mu - x_t)dt + \\sigma d W_t\n",
    "$$\n",
    "where $W_t$ denotes the [Wiener process](https://en.wikipedia.org/wiki/Wiener_process). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OUNoise( object ):\n",
    "    \n",
    "    def __init__( self, action_space, mu = 0.0, theta = 0.15, max_sigma = 0.3, min_sigma = 0.3, decay_period = 100000 ):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[ 0 ]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset( )\n",
    "        \n",
    "    def reset( self ):\n",
    "        # Caution! The state here is not the \"pendulum\"'s state, but the \"noise\" itself. \n",
    "        self.state = np.ones( self.action_dim ) * self.mu\n",
    "        \n",
    "    def step( self, t = 0 ):\n",
    "        \n",
    "        # Call the current noise value. \n",
    "        x  = self.state\n",
    "\n",
    "        # randn returns a sample from the standard (i.e., normal) distribution\n",
    "        dx = self.theta * ( self.mu - x ) + self.sigma * np.random.randn( self.action_dim )\n",
    "\n",
    "        # For our case, we simply set the max_sigma and min_sigma the same, hence the sigma value is constant for us\n",
    "        self.sigma = self.max_sigma - ( self.max_sigma - self.min_sigma ) * min( 1.0, t / self.decay_period )\n",
    "\n",
    "        # Time-increment. x_{n+1} = x_{n} + dx\n",
    "        self.state = x + dx\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    def add_noise2action( self, action, t = 0 ): \n",
    "        \n",
    "        # Calculate the noise with respect to the given time. \n",
    "        ou_noise   = self.step( t )\n",
    "\n",
    "        # Adding ou noise onto the action and then clipping it.\n",
    "        return np.clip( action + ou_noise, self.low, self.high )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Agent\n",
    "Note that when we develop the actor and critic networks, we also make the copy of those networks, called actor-target and critic-target networks. The details are again, explained in [this paper](https://arxiv.org/abs/1509.02971).\n",
    "\n",
    "The critic (or value) network is updated similarly as is done in Q-learning, where the target networks are employed. \n",
    "The loss function that are used for the policy gradient are defined as follows:\n",
    "$$\n",
    "    \\text{Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\big( y_i - Q( s_i, a_i | \\theta^{Q}) \\big )^2\n",
    "$$\n",
    "where $N$ is the sampled batch from the replay buffer. $y_i$ is defined as follows:\n",
    "$$\n",
    "    y_i = r_i + \\gamma Q'\\big( \\; s_{i+1}, \\; \\mu'(s_{i+1} | \\theta^{\\mu'} ) | \\theta^{Q'} \\big)\n",
    "$$\n",
    "where comma superscript stands for the target network parameters. \n",
    "\n",
    "The actor network is updated with the following equation:\n",
    "$$\n",
    "    J( \\theta ) = \\mathbb{E} \\big[ Q(s,a)|_{s=s_t, a = \\mu(s_t)} \\big]\n",
    "$$\n",
    "\n",
    "For the target networks, the parameters are \"soft\" updated with the following equations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\theta^{Q'}   & \\longleftarrow \\tau \\theta^{Q}   + ( 1 - \\tau ) \\theta^{Q'}   \\\\\n",
    "    \\theta^{\\mu'} & \\longleftarrow \\tau \\theta^{\\mu} + ( 1 - \\tau ) \\theta^{\\mu'} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\tau \\in [0,1)$ and we should choose a small value (for our example, $\\tau=0.01$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDPGagent( object ):\n",
    "\n",
    "    def __init__( self, n_state, n_action, max_action = 1., gamma = 0.99, tau = 0.005 ):\n",
    "\n",
    "        # Actor Network , its target (copy) Network, and the ADAM optimizer.\n",
    "        self.actor             = Actor( n_state, n_action, max_action = max_action ).to( device )\n",
    "        self.actor_target      = copy.deepcopy( self.actor )\n",
    "        self.actor_optimizer   = optim.Adam(  self.actor.parameters( ), lr = 1e-4 )\n",
    "\n",
    "        # Critic Network, its target (copy) Network, and the ADAM optimizer.\n",
    "        self.critic            = Critic( n_state, n_action )\n",
    "        self.critic_target     = copy.deepcopy( self.critic )\n",
    "        self.critic_optimizer  = optim.Adam(  self.critic.parameters( ), lr = 1e-3 )\n",
    "\n",
    "        # The discount factor gamma and the soft-update gain tau\n",
    "        self.gamma = gamma\n",
    "        self.tau   = tau\n",
    "\n",
    "        # The maximum action.\n",
    "        self.max_action = max_action\n",
    "\n",
    "    \n",
    "    def get_action( self, state ):\n",
    "\n",
    "        # Conduct the a = mu(s), where mu is a \"deterministic function\"\n",
    "        # Unsqueeze makes an 1 x n_s array of state. \n",
    "        state  = torch.from_numpy( state ).float( ).unsqueeze( 0 ).to( device )\n",
    "\n",
    "        # Returns an 1 x n_a array of state\n",
    "        # forward method can be omitted\n",
    "        action = self.actor( state )\n",
    "\n",
    "        # Change action from Torch to Numpy.\n",
    "        # Since n_a is 1 for this case, action is simply an 1x1 array.\n",
    "        # Hence, flattening the data. \n",
    "        action = action.cpu( ).data.numpy( ).flatten( )\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update( self, replay_buffer, batch_size = 256 ):\n",
    "        \"\"\"\n",
    "            Mini-batch update. \n",
    "        \"\"\"\n",
    "        # Randomly sample batch_size numbers of S A R S.\n",
    "        states, actions, rewards, next_states, is_done = replay_buffer.sample( batch_size )\n",
    "\n",
    "        # ====================================================== #\n",
    "        # ================ Critic Optimizer Part =============== #\n",
    "        # ====================================================== #\n",
    "        # Keep in mind that the original paper first optimizes the Critic network\n",
    "\n",
    "        # Critic loss \n",
    "        Qprime  = self.critic_target( next_states, self.actor_target( next_states ) )\n",
    "        Qprime  = rewards + ( ( 1. - is_done ) * self.gamma * Qprime ).detach( )\n",
    "        Q       = self.critic( states,actions )\n",
    "\n",
    "        critic_loss  = F.mse_loss( Q, Qprime )\n",
    "\n",
    "        # Update Critic network\n",
    "        self.critic_optimizer.zero_grad( )\n",
    "        critic_loss.backward( ) \n",
    "        self.critic_optimizer.step( )\n",
    "\n",
    "        # ====================================================== #\n",
    "        # ================ Actor Optimizer Part ================ #\n",
    "        # ====================================================== #\n",
    "        # Actor loss, it is simply the mean of the Q function \n",
    "        # The Q function value Q( s, a ) is actually Q( s, mu( s ) ), hence the Q function is described as the parameters of mu (actor).\n",
    "        # Since a is a continuous function, we can compute its gradient.   \n",
    "        actor_loss = - self.critic( states, self.actor( states ) ).mean( )\n",
    "        \n",
    "        # Update (Optimize) Actor network\n",
    "        self.actor_optimizer.zero_grad( )\n",
    "        actor_loss.backward( )\n",
    "        self.actor_optimizer.step( )\n",
    "\n",
    "\n",
    "        # The \"soft\" update of target networks\n",
    "        for target_param, param in zip( self.actor_target.parameters( ), self.actor.parameters( ) ):\n",
    "            target_param.data.copy_( param.data * self.tau + target_param.data * ( 1.0 - self.tau ) )\n",
    "       \n",
    "        for target_param, param in zip( self.critic_target.parameters( ), self.critic.parameters( ) ):\n",
    "            target_param.data.copy_( param.data * self.tau + target_param.data * ( 1.0 - self.tau ) )\n",
    "\n",
    "\n",
    "    def save( self, filename ):\n",
    "        \n",
    "        torch.save( self.critic.state_dict( )           , filename + \"_critic\"              )\n",
    "        torch.save( self.critic_optimizer.state_dict( ) , filename + \"_critic_optimizer\"    )\n",
    "        \n",
    "        torch.save( self.actor.state_dict( )            , filename + \"_actor\"               )\n",
    "        torch.save( self.actor_optimizer.state_dict( )  , filename + \"_actor_optimizer\"     )\n",
    "\n",
    "\n",
    "    def load( self, filename ):\n",
    "\n",
    "        # Load Critic\n",
    "        self.critic.load_state_dict(            torch.load( filename + \"_critic\"           )  )\n",
    "        self.critic_optimizer.load_state_dict(  torch.load( filename + \"_critic_optimizer\" )  )\n",
    "        self.critic_target = copy.deepcopy( self.critic )\n",
    "\n",
    "        # Load Actor\n",
    "        self.actor.load_state_dict(             torch.load( filename + \"_actor\"            )  )\n",
    "        self.actor_optimizer.load_state_dict(   torch.load( filename + \"_actor_optimizer\"  )  )\n",
    "        self.actor_target = copy.deepcopy( self.actor )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instances of the environment, DDPG agent and the OU noise.\n",
    "env   = gym.make( \"Pendulum-v1\" ) \n",
    "\n",
    "# Set the random seeds\n",
    "env.seed(              round( time.time( ) ) )\n",
    "env.action_space.seed( round( time.time( ) ) )\n",
    "torch.manual_seed(     round( time.time( ) ) )\n",
    "np.random.seed(        round( time.time( ) ) )\n",
    "\n",
    "# Get the dimension of states and actions, and also the \n",
    "# [WARNING] This is for environments where we assume the mean of action is 0. \n",
    "n_state    = env.observation_space.shape[ 0 ] \n",
    "n_action   = env.action_space.shape[ 0 ]\n",
    "max_action = float( env.action_space.high  )\n",
    "\n",
    "# Define the agent, noise and replay buffers\n",
    "agent         = DDPGagent( n_state, n_action, max_action )\n",
    "OUnoise       = OUNoise( env.action_space )\n",
    "replay_buffer = ReplayBuffer( n_state, n_action )\n",
    "\n",
    "# The number of \"batch\" that will be sampled from the replay buffer will be \"batch_size\" \n",
    "n_batch_size  = 256\n",
    "\n",
    "# Saving these values to plot the performance at the end.\n",
    "frames        = [ ]\n",
    "whole_rewards = [ ]\n",
    "\n",
    "# Flags for turning on or off the render.\n",
    "is_save_video = False\n",
    "is_save_model = True\n",
    "\n",
    "# For the pendulum model the best reward is 0, hence saving a -infinity value. \n",
    "best_model_val = -np.inf\n",
    "\n",
    "rewards       = [ ]\n",
    "avg_rewards   = [ ]\n",
    "\n",
    "for episode in range( 500 ):\n",
    "\n",
    "    # Initialize the gym environment and OU noise \n",
    "    state = env.reset()\n",
    "    OUnoise.reset( )\n",
    "\n",
    "    # Initialize the episode's reward\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # For pendulum v1 gym, a single simulation is maximum 200-steps long. \n",
    "    # [REF] https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py\n",
    "    for step in range( 200 ):\n",
    "\n",
    "        # Get the action value from the deterministic policy Actor network.\n",
    "        action = agent.get_action( state )\n",
    "\n",
    "        if is_save_video : frames.append( env.render( mode = 'rgb_array' ) )\n",
    "\n",
    "        # Apply the OU noise on this action\n",
    "        action = OUnoise.add_noise2action( action, step )\n",
    "\n",
    "        # Run a single step of simulation\n",
    "        new_state, reward, done, _ = env.step( action )  \n",
    "\n",
    "        # Add this to our replay buffer, note that push simply generates the tuple and add \n",
    "        replay_buffer.add( state, action, reward, new_state, done )\n",
    "        \n",
    "        # Once the agent memory is full, then update the policy via replay buffer.\n",
    "        if replay_buffer.current_size > n_batch_size: agent.update( replay_buffer, batch_size = n_batch_size )        \n",
    "        \n",
    "        # Update the state and reward value \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if best_model_val <= episode_reward:\n",
    "        best_model_val = episode_reward \n",
    "\n",
    "        # If this policy has a good result, save it \n",
    "        if is_save_model: agent.save( \"../models/DDPG_best_model\" ) \n",
    "\n",
    "    # Once a single simulation is done, append the values that will be plotted later\n",
    "    rewards.append( episode_reward )\n",
    "    avg_rewards.append( np.mean( rewards[ -10 : ] ) )\n",
    "\n",
    "    sys.stdout.write(\"episode: {}, reward: {}, average_reward: {} \\n\".format( episode, np.round( episode_reward, decimals = 2 ), avg_rewards[ -1 ] ) ) \n",
    "\n",
    "whole_rewards.append(  rewards  )\n",
    "\n",
    "env.close( )\n",
    "\n",
    "if is_save_video:\n",
    "    clip = mpy.ImageSequenceClip( frames, fps = 30 )\n",
    "    clip.write_gif( \"../videos/DDPG.gif\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot( rewards     )\n",
    "plt.plot( avg_rewards )\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_save_video = True\n",
    "\n",
    "env   = gym.make( \"Pendulum-v1\" )  \n",
    "\n",
    "# Set the random seeds\n",
    "env.seed(              round( time.time( ) ) )\n",
    "env.action_space.seed( round( time.time( ) ) )\n",
    "\n",
    "\n",
    "# Get the dimension of states and actions, and also the \n",
    "# [WARNING] This is for environments where we assume the mean of action is 0. \n",
    "n_state    = env.observation_space.shape[ 0 ] \n",
    "n_action   = env.action_space.shape[ 0 ]\n",
    "max_action = float( env.action_space.high  )\n",
    "\n",
    "model_path = \"../models/DDPG_best_model\"\n",
    "agent      = DDPGagent( n_state, n_action, max_action )\n",
    "agent.load( model_path )\n",
    "\n",
    "# Run trial\n",
    "state = env.reset()   \n",
    "\n",
    "frames = []\n",
    "# The maximum trial is 200 for the pendulum \n",
    "for _ in range( 200 ):\n",
    "\n",
    "    if is_save_video: frames.append( env.render( mode = \"rgb_array\")  ) \n",
    "    env.render( )\n",
    "\n",
    "    # Get the choice of action and the pi( a_t | s_t ) for the gradient calculation\n",
    "    action = agent.get_action( state )\n",
    "    new_state, _, done, _ = env.step( action )\n",
    "\n",
    "    # If the trail encounters the terminal state\n",
    "    if done: \n",
    "        break\n",
    "    state = new_state\n",
    "\n",
    "# Save Video\n",
    "env.close( )\n",
    "\n",
    "if is_save_video:\n",
    "    clip = mpy.ImageSequenceClip( frames, fps = 30 )\n",
    "    clip.write_gif( \"../videos/DDPG.gif\" )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742fb12a3d29bb1ab201ca06dfc1cb9996440041728c355377b69362e9fdf7ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
