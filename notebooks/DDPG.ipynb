{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradient (DDPG)\n",
    "\n",
    "We introduce a python code for Deep Deterministic Policy Gradient (DDPG). Compared to methods using stochastic policy, where from given state $s$ the policy is defined as a probability distribution $\\pi(\\cdot|s)$ and the algorithm chooses one of the action based on that distribution, deterministic policy $\\mu(s)$ provides a specific value from the given state $s$, hence called **deterministic**.  The paper can be found [here](https://arxiv.org/abs/1509.02971), and the code is modified from [This website](https://towardsdatascience.com/deep-deterministic-policy-gradients-explained-2d94655a9b7b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim         as optim\n",
    "import matplotlib.pyplot   as plt\n",
    "\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor and Critic Network (Separate)\n",
    "\n",
    "For the DDPG algorithm, we split the Actor and Critic networks\n",
    "\n",
    "## Critic \n",
    "The critic has four layers, and the critic learns the Q-function, $Q(s,a)$, hence the input is a concatenation of state and action, with a scalar output. \n",
    "\n",
    "## Actor\n",
    "The actor network also has four layers, and it gets state vector as input and returns an action $a=\\mu(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic( nn.Module ):\n",
    "\n",
    "    def __init__( self, input_size, hidden_size, output_size ):\n",
    "\n",
    "        super( Critic, self ).__init__()\n",
    "\n",
    "        # First Layer, changes array with size N x ( input_size ) to N x ( hidden_size )\n",
    "        self.linear1 = nn.Linear(  input_size, hidden_size )\n",
    "\n",
    "        # Second Layer, changes array with size N x ( hidden_size ) to N x ( hidden_size )\n",
    "        self.linear2 = nn.Linear( hidden_size, hidden_size )\n",
    "\n",
    "        # Third Layer, changes array with size N x ( hidden_size ) to N x ( output_size )\n",
    "        self.linear3 = nn.Linear( hidden_size, output_size )\n",
    "\n",
    "    \n",
    "    def forward( self, state, action ):\n",
    "\n",
    "        # Concatenation of state and action vector.\n",
    "        # The state  is assumed to be a 2D array with size N x n_s, where N is the number of samples\n",
    "        # The action is assumed to be a 2D array with size N x n_a, where N is the number of samples\n",
    "        # As a result of torch.cat( [ state, action ] along axis 1, ), we have size N x ( n_s + n_a ), and the dim = 0 must have the same size\n",
    "        x = torch.cat( [ state, action ], dim = 1 )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.linear1( x ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.linear2( x ) )\n",
    "\n",
    "        # A simple Ax + b combination \n",
    "        x = self.linear3( x )\n",
    "\n",
    "        # The output is a N x 1 array. \n",
    "        return x\n",
    "\n",
    "class Actor( nn.Module ):\n",
    "\n",
    "    def __init__( self, input_size, hidden_size, output_size ):\n",
    "\n",
    "        super( Actor, self ).__init__( )\n",
    "\n",
    "        # First Layer, changes array with size N x ( input_size ) to N x ( hidden_size )\n",
    "        self.linear1 = nn.Linear(  input_size, hidden_size )\n",
    "\n",
    "        # Second Layer, changes array with size N x ( hidden_size ) to N x ( hidden_size )\n",
    "        self.linear2 = nn.Linear( hidden_size, hidden_size )\n",
    "\n",
    "        # Third Layer, changes array with size N x ( hidden_size ) to N x ( output_size )\n",
    "        self.linear3 = nn.Linear( hidden_size, output_size )\n",
    "        \n",
    "    def forward( self, state ):\n",
    "        \n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.linear1( state ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.linear2( x ) )\n",
    "\n",
    "        # Applying to tanh, which ranges the value from -1 to +1\n",
    "        x = torch.tanh( self.linear3( x ) ) \n",
    "\n",
    "        return x\n",
    "\n",
    "actor1 = Actor( 3, 128, 1 )\n",
    "\n",
    "tmp1 = actor1.forward( torch.FloatTensor( np.array( [1., 3., 3.] )  ) )\n",
    "\n",
    "#tmp1 = actor1.forward( torch.FloatTensor( np.array( [1., 3., 3.] )  ).unsqueeze( 0 ) )\n",
    "tmp1.detach( ).numpy( ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "As in Deep Q-Network (DQN), we construct a replay buffer. As mentioned in [this paper](https://arxiv.org/abs/1509.02971), optimization algorithms assume that the samples are independently and identically distributed, and the replay buffer addresses that problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory( object ):\n",
    "\n",
    "    def __init__( self, max_size ):\n",
    "        # deque is used to \"forget\" the values of the other end.\n",
    "        # Meaning, for a deque with full capacity, if we append a new experience (to the right end) the element on the other end (i.e., the left end) is discarded. \n",
    "        self.buffer = deque( maxlen = max_size )\n",
    "    \n",
    "    def push( self, state, action, reward, next_state ):\n",
    "        \n",
    "        # We are making the reward as an array. \n",
    "        # Each element of the tuple has the following size:\n",
    "        #      state: n_s\n",
    "        #     action: n_a\n",
    "        #     reward: 1 (but need to change from scalar to array)\n",
    "        # next_state: n_s\n",
    "        \n",
    "        experience = ( state, action, np.array( [ reward ] ), next_state )\n",
    "\n",
    "        # Insert the tuple of (S A R S) into to the right end of the buffer deque.\n",
    "        self.buffer.append( experience )\n",
    "\n",
    "    def sample( self, batch_size, type = 0 ):\n",
    "        \"\"\"\n",
    "            Collect \"batch_size\" samples from the replay buffer and return it as a batch.\n",
    "        \"\"\"\n",
    "        # The sample is simply an array of \n",
    "        # Sn An Rn+1 Sn+1\n",
    "        # Where S, A and R are variables that are self-explanatory.  \n",
    "\n",
    "        # Sample batch_size amount of list from the buffer\n",
    "        # Returning a list of tuples\n",
    "        \n",
    "        batch = random.sample( self.buffer, batch_size ) \n",
    "\n",
    "        # The computation speed of Type 0 and 1 are quite similar. \n",
    "        if type == 0:\n",
    "            \n",
    "            # t_start = time.time( )\n",
    "\n",
    "            state_batch      = [ ]\n",
    "            action_batch     = [ ]\n",
    "            reward_batch     = [ ]\n",
    "            next_state_batch = [ ] \n",
    "\n",
    "            for experience in batch:\n",
    "                \n",
    "                state, action, reward, next_state = experience\n",
    "\n",
    "                state_batch.append(       state      )\n",
    "                action_batch.append(      action     )\n",
    "                reward_batch.append(      reward     )\n",
    "                next_state_batch.append(  next_state )\n",
    "\n",
    "            # print( \"[Method 1]\", time.time( ) - t_start)  \n",
    "            \n",
    "            \n",
    "        elif type == 1:\n",
    "\n",
    "            # t_start = time.time( )\n",
    "\n",
    "            # [REF] https://stackoverflow.com/questions/8081545/how-to-convert-list-of-tuples-to-multiple-lists\n",
    "            tmp = list( zip( *batch ) )\n",
    "\n",
    "            state_batch, action_batch, reward_batch, next_state_batch = tmp[ : ]\n",
    "\n",
    "            # print( \"[Method 2] \", time.time( ) - t_start ) \n",
    "\n",
    "        return state_batch, action_batch, reward_batch, next_state_batch\n",
    "        \n",
    "    def __len__( self ):\n",
    "        return len( self.buffer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Ornstein-Uhlenbeck Process\n",
    "\n",
    "The Ornstein-Uhlenbeck Process generates noise that is correlated with the previous noise, as to prevent the noise from canceling out or “freezing” the overall dynamics. Adding this noise is mentioned in the [original paper](https://arxiv.org/abs/1509.02971). [Wikipedia](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) provides a thorough explanation of the Ornstein-Uhlenbeck Process. The source code is from [this](https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py). The Ornstein-Uhlenbeck process with an additional drift term can be described as:\n",
    "$$\n",
    "    dx_t = \\theta (\\mu - x_t)dt + \\sigma d W_t\n",
    "$$\n",
    "where $W_t$ denotes the [Wiener process](https://en.wikipedia.org/wiki/Wiener_process). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OUNoise( object ):\n",
    "    \n",
    "    def __init__( self, action_space, mu = 0.0, theta = 0.15, max_sigma = 0.3, min_sigma = 0.3, decay_period = 100000 ):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[ 0 ]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset( self ):\n",
    "        # Caution! The state here is not the \"pendulum\"'s state, but the \"noise\" itself. \n",
    "        self.state = np.ones( self.action_dim ) * self.mu\n",
    "        \n",
    "    def evolve_state( self ):\n",
    "        \n",
    "        # Call the current noise value. \n",
    "        x  = self.state\n",
    "\n",
    "        # randn returns a sample from the standard (i.e., normal) distribution\n",
    "        dx = self.theta * ( self.mu - x ) + self.sigma * np.random.randn( self.action_dim )\n",
    "\n",
    "        # Time-increment. x_{n+1} = x_{n} + dx\n",
    "        self.state = x + dx\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    def get_action( self, action, t = 0 ): \n",
    "        \n",
    "        # Calculate the noise with respect to the given time. \n",
    "        ou_state   = self.evolve_state( )\n",
    "\n",
    "        # For our case, we simply set the max_sigma and min_sigma the same, hence the sigma value is constant for us\n",
    "        self.sigma = self.max_sigma - ( self.max_sigma - self.min_sigma ) * min( 1.0, t / self.decay_period )\n",
    "\n",
    "        # Adding ou noise onto the action and then clipping it.\n",
    "        return np.clip( action + ou_state, self.low, self.high )\n",
    "\n",
    "\n",
    "class NormalizedEnv( gym.ActionWrapper ):\n",
    "    \"\"\" \n",
    "        The pendulum v1's action min/max are -2/+2, respectively. \n",
    "        But the action output of tanh is -1 to +1, hence we need scale the action values between range [-2, +2] and [-1, +1]\n",
    "\n",
    "        [REF] https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py\n",
    "        [REF] https://github.com/openai/gym/blob/master/gym/core.py\n",
    "        [REF] https://www.gymlibrary.ml/content/wrappers/\n",
    "    \"\"\"\n",
    " \n",
    "    def action( self, act ):\n",
    "        \"\"\"\n",
    "            Action range must change from [-1, +1] (The actor network's output) to [-2, +2], which will be the input to the gym.\n",
    "            Hence, simply multiply 2. Note that this method does not work for other gym environments, where the range differs. \n",
    "        \"\"\"\n",
    "\n",
    "        return 2 * act\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Agent\n",
    "Note that when we develop the actor and critic networks, we also make the copy of those networks, called actor-target and critic-target networks. The details are again, explained in [this paper](https://arxiv.org/abs/1509.02971).\n",
    "\n",
    "The critic (or value) network is updated similarly as is done in Q-learning, where the target networks are employed. \n",
    "The loss function that are used for the policy gradient are defined as follows:\n",
    "$$\n",
    "    \\text{Loss} = \\frac{1}{N} \\sum_{i=1}^{N} \\big( y_i - Q( s_i, a_i | \\theta^{Q}) \\big )^2\n",
    "$$\n",
    "where $N$ is the sampled batch from the replay buffer. $y_i$ is defined as follows:\n",
    "$$\n",
    "    y_i = r_i + \\gamma Q'\\big( \\; s_{i+1}, \\; \\mu'(s_{i+1} | \\theta^{\\mu'} ) | \\theta^{Q'} \\big)\n",
    "$$\n",
    "where comma superscript stands for the target network parameters. \n",
    "\n",
    "The actor network is updated with the following equation:\n",
    "$$\n",
    "    J( \\theta ) = \\mathbb{E} \\big[ Q(s,a)|_{s=s_t, a = \\mu(s_t)} \\big]\n",
    "$$\n",
    "\n",
    "For the target networks, the parameters are \"soft\" updated with the following equations:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\theta^{Q'}   & \\longleftarrow \\tau \\theta^{Q}   + ( 1 - \\tau ) \\theta^{Q'}   \\\\\n",
    "    \\theta^{\\mu'} & \\longleftarrow \\tau \\theta^{\\mu} + ( 1 - \\tau ) \\theta^{\\mu'} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\tau \\in [0,1)$ and we should choose a small value (for our example, $\\tau=0.01$ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DDPGagent( object ):\n",
    "    def __init__( self, env, hidden_size = 256, actor_learning_rate = 1e-4, critic_learning_rate = 1e-3, gamma = 0.99, tau = 1e-2, max_memory_size = 50000 ):\n",
    "\n",
    "        # The n_s and n_a of the system\n",
    "        self.n_states  = env.observation_space.shape[ 0 ]\n",
    "        self.n_actions = env.action_space.shape[ 0 ]\n",
    "\n",
    "        # Actor Network and its target Network\n",
    "        self.actor        = Actor( self.n_states, hidden_size, self.n_actions )\n",
    "        self.actor_target = Actor( self.n_states, hidden_size, self.n_actions )\n",
    "\n",
    "        # Critic Network and its target Network\n",
    "        self.critic        = Critic( self.n_states + self.n_actions, hidden_size, self.n_actions )\n",
    "        self.critic_target = Critic( self.n_states + self.n_actions, hidden_size, self.n_actions )\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau   = tau\n",
    "\n",
    "        for target_param, param in zip( self.actor_target.parameters( ), self.actor.parameters( ) ):\n",
    "\n",
    "            # Copy the parameters of the main actor parameters to the target parameters\n",
    "            target_param.data.copy_( param.data )\n",
    "\n",
    "        for target_param, param in zip( self.critic_target.parameters( ), self.critic.parameters( ) ):\n",
    "\n",
    "            # Copy the parameters of the main critic parameters to the target parameters\n",
    "            target_param.data.copy_( param.data )\n",
    "        \n",
    "        # Construct the replay-buffer. \n",
    "        self.memory = Memory( max_memory_size )      \n",
    "\n",
    "        # Define the optimizers and the loss function. \n",
    "        self.critic_criterion  = nn.MSELoss( )\n",
    "        self.actor_optimizer   = optim.Adam(  self.actor.parameters( ), lr =  actor_learning_rate )\n",
    "        self.critic_optimizer  = optim.Adam( self.critic.parameters( ), lr = critic_learning_rate )\n",
    "    \n",
    "    def get_action( self, state ):\n",
    "\n",
    "        # Conduct the a = mu(s), where mu is a \"deterministic function\"\n",
    "        # Unsqueeze makes an 1 x n_s array of state. \n",
    "        state  = torch.from_numpy( state ).float( ).unsqueeze( 0 )\n",
    "\n",
    "        # Returns an 1 x n_a array of state\n",
    "        action = self.actor.forward( state )\n",
    "\n",
    "        # Change action from Torch to Numpy.\n",
    "        # Since n_a is 1 for this case, action is simply an 1x1 array.\n",
    "        # Thus, choosing the ( 0, 0 )th element for the action\n",
    "        action = action.detach( ).numpy( )[ 0,0 ]\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update( self, batch_size ):\n",
    "        \"\"\"\n",
    "            Mini-batch update. \n",
    "        \"\"\"\n",
    "        # Randomly sample batch_size numbers of S A R S.\n",
    "        states, actions, rewards, next_states = self.memory.sample( batch_size , type = 0 )\n",
    "\n",
    "        # Creating a tensor from a list of numpy.ndarrays is extremely slow. \n",
    "        # Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. \n",
    "        # A list with N of Ns-size arrays will become an N x Ns 2D array, to tensor with \"float\" data. \n",
    "        states      = torch.FloatTensor( np.array( states      )  )\n",
    "        actions     = torch.FloatTensor( np.array( actions     )  )\n",
    "        rewards     = torch.FloatTensor( np.array( rewards     )  )\n",
    "        next_states = torch.FloatTensor( np.array( next_states )  )\n",
    "    \n",
    "\n",
    "        # Critic loss \n",
    "        Qvals        = self.critic.forward( states, actions )\n",
    "        next_actions = self.actor_target.forward( next_states )\n",
    "        next_Q       = self.critic_target.forward( next_states, next_actions.detach( ) )\n",
    "        Qprime       = rewards + self.gamma * next_Q\n",
    "\n",
    "        critic_loss  = self.critic_criterion( Qvals, Qprime )\n",
    "\n",
    "        # Actor loss, it is simply the mean of the Q function \n",
    "        policy_loss = - self.critic.forward( states, self.actor.forward( states ) ).mean( )\n",
    "        \n",
    "        # Update Actor network\n",
    "        self.actor_optimizer.zero_grad( )\n",
    "        policy_loss.backward( )\n",
    "        self.actor_optimizer.step( )\n",
    "\n",
    "        # Update Critic network\n",
    "        self.critic_optimizer.zero_grad( )\n",
    "        critic_loss.backward( ) \n",
    "        self.critic_optimizer.step( )\n",
    "\n",
    "        # The \"soft\" update of target networks\n",
    "        for target_param, param in zip( self.actor_target.parameters( ), self.actor.parameters( ) ):\n",
    "            target_param.data.copy_( param.data * self.tau + target_param.data * ( 1.0 - self.tau ) )\n",
    "       \n",
    "        for target_param, param in zip( self.critic_target.parameters( ), self.critic.parameters( ) ):\n",
    "            target_param.data.copy_( param.data * self.tau + target_param.data * ( 1.0 - self.tau ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instances of the environment, DDPG agent and the OU noise.\n",
    "env   = NormalizedEnv( gym.make( \"Pendulum-v1\" )  )\n",
    "agent = DDPGagent( env )\n",
    "noise = OUNoise( env.action_space )\n",
    "\n",
    "# The number of \"batch\" that will be sampled from the replay buffer will be \"batch_size\" \n",
    "batch_size  = 128\n",
    "\n",
    "# Saving these values to plot the performance at the end.\n",
    "rewards     = [ ]\n",
    "avg_rewards = [ ]\n",
    "\n",
    "# Flags for turning on or off the render.\n",
    "is_save_video = False\n",
    "\n",
    "\n",
    "for episode in range( 3000 ):\n",
    "\n",
    "    # Initialize the gym environment and OU noise \n",
    "    state = env.reset()\n",
    "    noise.reset( )\n",
    "\n",
    "    # Initialize the episode's reward\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # A single simulation is 500-steps long. \n",
    "    for step in range( 500 ):\n",
    "\n",
    "        # Get the action value from the deterministic policy Actor network.\n",
    "        action = agent.get_action( state )\n",
    "\n",
    "        if is_render: env.render( )\n",
    "\n",
    "        # Apply the OU noise on this action\n",
    "        action = noise.get_action( action, step )\n",
    "\n",
    "        # Run a single step of simulation\n",
    "        new_state, reward, done, _ = env.step( action )  \n",
    "\n",
    "        # Add this to our replay buffer, note that push simply generates the tuple and add \n",
    "        agent.memory.push( state, action, reward, new_state )\n",
    "        \n",
    "        # Once the agent memory is full, then update the policy via replay buffer.\n",
    "        if len( agent.memory ) > batch_size: agent.update( batch_size )        \n",
    "        \n",
    "        # Update the state and reward value \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            sys.stdout.write(\"episode: {}, reward: {}, average _reward: {} \\n\".format( episode, np.round( episode_reward, decimals = 2 ), np.mean( rewards[ -10: ] ) ) ) \n",
    "            break\n",
    "    \n",
    "    # Once a single simulation is done, append the values that will be plotted later\n",
    "    rewards.append( episode_reward )\n",
    "    avg_rewards.append( np.mean( rewards[ -10 : ] ) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot( rewards     )\n",
    "plt.plot( avg_rewards )\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742fb12a3d29bb1ab201ca06dfc1cb9996440041728c355377b69362e9fdf7ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
