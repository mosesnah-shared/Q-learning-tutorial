{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twin Delayed Deep Deterministic Policy Gradients (TD3) \n",
    "\n",
    "This notebook presents the [Twin Delayed Deep Deterministic Policy Gradients (a.k.a., TD3)](https://arxiv.org/pdf/1802.09477.pdf), which is an upgraded version of [Deep Deterministic Policy Gradients (DDPG)](https://arxiv.org/pdf/1509.02971.pdf). The code is from [this repository](https://github.com/sfujim/TD3/blob/master/TD3.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import copy\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F \n",
    "import torch.optim         as optim\n",
    "import numpy               as np\n",
    "import matplotlib.pyplot   as plt\n",
    "import moviepy.editor      as mpy\n",
    "\n",
    "# Check whether GPU computation (i.e., CUDA) is available.\n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available( ) else \"cpu\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer( object ):\n",
    "\n",
    "    def __init__( self, n_state, n_action, max_size = 100000 ):\n",
    "\n",
    "        # Save the dimension of state, dimension of action and the maximum size of the replay buffer\n",
    "        self.n_state  = n_state\n",
    "        self.n_action = n_action\n",
    "        self.max_size = max_size\n",
    "\n",
    "        # Defining the current size of the replay buffer, just to make the sampling easy. \n",
    "        self.current_size = 0\n",
    "\n",
    "        # Defining the Index Pointer (ptr) of the replaybuffer. \n",
    "        # This is required for \"adding\" the experiences on the replaybuffer. \n",
    "        self.idx_ptr      = 0\n",
    "\n",
    "        # Defining the 2D arrays of the ReplayBuffer\n",
    "        # 2D array definition is necessary to forward the Neural Network\n",
    "        self.states      = np.zeros( ( max_size, n_state   ) )\n",
    "        self.actions     = np.zeros( ( max_size, n_action  ) )\n",
    "        self.rewards     = np.zeros( ( max_size, 1         ) )\n",
    "        self.next_states = np.zeros( ( max_size, n_state   ) )\n",
    "        self.is_done     = np.zeros( ( max_size, 1         ) )\n",
    "\n",
    "\n",
    "    def add( self, state, action, reward, next_state, is_done ):\n",
    "        \"\"\"\n",
    "            Adding a state-action-reward-next_state pair into the ReplayBuffer. \n",
    "        \"\"\"\n",
    "\n",
    "        self.states[      self.idx_ptr ] = state\n",
    "        self.actions[     self.idx_ptr ] = action\n",
    "        self.rewards[     self.idx_ptr ] = reward\n",
    "        self.next_states[ self.idx_ptr ] = next_state\n",
    "        self.is_done[     self.idx_ptr ] = is_done\n",
    "\n",
    "        # Update our index pointer. Note that the \"oldest\" experiences are overwritten.\n",
    "        self.idx_ptr = ( self.idx_ptr + 1 ) % self.max_size\n",
    "\n",
    "        # Update the current size of the replay buffer\n",
    "        self.current_size = min( self.current_size + 1, self.max_size )\n",
    "\n",
    "    def sample( self, n_batch_size ):\n",
    "        \"\"\"\n",
    "            Collect \"n_batch_size\" samples from the replay buffer and return it as a batch.\n",
    "        \"\"\"\n",
    "        idx = np.random.randint( 0, self.current_size, size = n_batch_size )\n",
    "\n",
    "        # Returning the 2D numpy array as a 2D torch array.\n",
    "\n",
    "        return ( \n",
    "            torch.FloatTensor(      self.states[ idx ]  ).to( device ) ,\n",
    "            torch.FloatTensor(     self.actions[ idx ]  ).to( device ) , \n",
    "            torch.FloatTensor(     self.rewards[ idx ]  ).to( device ) ,\n",
    "            torch.FloatTensor( self.next_states[ idx ]  ).to( device ) ,\n",
    "            torch.FloatTensor(     self.is_done[ idx ]  ).to( device )   \n",
    "        )\n",
    "\n",
    "\n",
    "    def reset( self ):\n",
    "        \"\"\"\n",
    "            Reset all the replay buffers to zeros\n",
    "        \"\"\"\n",
    "        self.states      = np.zeros( ( self.max_size, self.n_state   ) )\n",
    "        self.actions     = np.zeros( ( self.max_size, self.n_action  ) )\n",
    "        self.rewards     = np.zeros( ( self.max_size, 1              ) )\n",
    "        self.next_states = np.zeros( ( self.max_size, self.n_state   ) )\n",
    "        self.is_done     = np.zeros( ( self.max_size, 1              ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor and Critic Networks\n",
    "As with the DDPG algorithm, TD3 also has two separate Actor and Critic Networks. The only difference is the critic of TD3 method has two separate neural networks to get two Q values, $Q_1$ and $Q_2$. The TD3 algorithm is inspired from Double Q-learning, which is the reason why we have $Q_1$ and $Q_2$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor( nn.Module ):\n",
    "    def __init__( self, n_state: int, n_action: int, n_hidden: int = 256, max_action: float = 1.0 ):\n",
    "\n",
    "        # Class inheritance. \n",
    "        super( Actor, self ).__init__( )\n",
    "\n",
    "        # Save the maximum action value \n",
    "        assert max_action >= 0\n",
    "        self.max_action = max_action\n",
    "\n",
    "        # First Layer, changes array  with size N x ( n_state  ) to N x ( n_hidden )\n",
    "        self.l1 = nn.Linear(  n_state, n_hidden )\n",
    "\n",
    "        # Second Layer, changes array with size N x ( n_hidden ) to N x ( n_hidden )\n",
    "        self.l2 = nn.Linear( n_hidden, n_hidden )\n",
    "\n",
    "        # Third Layer, changes array  with size N x ( n_hidden ) to N x ( n_action )\n",
    "        self.l3 = nn.Linear( n_hidden, n_action )\n",
    "        \n",
    "    def forward( self, state ):\n",
    "        \n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l1( state ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        x = F.relu( self.l2( x ) )\n",
    "\n",
    "        # Applying to tanh, which ranges the value from -1 to +1\n",
    "        x = torch.tanh( self.l3( x ) ) \n",
    "\n",
    "        # Since the x value is from -1 to +1, we change the range to -max_action to +max_action.\n",
    "        return x * self.max_action\n",
    "\n",
    "class Critic( nn.Module ):\n",
    "    \"\"\"\n",
    "        Learning the Q(s,a) function, which is the \"Quality\" function. Hence, input is a concatenation of state, action and the output is a scalar. \n",
    "    \"\"\"\n",
    "    def __init__( self, n_state, n_action, n_hidden: int = 256 ):\n",
    "\n",
    "        # Class inheritance. \n",
    "        super( Critic, self ).__init__()\n",
    "\n",
    "\n",
    "        # ================================================================================ #\n",
    "        # ================================= First Q-Network ============================== #\n",
    "        # ================================================================================ #\n",
    "        # First Layer, changes array with size N x ( n_state + n_action ) to N x ( n_hidden )\n",
    "        self.l1 = nn.Linear( n_state + n_action, n_hidden )\n",
    "\n",
    "        # Second Layer, changes array with size N x ( n_hidden ) to N x ( n_hidden )\n",
    "        self.l2 = nn.Linear( n_hidden, n_hidden )\n",
    "\n",
    "        # Third Layer, changes array with size N x ( n_hidden ) to N x ( 1 ), since Q is a scalar function. \n",
    "        self.l3 = nn.Linear( n_hidden, 1 )\n",
    "\n",
    "        # ================================================================================ #\n",
    "        # ================================ Second Q-Network ============================== #\n",
    "        # ================================================================================ #\n",
    "        # First Layer, changes array with size N x ( n_state + n_action ) to N x ( n_hidden )\n",
    "        self.l4 = nn.Linear( n_state + n_action, n_hidden )\n",
    "\n",
    "        # Second Layer, changes array with size N x ( n_hidden ) to N x ( n_hidden )\n",
    "        self.l5 = nn.Linear( n_hidden, n_hidden )\n",
    "\n",
    "        # Third Layer, changes array with size N x ( n_hidden ) to N x ( 1 ), since Q is a scalar function. \n",
    "        self.l6 = nn.Linear( n_hidden, 1 )\n",
    "\n",
    "        # Note that the first and second Q-networks has exactly the same structure. The parameters are only difference. \n",
    "    \n",
    "    def forward( self, state, action ):\n",
    "\n",
    "        # Concatenation of state and action vector.\n",
    "        # The state  is assumed to be a 2D array with size N x n_s, where N is the number of samples\n",
    "        # The action is assumed to be a 2D array with size N x n_a, where N is the number of samples\n",
    "        # As a result of torch.cat( [ state, action ] along axis 1, ), we have size N x ( n_s + n_a ), and the dim = 0 must have the same size\n",
    "        x = torch.cat( [ state, action ], dim = 1 )\n",
    "\n",
    "\n",
    "        # ================================================================================ #\n",
    "        # =========================== Calculating Q1 and Q2 ============================== #\n",
    "        # ================================================================================ #\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        q1 = F.relu( self.l1( x ) )\n",
    "        q2 = F.relu( self.l4( x ) )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        q1 = F.relu( self.l2( q1 ) )\n",
    "        q2 = F.relu( self.l5( q2 ) )\n",
    "\n",
    "        # A simple Ax + b combination \n",
    "        q1 = self.l3( q1 )\n",
    "        q2 = self.l6( q2 )\n",
    "\n",
    "        # The output is a N x 1 array. \n",
    "        return q1, q2\n",
    "\n",
    "    def Q1( self, state, action ):\n",
    "        x = torch.cat( [ state, action ], dim = 1 )\n",
    "\n",
    "        # Applying Rectified Linear Unit (ReLU) to x\n",
    "        q1 = F.relu( self.l1( x ) )\n",
    "        q1 = F.relu( self.l2( q1 ) )\n",
    "        q1 = self.l3( q1 )\n",
    "\n",
    "        return q1\n",
    "\n",
    "\n",
    "\n",
    "class TD3( object ):\n",
    "\n",
    "    def __init__( self, n_state, n_action, max_action, gamma = 0.99, tau = 0.005, policy_noise = 0.2, noise_clip = 0.5, policy_freq = 2 ):\n",
    "\n",
    "        self.actor            = Actor( n_state, n_action, max_action = max_action ).to( device )\n",
    "        self.actor_target     = copy.deepcopy( self.actor )\n",
    "        self.actor_optimizer  = torch.optim.Adam( self.actor.parameters( ) , lr = 1e-4 )\n",
    "\n",
    "        self.critic           = Critic( n_state, n_action ).to( device )\n",
    "        self.critic_target    = copy.deepcopy( self.critic )\n",
    "        self.critic_optimizer = torch.optim.Adam( self.critic.parameters( ), lr = 1e-3 )\n",
    "\n",
    "        self.max_action = max_action\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.tau   = tau \n",
    "\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip   = noise_clip\n",
    "        self.policy_freq  = policy_freq\n",
    "        \n",
    "        self.total_it = 0 \n",
    "\n",
    "    def get_action( self, state ):\n",
    "\n",
    "        # Conduct the a = mu(s), where mu is a \"deterministic function\"\n",
    "        # Unsqueeze makes an 1 x n_s array of state. \n",
    "        state  = torch.from_numpy( state ).float( ).unsqueeze( 0 ).to( device )\n",
    "\n",
    "        # Returns an 1 x n_a array of state\n",
    "        # forward method can be omitted\n",
    "        action = self.actor( state )\n",
    "\n",
    "        # Change action from Torch to Numpy.\n",
    "        # Since n_a is 1 for this case, action is simply an 1x1 array.\n",
    "        # Hence, flattening the data. \n",
    "        action = action.cpu( ).data.numpy( ).flatten( )\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def update( self, replay_buffer, batch_size: int = 256 ):\n",
    "\n",
    "        self.total_it += 1\n",
    "        state, action, reward, next_state, is_done = replay_buffer.sample( batch_size )\n",
    "\n",
    "        with torch.no_grad( ):\n",
    "            noise       = ( torch.randn_like( action ) * self.policy_noise  ).clamp( -self.noise_clip, self.noise_clip )\n",
    "            next_action = ( self.actor_target( next_state ) + noise ).clamp( -self.max_action, self.max_action )\n",
    "\n",
    "            target_Q1, target_Q2 = self.critic_target( next_state, next_action )\n",
    "            target_Q = torch.min( target_Q1, target_Q2 )\n",
    "            target_Q = reward + (  ( 1. - is_done ) * self.gamma * target_Q )#.detach( )\n",
    "\n",
    "\t\t# Get current Q estimates\n",
    "        current_Q1, current_Q2 = self.critic( state, action )\n",
    "\n",
    "\t\t# Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q1, target_Q) + F.mse_loss(current_Q2, target_Q)\n",
    "\n",
    "\t\t# Optimize the critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "\t\t# Delayed policy updates\n",
    "        if self.total_it % self.policy_freq == 0:\n",
    "\n",
    "\t\t\t# Compute actor losse\n",
    "            actor_loss = -self.critic.Q1( state, self.actor( state ) ).mean()\n",
    "\t\t\t\n",
    "\t\t\t# Optimize the actor \n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "\t\t\t# Update the frozen target models\n",
    "            for target_param, param in zip( self.critic_target.parameters( ), self.critic.parameters( ) ):\n",
    "                target_param.data.copy_( self.tau * param.data + ( 1 - self.tau ) * target_param.data )\n",
    "                \n",
    "            for target_param, param in zip( self.actor_target.parameters( ) ,  self.actor.parameters( ) ):\n",
    "                target_param.data.copy_( self.tau * param.data + ( 1 - self.tau ) * target_param.data )\n",
    "\n",
    "\n",
    "    def save( self, filename ):\n",
    "        \n",
    "        torch.save( self.critic.state_dict( )           , filename + \"_critic\"              )\n",
    "        torch.save( self.critic_optimizer.state_dict( ) , filename + \"_critic_optimizer\"    )\n",
    "        \n",
    "        torch.save( self.actor.state_dict( )            , filename + \"_actor\"               )\n",
    "        torch.save( self.actor_optimizer.state_dict( )  , filename + \"_actor_optimizer\"     )\n",
    "\n",
    "\n",
    "    def load( self, filename ):\n",
    "\n",
    "        # Load Critic\n",
    "        self.critic.load_state_dict(            torch.load( filename + \"_critic\"           )  )\n",
    "        self.critic_optimizer.load_state_dict(  torch.load( filename + \"_critic_optimizer\" )  )\n",
    "        self.critic_target = copy.deepcopy( self.critic )\n",
    "\n",
    "        # Load Actor\n",
    "        self.actor.load_state_dict(             torch.load( filename + \"_actor\"            )  )\n",
    "        self.actor_optimizer.load_state_dict(   torch.load( filename + \"_actor_optimizer\"  )  )\n",
    "        self.actor_target = copy.deepcopy( self.actor )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define instances of the environment, DDPG agent and the OU noise.\n",
    "env   = gym.make( \"Pendulum-v1\" ) \n",
    "\n",
    "# Set the random seeds\n",
    "env.seed(              round( time.time( ) ) )\n",
    "env.action_space.seed( round( time.time( ) ) )\n",
    "torch.manual_seed(     round( time.time( ) ) )\n",
    "np.random.seed(        round( time.time( ) ) )\n",
    "\n",
    "# Get the dimension of states and actions, and also the \n",
    "# [WARNING] This is for environments where we assume the mean of action is 0. \n",
    "n_state    = env.observation_space.shape[ 0 ] \n",
    "n_action   = env.action_space.shape[ 0 ]\n",
    "max_action = float( env.action_space.high  )\n",
    "\n",
    "# Define the agent, noise and replay buffers\n",
    "agent         = TD3( n_state, n_action, max_action )\n",
    "replay_buffer = ReplayBuffer( n_state, n_action )\n",
    "\n",
    "# The number of \"batch\" that will be sampled from the replay buffer will be \"batch_size\" \n",
    "n_batch_size  = 256\n",
    "\n",
    "# Saving these values to plot the performance at the end.\n",
    "frames        = [ ]\n",
    "whole_rewards = [ ]\n",
    "\n",
    "# Flags for turning on or off the render.\n",
    "is_save_video = False\n",
    "is_save_model = False\n",
    "\n",
    "# For the pendulum model the best reward is 0, hence saving a -infinity value. \n",
    "best_model_val = -np.inf\n",
    "\n",
    "rewards       = [ ]\n",
    "avg_rewards   = [ ]\n",
    "\n",
    "for episode in range( 500 ):\n",
    "\n",
    "    # Initialize the gym environment and OU noise \n",
    "    state = env.reset()\n",
    "\n",
    "    # Initialize the episode's reward\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # For pendulum v1 gym, a single simulation is maximum 200-steps long. \n",
    "    # [REF] https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py\n",
    "    for step in range( 200 ):\n",
    "\n",
    "        # Get the action value from the deterministic policy Actor network.\n",
    "        action = agent.get_action( state )\n",
    "\n",
    "        if is_save_video : frames.append( env.render( mode = 'rgb_array' ) )\n",
    "\n",
    "        # Apply the OU noise on this action\n",
    "\n",
    "        # Run a single step of simulation\n",
    "        new_state, reward, done, _ = env.step( action )  \n",
    "\n",
    "        # Add this to our replay buffer, note that push simply generates the tuple and add \n",
    "        replay_buffer.add( state, action, reward, new_state, done )\n",
    "        \n",
    "        # Once the agent memory is full, then update the policy via replay buffer.\n",
    "        if replay_buffer.current_size > n_batch_size: agent.update( replay_buffer, batch_size = n_batch_size )        \n",
    "        \n",
    "        # Update the state and reward value \n",
    "        state = new_state\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    if best_model_val <= episode_reward:\n",
    "        best_model_val = episode_reward \n",
    "\n",
    "        # If this policy has a good result, save it \n",
    "        if is_save_model: agent.save( \"../models/DDPG_best_model\" ) \n",
    "\n",
    "    # Once a single simulation is done, append the values that will be plotted later\n",
    "    rewards.append( episode_reward )\n",
    "    avg_rewards.append( np.mean( rewards[ -10 : ] ) )\n",
    "\n",
    "    sys.stdout.write(\"episode: {}, reward: {}, average_reward: {} \\n\".format( episode, np.round( episode_reward, decimals = 2 ), avg_rewards[ -1 ] ) ) \n",
    "\n",
    "whole_rewards.append(  rewards  )\n",
    "\n",
    "env.close( )\n",
    "\n",
    "if is_save_video:\n",
    "    clip = mpy.ImageSequenceClip( frames, fps = 30 )\n",
    "    clip.write_gif( \"../videos/DDPG.gif\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot( rewards     )\n",
    "plt.plot( avg_rewards )\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "742fb12a3d29bb1ab201ca06dfc1cb9996440041728c355377b69362e9fdf7ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
