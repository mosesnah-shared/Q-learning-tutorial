{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient - REINFORCE\n",
    "\n",
    "This code is an example using Policy Gradient Method. The code is modified from [this Github repository](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py). There is also another code from [this website](https://medium.com/@thechrisyoon/deriving-policy-gradients-and-implementing-reinforce-f887949bd63), but the code seems out-dated.\n",
    "These example uses the gym [CartPole system](https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py).\n",
    "Here we use [pyTorch](https://pytorch.org/) rather than [tensorFlow](https://www.tensorflow.org/). \n",
    "- `torch.nn` is used to inherit and use the neural network class.\n",
    "- `torch.nn.functional` is used to apply ReLU or softmax.\n",
    "- `torch.optim` is used to apply [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/), a famous method for stochastic gradient descent.\n",
    "- `torch.autograd`'s `Variable` is known to be deprecated [[REF]](https://stackoverflow.com/questions/57580202/whats-the-purpose-of-torch-autograd-variable), but since the original code uses it we will leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import time\n",
    "import numpy as np  \n",
    "\n",
    "import torch  \n",
    "import torch.nn            as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim         as optim\n",
    "\n",
    "import matplotlib.pyplot   as plt\n",
    "import moviepy.editor as mpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a Policy Network\n",
    "\n",
    "The policy neural network has three layers:\n",
    "- [Layer 1] Denoted as $l_1$ with $n_s$ Neurons. The input is the state of the current cart-and-pole.\n",
    "- [Layer 2] Denoted as $l_2$ with $n_{h}$ Neurons, where subscript $h$ stands for \"hidden\".\n",
    "- [Layer 3] Denoted as $l_3$ with $n_a$ Neurons\n",
    "\n",
    "From $l_1$ to $l_2$ and $l_2$ to $l_3$, the network uses a linear combination, i.e., using $l_1$, $l_2$ and $l_3$ as an array of values of the corresponding neurons:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    l_2 &= W_{12} \\cdot l_1 + b_{12} \\\\\n",
    "    l_3 &= W_{23} \\cdot l_2 + b_{23}\n",
    "\\end{align*}    \n",
    "$$\n",
    "where $W$ and $b$ are weight matrix and bias array, respectively.\n",
    "Note that the values are additionally fed into a nonlinear function.\n",
    "$l_2$ is fed into a ReLU (Rectified Linear Unit) function, and $l_3$ is fed into a softmax function to make it as a probability distribution function (i.e., all the values are non-negative and it sums to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyNetwork( nn.Module ):\n",
    "    def __init__( self, num_inputs, num_actions, hidden_size, learning_rate = 3e-4 ):\n",
    "        super( PolicyNetwork , self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # ======== nn.Linear ======== #\n",
    "        # nn.Linear( nx, ny ), nx = |x|, ny = |y|, where |-| is an operator measuring the set's cardinality.\n",
    "        # nn.Linear gets input x and output y with combination of: y = A^Tx + b\n",
    "        # for parameters(), A^T matrix with size ny X nx and a bias vector b with size ny is saved.\n",
    "        self.linear1     = nn.Linear( num_inputs, hidden_size  )\n",
    "        self.linear2     = nn.Linear( hidden_size, num_actions )\n",
    "\n",
    "        # ======== Optimizer ======== #\n",
    "        # For policy gradient, we need the derivatives with respect to A^T and b values.\n",
    "        # Using the ADAM optimizer. \n",
    "        self.optimizer   = optim.Adam( self.parameters( ), lr = learning_rate )\n",
    "\n",
    "    def forward( self, state ):\n",
    "\n",
    "        # Forward to neural network\n",
    "        x = F.relu( self.linear1( state ) )\n",
    "        x = F.softmax( self.linear2( x ), dim = 1 )\n",
    "        return x \n",
    "    \n",
    "    def get_action( self, state ):\n",
    "\n",
    "        # from_numpy( array ): change numpy \"array\" to tensor\n",
    "        #            float( ): define the variable type as \"float( )\"\n",
    "        #      unsqueeze( 0 ): force the input with size n_s to be (1 x n_s) \n",
    "        \n",
    "        state = torch.from_numpy( state ).float( ).unsqueeze( 0 ) \n",
    "\n",
    "        # Forward the neural network and return the prbability distribution function\n",
    "        probs = self.forward( state  )\n",
    "\n",
    "        # Choosing the action based on the output policy\n",
    "        highest_prob_action = np.random.choice( self.num_actions, p = np.squeeze( probs.detach( ).numpy( ) ) )\n",
    "\n",
    "        # The log-value of the probability just for the sake of policy gradient\n",
    "        log_prob = torch.log( probs.squeeze( 0 )[ highest_prob_action ] )\n",
    "        \n",
    "        return highest_prob_action, log_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Goal of Policy Gradient Method!\n",
    "\n",
    "Once we parameterize the policy with a neural network $\\pi_{\\theta}(a|s)$, we need an objective function to conduct the gradient ascent (recall that we are optimizing rewards not costs!).\n",
    "Given a trajectory $\\tau$ of Markov Decision Process with length $T$ (starts from 0, hence $T+1$ states):\n",
    "$$\n",
    "    \\tau: s_0, a_0, r_1, s_1, a_1, \\cdots, s_{T-1}, a_{T-1}, r_T, s_T\n",
    "$$\n",
    "\n",
    "The objective function $J(\\theta)$ that we are planning to maximize is the expected value of the reward from a given trajectory:\n",
    "$$\n",
    "    J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Big[R(\\tau)\\Big], \\;\\; \\text{where} \\;\\;     R(\\tau) =\\sum_{i=1}^{T}r_i\n",
    "$$\n",
    "Note that the $R(\\cdot)$ function is simply the sum of all rewards of a given trajectory, and subscript $\\tau \\sim \\pi_{\\theta}$ means that the trajectory is generated following policy $\\pi_{\\theta}$.\n",
    "The gradient of the objective function is simply:\n",
    "$$\n",
    "    \\nabla_{\\theta} J(\\theta) = \\nabla_{\\theta}\\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Big[R(\\tau)\\Big] = \\nabla_{\\theta}\\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\bigg[ \\sum_{i=1}^{T}r_i \\bigg]\n",
    "$$\n",
    "Since it is the expectation over all possible trajectories, we use a simple trick to simplify the gradient:\n",
    "$$\n",
    "    \\nabla_{\\theta}\\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\bigg[ R(\\tau) \\bigg] = \\nabla_{\\theta}\\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) = \\sum_{\\tau} R(\\tau) \\nabla_{\\theta} p_{\\theta}(\\tau) = \\sum_{\\tau} R(\\tau) \\frac{\\nabla_{\\theta} p_{\\theta}(\\tau)}{p_{\\theta}(\\tau)}p_{\\theta}(\\tau) = \\sum_{\\tau} R(\\tau) p_{\\theta}(\\tau) \\nabla_{\\theta}\\log{p_{\\theta}(\\tau)} = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Big[ R(\\tau)\\nabla_{\\theta}\\log{p_{\\theta}(\\tau)} \\Big]\n",
    "$$\n",
    "Meaning, the gradient with respect to parameter $\\theta$ goes into the expectation operator and we are taking the log of the probability of the trajectory. \n",
    "Our next step is to conduct a calculation of $p_{\\theta}(\\tau)$, $\\log{p_{\\theta}(\\tau)}$:\n",
    "$$\n",
    "\\begin{align*}\n",
    "    p_{\\theta}(\\tau)       & = \\text{Pr}\\Big[ S_0 = s_0, A_0 = a_0, \\cdots, S_{T-1}=s_{T-1}, A_{T-1}=a_{T-1} \\Big] \\\\\n",
    "                           & =  p(s_0)\\cdot\\pi_{\\theta}(a_0|s_0)\\cdot p(r_1, s_1 | a_0, s_0) \\cdot \\pi_{\\theta}(a_1 | s_1) \\cdot p(r_2, s_2 | a_1, s_1) \\cdots \\pi_{\\theta}(a_{T-1}|s_{T-1})p(r_T, s_T | a_{T-1}, s_{T-1}) \\\\\n",
    "                           \\\\\n",
    "    \\log{p_{\\theta}(\\tau)} & = \\log{p(s_0)} + \\log{\\pi_{\\theta}(a_0|s_0)} + \\log{p(r_1, s_1 | a_0, s_0)} + \\cdots + \\log{\\pi_{\\theta}(a_{T-1}|s_{T-1})} + \\log{p(r_T, s_T | a_{T-1}, s_{T-1})}  \\\\\n",
    "    \\\\\n",
    "    \\nabla_{\\theta}\\log{p_{\\theta}(\\tau)} & = \\nabla_{\\theta} \\bigg( \\sum_{i=0}^{T-1} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg) \n",
    "\\end{align*}    \n",
    "$$\n",
    "The great point about policy gradients is the fact that only the policy terms $\\pi_{\\theta}(a_i|s_i)$ survive for the gradient. Summarizing, \n",
    "$$\n",
    "    \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Big[ R(\\tau)\\nabla_{\\theta}\\log{p_{\\theta}(\\tau)} \\Big] \n",
    "    = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ R(\\tau)\\nabla_{\\theta} \\bigg( \\sum_{i=0}^{T-1} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg)  \\Bigg] \n",
    "    = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\bigg( \\sum_{i=1}^{T}r_i \\bigg) \\nabla_{\\theta} \\bigg( \\sum_{i=0}^{T-1} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg)  \\Bigg]\n",
    "$$\n",
    "And using some math tricks for the simplification [[REF]](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/):\n",
    "$$\n",
    "\\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\bigg( \\sum_{i=1}^{T}r_i \\bigg) \\nabla_{\\theta} \\bigg( \\sum_{i=0}^{T-1} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg)  \\Bigg] = \\cdots = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\sum_{i=0}^{T-1} \\nabla_{\\theta} \\log{\\pi_{\\theta}(a_i|s_i)} \\bigg(\\sum_{j=i+1}^{T}r_j\\bigg) \\Bigg]\n",
    "$$\n",
    "Note that the last term is simply the undiscounted sum of rewards, i.e., $G_t$ with $\\gamma = 1$. Meaning, we can simply generalize this equation for cases with $\\gamma \\in [0,1)$.\n",
    "\n",
    "Summarizing, we calculate the gradient via:\n",
    "$$\n",
    "    \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\sum_{i=0}^{T-1} \\nabla_{\\theta} \\log{\\pi_{\\theta}(a_i|s_i)} G_t \\Bigg]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# REINFORCE Algorithm\n",
    "Since we introduced the theory, we show the most basic algorithm called REINFORCE, which is a Monte-Carlo variant of policy gradients. Note that Monte-Carlo methods is simply a hard way of saying \"methods using random samples\".\n",
    "The Monte-Carlo method is required since we cannot calculate actual \"expectations\" in real life $\\mathbb{E}[\\cdot]$. We need to somehow take samples and average it to calculate the expectations. \n",
    "The psuedo-code image is from [[REF]](http://www.cs.toronto.edu/~tingwuwang/REINFORCE.pdf).\n",
    "\n",
    "# Speeding up the python code.\n",
    "Before diving into the codes are some tips to speed-up the python code. One of the method is suppressing the usage of for-loops. \n",
    "In fact, for loops are known to be **Evil!** Please check out [this post](https://medium.com/python-pandemonium/never-write-for-loops-again-91a5a4c84baf) in case you are interested. Summarizing, it is a good exercise NOT to use for-loop, since there might be other ways that are way better and faster.\n",
    "\n",
    "We present an example that calculates the $G_t$ array from a reward trajectory. Please check below for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Gt_method1( r_arr: np.ndarray, gamma: float = 1, is_print: bool = False) :\n",
    "    \"\"\"\n",
    "        This method uses vectors for the Gt array calculation.\n",
    "\n",
    "        Args:\n",
    "            [1] r_arr: an array of rewards for each time step. \n",
    "\n",
    "            [2] gamma: discount ratio, valued between 0 to 1. \n",
    "                       If gamma = 1, then there is no discount applied\n",
    "\n",
    "            [3] is_print: a boolean value which determines whether to print out the computation time or not.\n",
    "\n",
    "        Return:\n",
    "            G_t: an array of discounted (or if gamma = 1, simple sum of) rewards\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    N = len( r_arr )\n",
    "\n",
    "    tmp = np.concatenate( ( np.ones( 1 ), np.cumprod( gamma * np.ones( N - 1 ) ) ) )\n",
    "    Gt_arr = np.flip( np.cumsum( tmp * r_arr  ) )\n",
    "\n",
    "    if is_print: print( \"[Method 1] [Vectorization]\", time.time( ) - start) \n",
    "\n",
    "    return Gt_arr\n",
    "\n",
    "def Gt_method2( r_arr: np.ndarray, gamma: float = 1, is_print: bool = False ):\n",
    "    \"\"\"\n",
    "        This method uses a brute-force for loop executation for calculating the Gt array.\n",
    "\n",
    "        Args:\n",
    "            [1] r_arr: an array of rewards for each time step. \n",
    "\n",
    "            [2] gamma: discount ratio, valued between 0 to 1. \n",
    "                       If gamma = 1, then there is no discount applied\n",
    "\n",
    "            [3] is_print: a boolean value which determines whether to print out the computation time or not.\n",
    "\n",
    "        Return:\n",
    "            G_t: an array of discounted (or if gamma = 1, simple sum of) rewards\n",
    "    \"\"\"\n",
    "\n",
    "    start = time.time()\n",
    "    Gt_arr = []\n",
    "\n",
    "    for t in range( len( r_arr ) ) :\n",
    "\n",
    "        # Initialization of Gt\n",
    "        Gt = 0 \n",
    "\n",
    "        # The power for the calculation\n",
    "        pw = 0\n",
    "        \n",
    "        for r in r_arr[ t: ] :\n",
    "            Gt += gamma ** pw * r\n",
    "            pw += 1\n",
    "\n",
    "        Gt_arr.append( Gt )\n",
    "\n",
    "    if is_print: print( \"[Method 2] [For-loop] \", time.time() - start )\n",
    "\n",
    "    return Gt_arr\n",
    "\n",
    "# Play with this number, the effect is in fact, dramatic\n",
    "N     = 1000\n",
    "r_arr = np.ones( N )\n",
    "\n",
    "# _ is just to suppress the output print of the function\n",
    "Gt1 = Gt_method1( r_arr, gamma = 1, is_print = True )\n",
    "Gt2 = Gt_method2( r_arr, gamma = 1, is_print = True )\n",
    "\n",
    "# For N = 1000, method1 takes an average of 0.0001 second, while method2 take 0.17s \n",
    "# Try it yourself!\n",
    "Gt_calc = Gt_method2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy( policy_network, rewards, log_probs, is_whitening = False ):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            log_probs: list of individual tensor with pi( at | st ) value.\n",
    "                       This value is a function of parameters of the neural network.\n",
    "    \"\"\"\n",
    "\n",
    "    # This is the discount rate, it should be smaller than 1 for convergence. \n",
    "    GAMMA  = 1.0\n",
    "\n",
    "    # Calculate the Gt_arr for the training. \n",
    "    Gt_arr = Gt_calc( rewards, GAMMA, is_print = False )\n",
    "\n",
    "    # numpy to tensor. Note that we don't need to set \"grad_required\" as True, as \"log_probs\" already has it\n",
    "    Gt_arr = torch.tensor( Gt_arr )\n",
    "\n",
    "    # The Gt_arr method has high variant, hence there are tricks to suppress the high variance of the approach.\n",
    "    # One basic approach is to normalize this value (a.k.a., the whitening method)\n",
    "    # But personally, not sure whether this method is useful. It seems more of a \"hack\" with baseless math. \n",
    "    if is_whitening: Gt_arr = ( Gt_arr - Gt_arr.mean( ) ) / ( Gt_arr.std( ) + 1e-9 ) \n",
    "\n",
    "    # policy_gradient = []\n",
    "\n",
    "    # for log_prob, Gt in zip( log_probs, Gt_arr ):\n",
    "    #     policy_gradient.append( -log_prob * Gt )\n",
    "\n",
    "    # Initialize the gradient value to zeros\n",
    "    policy_network.optimizer.zero_grad( )\n",
    "    \n",
    "    # policy_gradient = torch.stack( policy_gradient ).sum()\n",
    "    # log_probs is a \"list\" of tensors. \n",
    "    # Thus, we need to change this from \"list\" to simply tensors via .stack( ) method, squeeze it since from 2D to 1D array, dot product and sum\n",
    "\n",
    "    policy_gradient = -torch.dot( torch.stack( log_probs, dim = 0 ), Gt_arr ).sum( )\n",
    "    policy_gradient.backward()\n",
    "    policy_network.optimizer.step()  \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of $G_t$?\n",
    "Due to practical reason, just directly using the Gt values lead to unstable + highly-variant result of the learning process. \n",
    "Thus, there are tricks that can be applied. \n",
    "The most common one is the **whitening transformation**, which is mentioned in [this post](https://medium.com/nerd-for-tech/policy-gradients-reinforce-with-baseline-6c871a3a068). \n",
    "The idea is simple: just change the $G_t$ via normalization:\n",
    "\n",
    "$$\n",
    "    \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\sum_{i=0}^{T-1} \\nabla_{\\theta} \\log{\\pi_{\\theta}(a_i|s_i)} G_t \\Bigg] \\;\\; \\Longrightarrow \\;\\; \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\sum_{i=0}^{T-1} \\nabla_{\\theta} \\log{\\pi_{\\theta}(a_i|s_i)} \\frac{(G_t  - \\bar{G})}{\\sigma_G} \\Bigg]\n",
    "$$\n",
    "where $\\bar{G}$ is the mean of $T$ samples of the trajectory, and $\\sigma_G$ is the standard deviation of it.\n",
    "```\n",
    "    Gt_arr = ( Gt_arr - Gt_arr.mean( ) ) / ( Gt_arr.std() + 1e-9)  # normalize discounted rewards\n",
    "```\n",
    "[This post](https://datascience.stackexchange.com/questions/20098/why-do-we-normalize-the-discounted-rewards-when-doing-policy-gradient-reinforcem) will be helpful. \n",
    "The bottom line is to simply stabilize the learning process by normalization, which keeps the $G_t$ value in some reasonable value range. I personally tried myself, although the effects are not clear enough to see the benefit. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example - Cart-and-Pole System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Whitening \n",
    "is_white      = False\n",
    "is_save_video = False\n",
    "\n",
    "# Generate the gym of Cart-and-Pole\n",
    "env = gym.make( 'CartPole-v1' ) \n",
    "\n",
    "\n",
    "# The number of states and actions are +4 and +2\n",
    "ns  = env.observation_space.shape[ 0 ]\n",
    "na  = env.action_space.n\n",
    "\n",
    "# Generate the 3-layer Policy Network.\n",
    "policy_net = PolicyNetwork( ns, na, 128 ) \n",
    "\n",
    "# We conduct max_episode_num trails (or episodes)\n",
    "max_episode_num = 2000\n",
    "\n",
    "# For each trial, we run (e.g.,) 500 steps, i.e., T = 500 and the trajectory will be as follows:\n",
    "# S0, A0, R1, S1, A1, ... S499 A499 R500, S500\n",
    "# .... in case if the simulation does not reach the terminal state\n",
    "max_steps       = 500\n",
    "\n",
    "# Saving the number of steps for each trial\n",
    "numsteps        = []\n",
    "avg_numsteps    = []\n",
    "\n",
    "# Saving the sum of rewards of a single trial\n",
    "all_rewards     = []\n",
    "\n",
    "# Saving the best model parameters \n",
    "best_model_val  = 0\n",
    "\n",
    "frames = []\n",
    "\n",
    "for episode in range( max_episode_num ):\n",
    "\n",
    "    # gym initialization\n",
    "    state     = env.reset()\n",
    "\n",
    "    # To run gradient ascent, we need to save the array of log_probs and rewards. \n",
    "    # In detail, it is the \"discounted\" rewards, but the \"update_policy\" method executes that calculation internally. \n",
    "    log_probs = []\n",
    "    rewards   = []\n",
    "\n",
    "    # Run a single trial (i.e., a single trajectory)\n",
    "    for steps in range( max_steps ):\n",
    "\n",
    "    \n",
    "        if is_save_video : frames.append( env.render( mode = 'rgb_array' ) )\n",
    "        \n",
    "        # Get the choice of action and the pi( a_t | s_t ) for the gradient calculation\n",
    "        action, log_prob = policy_net.get_action( state )\n",
    "\n",
    "\n",
    "        # The 4th argument is \"info\", which is some sort of additional information that we don't use for this example.\n",
    "        new_state, reward, done, _ = env.step( action )\n",
    "\n",
    "        log_probs.append( log_prob )\n",
    "        rewards.append( reward )\n",
    "\n",
    "        # If the trail encounters the terminal state\n",
    "        if done: \n",
    "\n",
    "            numsteps.append( steps )\n",
    "\n",
    "            # Taking the average of the number of steps for the learning process\n",
    "            avg_numsteps.append( np.mean( numsteps[ -10: ] ) )\n",
    "\n",
    "            # The rewards of the whole process. \n",
    "            sum_rewards = np.sum( rewards )\n",
    "            all_rewards.append( sum_rewards )\n",
    "\n",
    "            if best_model_val <= sum_rewards:\n",
    "                best_model_val = sum_rewards \n",
    "\n",
    "                # If this policy has a good result, save it \n",
    "                torch.save( policy_net, '../models/PG_raw_best_model.pth') if not is_white else torch.save( policy_net, '../models/PG_white_best_model.pth')\n",
    "\n",
    "            # Update the policy\n",
    "            update_policy( policy_net, rewards, log_probs, is_whitening = is_white )\n",
    "\n",
    "            if episode % 1 == 0:\n",
    "                sys.stdout.write( \"episode: {}, total reward: {}, average_reward: {}, length: {}\\n\".format(episode, np.round( sum_rewards, decimals = 3 ),  np.round( np.mean( all_rewards[ -10 : ] ), decimals = 3 ), steps ) )\n",
    "            break\n",
    "\n",
    "            \n",
    "        state = new_state\n",
    "\n",
    "if is_save_video:\n",
    "    clip = mpy.ImageSequenceClip( frames, fps = 30 )\n",
    "    clip.write_gif( \"../videos/circle.gif\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close( )\n",
    "plt.plot( numsteps )\n",
    "plt.plot( avg_numsteps )\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the optimal policy (i.e., optimal neural network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_save_video = True\n",
    "\n",
    "env = gym.make( 'CartPole-v1' )\n",
    "\n",
    "# Saving the path \n",
    "# model_path = \"../models/PG_raw_best_model.pth\"\n",
    "model_path = \"../models/PG_white_best_model_1p0.pth\"\n",
    "\n",
    "# Load the pyTorch Model\n",
    "model = torch.load( model_path )\n",
    "\n",
    "\n",
    "# Run trial\n",
    "state = env.reset()   \n",
    "\n",
    "frames = []\n",
    "# The maximum trial is 500 for the cart-and-pole\n",
    "for _ in range( 500 ):\n",
    "\n",
    "    if is_save_video: frames.append( env.render( mode = \"rgb_array\")  )\n",
    "    env.render( )\n",
    "\n",
    "    # Get the choice of action and the pi( a_t | s_t ) for the gradient calculation\n",
    "    action, _ = model.get_action( state )\n",
    "    new_state, _, done, _ = env.step( action )\n",
    "\n",
    "    # If the trail encounters the terminal state\n",
    "    if done: \n",
    "        break\n",
    "    state = new_state\n",
    "\n",
    "# Save Video\n",
    "env.close( )\n",
    "\n",
    "if is_save_video:\n",
    "    clip = mpy.ImageSequenceClip( frames, fps = 30 )\n",
    "    clip.write_gif( \"../videos/circle.gif\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "- [1] [Great post on Github](https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/README.md)\n",
    "- [2] [Policy Gradient with Stephan Curry](https://jonathan-hui.medium.com/rl-policy-gradients-explained-9b13b688b146)\n",
    "- [3] [Policy Gradient in a Nutshell](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d)\n",
    "- [4] [Pong from Pixels](http://karpathy.github.io/2016/05/31/rl/)\n",
    "- [5] [Great post from Dr. Daniel Seita](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742fb12a3d29bb1ab201ca06dfc1cb9996440041728c355377b69362e9fdf7ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
