{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Method\n",
    "\n",
    "This code is an example using Advantage Actor-Critic Method (a.k.a., A2C) The code is modified from [this one](https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py). [This website](https://medium.com/towards-data-science/understanding-actor-critic-methods-931b97b6df3f) might also be useful as a future reference, although the code can be outdated. \n",
    "\n",
    "There are multiple Actor-Critic Methods, but here we focus on \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "gamma     = 0.99\n",
    "is_render = False \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Neural Network.\n",
    "\n",
    "We merge the two networks in a single class, although we can separate these two. Since we have merged the two Actor and Critic networks, they are updated in synchrony, and they also share the same state input.\n",
    "A method which asynchronously updates the two networks are called the \"Asynchronous Advantage Actor-Critic Method\" (a.k.a. A3C), where the details are in [this website](https://medium.com/@shagunm1210/implementing-the-a3c-algorithm-to-train-an-agent-to-play-breakout-c0b5ce3b3405).\n",
    "\n",
    "### Actor Network\n",
    "The actor network maps state to action, hence the input is the number of states of the example, $n_s$ and the output is the number of actions $n_a$.\n",
    "\n",
    "### Critic Network\n",
    "The critic network maps state to a scalar real value, which is known to be the Value function $V^{\\pi}(s)$.\n",
    "\n",
    "### Advantage Value \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gt_calc( rewards: np.ndarray, gamma: float = 1, is_normalize = False ) :\n",
    "    \"\"\"\n",
    "        This method uses vectors for the Gt array calculation.\n",
    "\n",
    "        Args:\n",
    "            [1] rewards: an array of rewards for each time step. \n",
    "\n",
    "            [2] gamma: discount ratio, valued between 0 to 1. \n",
    "                       If gamma = 1, then there is no discount applied\n",
    "\n",
    "        Return:\n",
    "            G_t: an array of discounted (or if gamma = 1, simple sum of) rewards\n",
    "    \"\"\"\n",
    "\n",
    "    N = len( rewards )\n",
    "\n",
    "    tmp = np.concatenate( ( np.ones( 1 ), np.cumprod( gamma * np.ones( N - 1 ) ) ) )\n",
    "    Gt_arr = np.flip( np.cumsum( tmp * rewards  ) )\n",
    "\n",
    "    if is_normalize: Gt_arr = ( Gt_arr - Gt_arr.mean( ) ) / ( Gt_arr.std( ) + 1e-9 ) \n",
    "\n",
    "\n",
    "    return Gt_arr\n",
    "\n",
    "\n",
    "class ActorCritic( nn.Module ):\n",
    "    \"\"\"\n",
    "        Implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__( self, num_input, num_output, num_hidden  ):\n",
    "\n",
    "        super( ActorCritic, self ).__init__(  )\n",
    "\n",
    "        # Saving this for choosing the action.\n",
    "        self.n_action = num_output\n",
    "\n",
    "        # First layer : from input to hidden variable\n",
    "        self.affine1 = nn.Linear( num_input, num_hidden)\n",
    "\n",
    "        # Second Actor's layer\n",
    "        self.actor_layer = nn.Linear( num_hidden, num_output )\n",
    "\n",
    "        # Second Critic's layer\n",
    "        self.critic_layer = nn.Linear( num_hidden, 1 )\n",
    "\n",
    "        # Optimizer \n",
    "        self.optimizer = optim.Adam( self.parameters( ), lr = 0.03 )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        \"\"\"\n",
    "            Forward of both actor and critic.\n",
    "\n",
    "            Args:\n",
    "                - Current State Vector\n",
    "\n",
    "            Outputs:\n",
    "                - [1] action_prob: The pi( . | s ) itself\n",
    "                - [2] state_value: The value function V(s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward the 1st layer\n",
    "        x = F.relu( self.affine1( x ) )\n",
    "\n",
    "        # Actor: Chooses action to take from state s_t \n",
    "        action_prob = F.softmax( self.actor_layer( x ), dim = -1 )\n",
    "\n",
    "        # Critic: evaluates being in the state s_t\n",
    "        state_value = self.critic_layer( x )\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t \n",
    "        return action_prob, state_value\n",
    "\n",
    "    def get_action_and_values( self, state: np.ndarray ):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                [1] State vector\n",
    "\n",
    "            Outputs:\n",
    "                [1]   action: The action chosen from the distribution\n",
    "                [2]    value: Value from forwarding the network.\n",
    "                [3] log_prob: The log probability of the action distruction\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy( state ).float( )\n",
    "\n",
    "        # Forwarding the network.\n",
    "        prob_dist, value = self.forward( state )\n",
    "\n",
    "        # The action to take (left or right)\n",
    "        action = np.random.choice( self.n_action, p = np.squeeze( prob_dist.detach( ).numpy( ) ) )\n",
    "\n",
    "        # Log-probability\n",
    "        log_prob = torch.log( prob_dist.squeeze( 0 )[ action ] )\n",
    "\n",
    "        return action, value.squeeze( 0 ), log_prob\n",
    "\n",
    "    def update_network( self, rewards, values, log_probs ):\n",
    "        \"\"\"\n",
    "            Training code. Calculates actor and critic loss and performs backprop.\n",
    "\n",
    "            Args:\n",
    "                -   rewards: Given the trajectory, a list of rewards \n",
    "                -    values: Given the trajectory, there is a list of states, and for each state there is an associated value function\n",
    "                - log_probs: Given the trajectory, we have a list of p(a|s). \n",
    "        \"\"\"\n",
    "        \n",
    "        Gt_arr    = torch.tensor( Gt_calc( rewards, gamma = 0.9, is_normalize = True ) ) \n",
    "        values    = torch.stack( values )\n",
    "        log_probs = torch.stack( log_probs )\n",
    "\n",
    "\n",
    "        adv_arr   = Gt_arr - values\n",
    "\n",
    "        policy_losses = -torch.dot( log_probs , adv_arr.float( ) )\n",
    "        value_losses  = F.smooth_l1_loss( values, Gt_arr )\n",
    "\n",
    "        # reset gradients\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # sum up all the values of policy_losses and value_losses\n",
    "        loss = policy_losses + value_losses \n",
    "\n",
    "        # perform backprop\n",
    "        loss.backward( retain_graph=True )\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Advantage Actor-Critic (A2C) Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, total reward: 20.0, average_reward: 0.0, length: 19\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb#ch0000005?line=40'>41</a>\u001b[0m log_probs\u001b[39m.\u001b[39mappend( log_prob )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb#ch0000005?line=42'>43</a>\u001b[0m \u001b[39mif\u001b[39;00m done: \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb#ch0000005?line=44'>45</a>\u001b[0m     a2c_nn\u001b[39m.\u001b[39;49mupdate_network( rewards, values, log_probs )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb#ch0000005?line=45'>46</a>\u001b[0m     sum_rewards \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m( rewards )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb#ch0000005?line=47'>48</a>\u001b[0m     num_steps[   i ] \u001b[39m=\u001b[39m t\n",
      "\u001b[1;32m/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb Cell 4'\u001b[0m in \u001b[0;36mActorCritic.update_network\u001b[0;34m(self, rewards, values, log_probs)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb#ch0000003?line=121'>122</a>\u001b[0m loss \u001b[39m=\u001b[39m policy_losses \u001b[39m+\u001b[39m value_losses \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb#ch0000003?line=123'>124</a>\u001b[0m \u001b[39m# perform backprop\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb#ch0000003?line=124'>125</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward( retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m )\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/machine-learning-tutorial/notebooks/A2C.ipynb#ch0000003?line=125'>126</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/mosesnah/Documents/projects/machine-learning-tutorial/.venv/lib/python3.8/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [128, 1]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "# Generate the gym of Cart-and-Pole\n",
    "env = gym.make( 'CartPole-v1' ) \n",
    "\n",
    "# The number of states and actions are +4 and +2\n",
    "ns  = env.observation_space.shape[ 0 ]\n",
    "na  = env.action_space.n\n",
    "\n",
    "# Add the Adam optimizer\n",
    "a2c_nn    = ActorCritic( ns, na, 128 )\n",
    "eps       = np.finfo( np.float32 ).eps\n",
    "\n",
    "# The number of episodes\n",
    "N_eps = 1000 \n",
    "\n",
    "num_steps   = np.zeros( N_eps )\n",
    "all_rewards = np.zeros( N_eps )\n",
    "\n",
    "for i in range( N_eps ):\n",
    "\n",
    "    # reset environment and episode reward\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    # for each episode, only run 9999 steps so that we don't \n",
    "    rewards   = []\n",
    "    log_probs = [] \n",
    "    values    = []\n",
    "\n",
    "    for t in range( 1000 ):\n",
    "\n",
    "        # select action from policy\n",
    "        if is_render: env.render( )\n",
    "\n",
    "        action, value, log_prob  = a2c_nn.get_action_and_values( state )\n",
    "\n",
    "        # Run one step\n",
    "        new_state, reward, done, _ = env.step( action )\n",
    "\n",
    "        rewards.append( reward )\n",
    "        values.append( value )\n",
    "        log_probs.append( log_prob )\n",
    "\n",
    "        if done: \n",
    "\n",
    "            a2c_nn.update_network( rewards, values, log_probs )\n",
    "            sum_rewards = sum( rewards )\n",
    "\n",
    "            num_steps[   i ] = t\n",
    "            all_rewards[ i ] = sum_rewards\n",
    "            \n",
    "            sys.stdout.write( \"episode: {}, total reward: {}, average_reward: {}, length: {}\\n\".format( i , np.round( sum_rewards, decimals = 3 ),  np.round( np.mean( all_rewards[ -10 : ] ), decimals = 3 ), steps ) )\n",
    "\n",
    "\n",
    "        state = new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "[1] [Great post](https://danieltakeshi.github.io/2018/06/28/a2c-a3c/)\n",
    "[2] [Google Co-lab example](https://colab.research.google.com/github/yfletberliac/rlss-2019/blob/master/labs/DRL.01.REINFORCE%2BA2C.ipynb#scrollTo=xDifFS9I4X7A)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "742fb12a3d29bb1ab201ca06dfc1cb9996440041728c355377b69362e9fdf7ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
