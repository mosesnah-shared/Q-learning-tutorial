{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Method\n",
    "\n",
    "This code is an example using Advantage Actor-Critic Method (a.k.a., A2C). The code is modified from [this one](https://github.com/pytorch/examples/blob/main/reinforcement_learning/actor_critic.py). [This website](https://medium.com/towards-data-science/understanding-actor-critic-methods-931b97b6df3f) might also be useful as a future reference, although the code can be outdated. \n",
    "\n",
    "There are multiple Actor-Critic Methods, but here we focus on the most method A2C method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "import torch\n",
    "import torch.nn            as nn\n",
    "import torch.optim         as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot   as plt\n",
    "import moviepy.editor      as mpy\n",
    "\n",
    "\n",
    "gamma = 0.90 \n",
    "eps   = np.finfo( np.float32 ).eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Neural Network.\n",
    "\n",
    "We merged the two networks in a single class. \n",
    "Since we have merged the two Actor and Critic networks, they are updated in synchrony, and they also share the same state input.\n",
    "A method which asynchronously updates the two networks is called the **Asynchronous Advantage Actor-Critic Method** (a.k.a. A3C), where the details are in [this website](https://medium.com/@shagunm1210/implementing-the-a3c-algorithm-to-train-an-agent-to-play-breakout-c0b5ce3b3405).\n",
    "\n",
    "### Actor Network\n",
    "The actor network maps state to action, hence the input is the number of states of the example, $n_s$ and the output is the number of actions $n_a$.\n",
    "\n",
    "### Critic Network\n",
    "The critic network maps state to a scalar real value, which is known to be the Value function $V^{\\pi}(s)$.\n",
    "\n",
    "### Advantage Value\n",
    "For the REINFORCE algorithm, we used the following gradient value to optimize the policy:\n",
    "$$\n",
    "    \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\sum_{i=0}^{T-1} \\nabla_{\\theta} \\log{\\pi_{\\theta}(a_i|s_i)} G_t \\Bigg]\n",
    "$$\n",
    "For the A2C algorithm, rather than using $G_t$ we use the **Advantage Value** $A_t$:\n",
    "$$\n",
    "    \\nabla_{\\theta} J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta} }\\Bigg[ \\sum_{i=0}^{T-1} \\nabla_{\\theta} \\log{\\pi_{\\theta}(a_i|s_i)} A_t \\Bigg]\n",
    "$$\n",
    "where $A_t$ is defined by:\n",
    "$$\n",
    "    A_t = G_t - V(s_t)\n",
    "$$\n",
    "The $V(s_t)$ value is generated from the Critic network, and the policy $\\pi_{\\theta}$ is parameterized as an Actor Network.\n",
    "The reason why we subtract $V(s_t)$ from the $G_t$ value is to reduce the variance in the gradient value, hence to \"stabilize\" the algorithm. $V(s)$, the state-value function is solely a function of state $s$, and this makes the algorithm \"unbiased\" [[REF1]](https://medium.com/intro-to-artificial-intelligence/the-actor-critic-reinforcement-learning-algorithm-c8095a655c14) [[REF2]](https://danieltakeshi.github.io/2017/03/28/going-deeper-into-reinforcement-learning-fundamentals-of-policy-gradients/). \n",
    "We intentionally omit the details here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gt_calc( r_arr: np.ndarray, gamma: float = .99, is_normalize: bool = True ) :\n",
    "    \"\"\"\n",
    "        This method uses vectors for the Gt array calculation.\n",
    "\n",
    "        Args:\n",
    "            [1] r_arr: an array of rewards for each time step. \n",
    "\n",
    "            [2] gamma: discount ratio, valued between 0 to 1. \n",
    "                       If gamma = 1, then there is no discount applied\n",
    "\n",
    "            [3] is_print: a boolean value which determines whether to print out the computation time or not.\n",
    "\n",
    "        Return:\n",
    "            G_t: an array of discounted (or if gamma = 1, simple sum of) rewards\n",
    "\n",
    "        ======================================================\n",
    "        ================ Detailed Explanation ================\n",
    "        ======================================================\n",
    "\n",
    "        Our Goal is to generate Gt_arr as follows:\n",
    "        Gt_arr = [ R1 + r x R2 + r^2 x R3 + ... + r^(N-1) x RN, R2 + r x R3 + ... + r^(N-2) x RN, ..., RN  ]\n",
    "        For this, we have to run the following operation. \n",
    "        [1] Generate the following N x N matrix called tmp1\n",
    "            |  1  r  r^2  r^3  ...  r^(N-1)   |\n",
    "            |  0  1  r    r^2  ...  r^(N-2)   |\n",
    "            |  0  0  1    r    ...  r^(N-3)   |\n",
    "            |            ...                  |\n",
    "            |  0  0  0                    1   |                \n",
    "        [REF] https://stackoverflow.com/questions/28705834/fastest-way-to-compute-upper-triangular-matrix-of-geometric-series-python\n",
    "        \n",
    "        [2] Generate the following N x N matrix called tmp2\n",
    "            |  R1  R2  R3  R4  ...  RN   |\n",
    "            |  R1  R2  R3  R4  ...  RN   |\n",
    "            |  R1  R2  R3  R4  ...  RN   |\n",
    "            |                  ...       |\n",
    "            |  R1  R2  R3  R4  ...  RN   |\n",
    "        \n",
    "        [3] Conduct a dot product (Hadamard Operator) between tmp1 and tmp2\n",
    "            |  R1  r x R2  r^2 x R3  ...  r^(N-1) x RN   |\n",
    "            |   0      R2  r^1 x R3  ...  r^(N-2) x RN   |\n",
    "            |   0       0        R3  ...  r^(N-3) x RN   |\n",
    "            |                        ...                 |\n",
    "            |   0       0         0  ...           RN    |\n",
    "        [4] Take the sum along axis 01\n",
    "            |  R1 + r x R2 + r^2 x R3 + ... + r^(N-1) x RN  |\n",
    "            |           R2 + r^1 x R3 + ... + r^(N-2) x RN  |\n",
    "            |                      R3 + ... + r^(N-3) x RN  |\n",
    "            |                        ...                    |\n",
    "            |                                           RN  |\n",
    "\n",
    "        ======================================================\n",
    "        ======================================================\n",
    "        ======================================================\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    N = len( r_arr )\n",
    "\n",
    "    # [1] Generate the following N x N matrix called tmp1\n",
    "    tmp1 = scipy.linalg.toeplitz( gamma ** np.arange( N ), np.zeros( N ) ).T\n",
    "\n",
    "    # [2] Generate the following N x N matrix called tmp2\n",
    "    tmp2 = np.tile( r_arr ,( N,1 ) ) \n",
    "    \n",
    "    # [3] Conduct a dot product (Hadamard Operator) between tmp1 and tmp2\n",
    "    # [4] Take the sum along axis 01\n",
    "    Gt_arr = np.sum( tmp1 * tmp2, axis = 1 )\n",
    "\n",
    "    # [-] Normalization of the Gt_arr, which is known to be important. \n",
    "    if is_normalize: Gt_arr = ( Gt_arr - Gt_arr.mean( ) ) / ( Gt_arr.std( ) + eps )\n",
    "\n",
    "    return Gt_arr\n",
    "\n",
    "class ActorCritic( nn.Module ):\n",
    "    \"\"\"\n",
    "        Generate the actor and critic networks in a single class\n",
    "    \"\"\"\n",
    "    def __init__( self, num_input, num_output, num_hidden  ):\n",
    "\n",
    "        super( ActorCritic, self ).__init__(  )\n",
    "\n",
    "        # Saving this for choosing the action.\n",
    "        self.n_action = num_output\n",
    "\n",
    "        # First layer : from input to hidden variable\n",
    "        self.affine1 = nn.Linear( num_input, num_hidden )\n",
    "\n",
    "        # Second Actor's layer\n",
    "        self.actor_layer = nn.Linear( num_hidden, num_output )\n",
    "\n",
    "        # Second Critic's layer\n",
    "        self.critic_layer = nn.Linear( num_hidden, 1 )\n",
    "\n",
    "        # Optimizer \n",
    "        self.optimizer = optim.Adam( self.parameters( ), lr = 0.003 )\n",
    "\n",
    "    def forward( self, x ):\n",
    "        \"\"\"\n",
    "            Forward of both actor and critic.\n",
    "\n",
    "            Args:\n",
    "                - Current State Vector\n",
    "\n",
    "            Outputs:\n",
    "                - [1] action_prob: The pi( . | s ) itself\n",
    "                - [2] state_value: The value function V(s)\n",
    "        \"\"\"\n",
    "\n",
    "        # Forward the 1st layer\n",
    "        x = F.relu( self.affine1( x ) )\n",
    "\n",
    "        # Actor: Chooses action to take from state s_t \n",
    "        action_prob = F.softmax( self.actor_layer( x ), dim = -1 )\n",
    "\n",
    "        # Critic: Evaluates the value function at state s_t\n",
    "        state_value = self.critic_layer( x )\n",
    "\n",
    "        # return values for both actor and critic as a tuple of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t \n",
    "\n",
    "        return action_prob, state_value.squeeze( )\n",
    "\n",
    "    def get_action_and_values( self, state: np.ndarray ):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                [1] State vector\n",
    "\n",
    "            Outputs:\n",
    "                [1]   action: The action chosen from the distribution\n",
    "                [2]    value: Value from forwarding the network.\n",
    "                [3] log_prob: The log probability of the action distruction\n",
    "                \n",
    "        \"\"\"\n",
    "        state = torch.from_numpy( state ).float( )\n",
    "\n",
    "        # Forwarding the network.\n",
    "        prob_dist, value = self.forward( state )\n",
    "\n",
    "        # The action to take (left or right)\n",
    "        action = np.random.choice( self.n_action, p = prob_dist.detach( ).numpy( ) )\n",
    "\n",
    "        # Log-probability\n",
    "        log_prob = torch.log( prob_dist[ action ] )\n",
    "\n",
    "        return action, value, log_prob\n",
    "\n",
    "    def update_network( self, rewards, values, log_probs ):\n",
    "        \"\"\"\n",
    "            Training code. Calculates actor and critic loss and performs backprop.\n",
    "\n",
    "            Args:\n",
    "                -   rewards: Given the trajectory, a list of rewards \n",
    "                -    values: Given the trajectory, there is a list of states, and for each state there is an associated value function\n",
    "                - log_probs: Given the trajectory, we have a list of p(a|s). \n",
    "        \"\"\"\n",
    "        \n",
    "        Gt_arr    = torch.tensor( Gt_calc( rewards, gamma, is_normalize = True ) ) \n",
    "        \n",
    "        values    = torch.stack(    values, dim = 0  )\n",
    "        log_probs = torch.stack( log_probs, dim = 0  )\n",
    "\n",
    "        adv_arr   = torch.sub( Gt_arr, values )\n",
    "\n",
    "        policy_losses = -torch.dot( log_probs, adv_arr.float( ) )\n",
    "        value_losses  = F.smooth_l1_loss( values, Gt_arr, reduction = 'sum' )\n",
    "\n",
    "        # reset gradients\n",
    "        self.optimizer.zero_grad( )\n",
    "\n",
    "        # sum up all the values of policy_losses and value_losses\n",
    "        loss = policy_losses + value_losses \n",
    "\n",
    "        # perform backprop\n",
    "        loss.backward( )\n",
    "        self.optimizer.step()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Advantage Actor-Critic (A2C) Method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the gym of Cart-and-Pole\n",
    "env = gym.make( 'CartPole-v1' ) \n",
    "\n",
    "# The number of states and actions are +4 and +2\n",
    "ns  = env.observation_space.shape[ 0 ]\n",
    "na  = env.action_space.n\n",
    "\n",
    "# Add the Adam optimizer\n",
    "a2c_nn    = ActorCritic( ns, na, 128 )\n",
    "is_render = False\n",
    "\n",
    "# The number of episodes\n",
    "N_eps = 2000\n",
    "\n",
    "num_steps     = [ ]\n",
    "avg_numsteps  = [ ]\n",
    "all_rewards   = [ ]\n",
    "frames        = [ ]\n",
    "\n",
    "is_save_video = False\n",
    "is_save_model = False    \n",
    "\n",
    "for i in range( N_eps ):\n",
    "\n",
    "    # reset environment and episode reward\n",
    "    state = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    # for each episode, only run 9999 steps so that we don't \n",
    "    rewards   = []\n",
    "    log_probs = [] \n",
    "    values    = []\n",
    "\n",
    "\n",
    "    for t in range( 1000 ):\n",
    "\n",
    "        # select action from policy\n",
    "        if is_save_video : frames.append( env.render( mode = 'rgb_array' ) )\n",
    "\n",
    "\n",
    "        action, value, log_prob  = a2c_nn.get_action_and_values( state )\n",
    "\n",
    "        # Run one step\n",
    "        new_state, reward, done, _ = env.step( action )\n",
    "\n",
    "        rewards.append( reward )\n",
    "        values.append( value )\n",
    "        log_probs.append( log_prob )\n",
    "\n",
    "        if done: \n",
    "\n",
    "            a2c_nn.update_network( rewards, values, log_probs )\n",
    "            sum_rewards = sum( rewards )\n",
    "\n",
    "            num_steps.append( t )\n",
    "            avg_numsteps.append( np.mean( num_steps[ -10: ] ) )\n",
    "\n",
    "            all_rewards.append( sum_rewards )\n",
    "            \n",
    "            sys.stdout.write( \"episode: {}, total reward: {}, average_reward: {}, length: {}\\n\".format( i , np.round( sum_rewards, decimals = 3 ),  np.round( np.mean( all_rewards[ -10 : ] ), decimals = 3 ), t ) )\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    # check if we have \"solved\" the cart pole problem\n",
    "    if np.mean( all_rewards[ -10 : ] ) >= env.spec.reward_threshold: break\n",
    "\n",
    "if is_save_video:\n",
    "    clip = mpy.ImageSequenceClip( frames, fps = 30 )\n",
    "    clip.write_gif( \"../videos/A2C.gif\" )        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close( )\n",
    "plt.plot( num_steps )\n",
    "plt.plot( avg_numsteps )\n",
    "plt.xlabel('Episode')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References \n",
    "\n",
    "[1] [Great post](https://danieltakeshi.github.io/2018/06/28/a2c-a3c/)\n",
    "[2] [Google Co-lab example](https://colab.research.google.com/github/yfletberliac/rlss-2019/blob/master/labs/DRL.01.REINFORCE%2BA2C.ipynb#scrollTo=xDifFS9I4X7A)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "742fb12a3d29bb1ab201ca06dfc1cb9996440041728c355377b69362e9fdf7ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
