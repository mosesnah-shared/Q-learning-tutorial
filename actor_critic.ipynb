{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Method\n",
    "\n",
    "This code is an example using Actor-Critic Method [This website](https://medium.com/towards-data-science/understanding-actor-critic-methods-931b97b6df3f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch  \n",
    "import gym\n",
    "import numpy as np  \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# hyperparameters\n",
    "hidden_size = 256\n",
    "learning_rate = 3e-4\n",
    "\n",
    "# Constants\n",
    "GAMMA = 0.99\n",
    "num_steps = 300\n",
    "max_episodes = 3000\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, learning_rate=3e-4):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.critic_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.critic_linear2 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.actor_linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "        self.actor_linear2 = nn.Linear(hidden_size, num_actions)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n",
    "        value = F.relu(self.critic_linear1(state))\n",
    "        value = self.critic_linear2(value)\n",
    "        \n",
    "        policy_dist = F.relu(self.actor_linear1(state))\n",
    "        policy_dist = F.softmax(self.actor_linear2(policy_dist), dim=1)\n",
    "\n",
    "        return value, policy_dist\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def a2c(env):\n",
    "    num_inputs = env.observation_space.shape[0]\n",
    "    num_outputs = env.action_space.n\n",
    "    \n",
    "    actor_critic = ActorCritic(num_inputs, num_outputs, hidden_size)\n",
    "    ac_optimizer = optim.Adam(actor_critic.parameters(), lr=learning_rate)\n",
    "\n",
    "    all_lengths = []\n",
    "    average_lengths = []\n",
    "    all_rewards = []\n",
    "    entropy_term = 0\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "\n",
    "        state = env.reset()\n",
    "        for steps in range(num_steps):\n",
    "            value, policy_dist = actor_critic.forward(state)\n",
    "            value = value.detach().numpy()[0,0]\n",
    "            dist = policy_dist.detach().numpy() \n",
    "\n",
    "            action = np.random.choice(num_outputs, p=np.squeeze(dist))\n",
    "            log_prob = torch.log(policy_dist.squeeze(0)[action])\n",
    "            entropy = -np.sum(np.mean(dist) * np.log(dist))\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            rewards.append(reward)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            entropy_term += entropy\n",
    "            state = new_state\n",
    "            \n",
    "            if done or steps == num_steps-1:\n",
    "                Qval, _ = actor_critic.forward(new_state)\n",
    "                Qval = Qval.detach().numpy()[0,0]\n",
    "                all_rewards.append(np.sum(rewards))\n",
    "                all_lengths.append(steps)\n",
    "                average_lengths.append(np.mean(all_lengths[-10:]))\n",
    "                if episode % 10 == 0:                    \n",
    "                    sys.stdout.write(\"episode: {}, reward: {}, total length: {}, average length: {} \\n\".format(episode, np.sum(rewards), steps, average_lengths[-1]))\n",
    "                break\n",
    "        \n",
    "        # compute Q values\n",
    "        Qvals = np.zeros_like(values)\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            Qval = rewards[t] + GAMMA * Qval\n",
    "            Qvals[t] = Qval\n",
    "  \n",
    "        #update actor critic\n",
    "        values = torch.FloatTensor(values)\n",
    "        Qvals = torch.FloatTensor(Qvals)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        advantage = Qvals - values\n",
    "        actor_loss = (-log_probs * advantage).mean()\n",
    "        critic_loss = 0.5 * advantage.pow(2).mean()\n",
    "        ac_loss = actor_loss + critic_loss + 0.001 * entropy_term\n",
    "\n",
    "        ac_optimizer.zero_grad()\n",
    "        ac_loss.backward()\n",
    "        ac_optimizer.step()\n",
    "\n",
    "        \n",
    "    \n",
    "    # Plot results\n",
    "    smoothed_rewards = pd.Series.rolling(pd.Series(all_rewards), 10).mean()\n",
    "    smoothed_rewards = [elem for elem in smoothed_rewards]\n",
    "    plt.plot(all_rewards)\n",
    "    plt.plot(smoothend_rewards)\n",
    "    plt.plot()\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(all_lengths)\n",
    "    plt.plot(average_lengths)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode length')\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, reward: 19.0, total length: 18, average length: 18.0 \n",
      "episode: 10, reward: 13.0, total length: 12, average length: 25.8 \n",
      "episode: 20, reward: 32.0, total length: 31, average length: 27.6 \n",
      "episode: 30, reward: 32.0, total length: 31, average length: 36.1 \n",
      "episode: 40, reward: 10.0, total length: 9, average length: 22.5 \n",
      "episode: 50, reward: 29.0, total length: 28, average length: 31.0 \n",
      "episode: 60, reward: 24.0, total length: 23, average length: 27.4 \n",
      "episode: 70, reward: 26.0, total length: 25, average length: 23.9 \n",
      "episode: 80, reward: 47.0, total length: 46, average length: 32.6 \n",
      "episode: 90, reward: 17.0, total length: 16, average length: 23.1 \n",
      "episode: 100, reward: 10.0, total length: 9, average length: 20.4 \n",
      "episode: 110, reward: 38.0, total length: 37, average length: 30.9 \n",
      "episode: 120, reward: 40.0, total length: 39, average length: 27.6 \n",
      "episode: 130, reward: 42.0, total length: 41, average length: 21.5 \n",
      "episode: 140, reward: 45.0, total length: 44, average length: 20.2 \n",
      "episode: 150, reward: 34.0, total length: 33, average length: 26.0 \n",
      "episode: 160, reward: 11.0, total length: 10, average length: 21.3 \n",
      "episode: 170, reward: 27.0, total length: 26, average length: 18.1 \n",
      "episode: 180, reward: 18.0, total length: 17, average length: 18.8 \n",
      "episode: 190, reward: 76.0, total length: 75, average length: 28.3 \n",
      "episode: 200, reward: 81.0, total length: 80, average length: 26.6 \n",
      "episode: 210, reward: 17.0, total length: 16, average length: 28.3 \n",
      "episode: 220, reward: 19.0, total length: 18, average length: 27.0 \n",
      "episode: 230, reward: 43.0, total length: 42, average length: 32.1 \n",
      "episode: 240, reward: 25.0, total length: 24, average length: 40.2 \n",
      "episode: 250, reward: 32.0, total length: 31, average length: 33.9 \n",
      "episode: 260, reward: 80.0, total length: 79, average length: 34.9 \n",
      "episode: 270, reward: 38.0, total length: 37, average length: 35.1 \n",
      "episode: 280, reward: 29.0, total length: 28, average length: 43.7 \n",
      "episode: 290, reward: 23.0, total length: 22, average length: 36.2 \n",
      "episode: 300, reward: 28.0, total length: 27, average length: 27.3 \n",
      "episode: 310, reward: 54.0, total length: 53, average length: 24.8 \n",
      "episode: 320, reward: 33.0, total length: 32, average length: 41.3 \n",
      "episode: 330, reward: 58.0, total length: 57, average length: 40.6 \n",
      "episode: 340, reward: 57.0, total length: 56, average length: 32.6 \n",
      "episode: 350, reward: 79.0, total length: 78, average length: 41.6 \n",
      "episode: 360, reward: 18.0, total length: 17, average length: 43.8 \n",
      "episode: 370, reward: 35.0, total length: 34, average length: 29.1 \n",
      "episode: 380, reward: 16.0, total length: 15, average length: 26.3 \n",
      "episode: 390, reward: 19.0, total length: 18, average length: 31.9 \n",
      "episode: 400, reward: 88.0, total length: 87, average length: 33.4 \n",
      "episode: 410, reward: 26.0, total length: 25, average length: 21.2 \n",
      "episode: 420, reward: 34.0, total length: 33, average length: 26.2 \n",
      "episode: 430, reward: 77.0, total length: 76, average length: 42.0 \n",
      "episode: 440, reward: 26.0, total length: 25, average length: 38.7 \n",
      "episode: 450, reward: 69.0, total length: 68, average length: 49.3 \n",
      "episode: 460, reward: 49.0, total length: 48, average length: 41.3 \n",
      "episode: 470, reward: 20.0, total length: 19, average length: 41.3 \n",
      "episode: 480, reward: 67.0, total length: 66, average length: 51.5 \n",
      "episode: 490, reward: 50.0, total length: 49, average length: 45.8 \n",
      "episode: 500, reward: 28.0, total length: 27, average length: 44.0 \n",
      "episode: 510, reward: 73.0, total length: 72, average length: 43.6 \n",
      "episode: 520, reward: 66.0, total length: 65, average length: 67.5 \n",
      "episode: 530, reward: 23.0, total length: 22, average length: 43.5 \n",
      "episode: 540, reward: 24.0, total length: 23, average length: 41.6 \n",
      "episode: 550, reward: 30.0, total length: 29, average length: 41.8 \n",
      "episode: 560, reward: 55.0, total length: 54, average length: 35.6 \n",
      "episode: 570, reward: 14.0, total length: 13, average length: 51.1 \n",
      "episode: 580, reward: 70.0, total length: 69, average length: 44.6 \n",
      "episode: 590, reward: 28.0, total length: 27, average length: 57.0 \n",
      "episode: 600, reward: 29.0, total length: 28, average length: 49.2 \n",
      "episode: 610, reward: 48.0, total length: 47, average length: 43.8 \n",
      "episode: 620, reward: 55.0, total length: 54, average length: 42.7 \n",
      "episode: 630, reward: 49.0, total length: 48, average length: 44.6 \n",
      "episode: 640, reward: 79.0, total length: 78, average length: 52.1 \n",
      "episode: 650, reward: 30.0, total length: 29, average length: 52.0 \n",
      "episode: 660, reward: 37.0, total length: 36, average length: 68.9 \n",
      "episode: 670, reward: 20.0, total length: 19, average length: 43.8 \n",
      "episode: 680, reward: 19.0, total length: 18, average length: 53.1 \n",
      "episode: 690, reward: 124.0, total length: 123, average length: 66.3 \n",
      "episode: 700, reward: 22.0, total length: 21, average length: 48.4 \n",
      "episode: 710, reward: 39.0, total length: 38, average length: 58.8 \n",
      "episode: 720, reward: 63.0, total length: 62, average length: 60.5 \n",
      "episode: 730, reward: 25.0, total length: 24, average length: 62.6 \n",
      "episode: 740, reward: 49.0, total length: 48, average length: 67.8 \n",
      "episode: 750, reward: 33.0, total length: 32, average length: 41.8 \n",
      "episode: 760, reward: 23.0, total length: 22, average length: 71.9 \n",
      "episode: 770, reward: 134.0, total length: 133, average length: 72.3 \n",
      "episode: 780, reward: 77.0, total length: 76, average length: 89.1 \n",
      "episode: 790, reward: 52.0, total length: 51, average length: 68.2 \n",
      "episode: 800, reward: 42.0, total length: 41, average length: 50.3 \n",
      "episode: 810, reward: 18.0, total length: 17, average length: 70.9 \n",
      "episode: 820, reward: 66.0, total length: 65, average length: 53.7 \n",
      "episode: 830, reward: 82.0, total length: 81, average length: 70.1 \n",
      "episode: 840, reward: 68.0, total length: 67, average length: 67.3 \n",
      "episode: 850, reward: 61.0, total length: 60, average length: 81.5 \n",
      "episode: 860, reward: 75.0, total length: 74, average length: 99.6 \n",
      "episode: 870, reward: 161.0, total length: 160, average length: 92.5 \n",
      "episode: 880, reward: 63.0, total length: 62, average length: 85.9 \n",
      "episode: 890, reward: 57.0, total length: 56, average length: 103.7 \n",
      "episode: 900, reward: 114.0, total length: 113, average length: 129.5 \n",
      "episode: 910, reward: 88.0, total length: 87, average length: 57.2 \n",
      "episode: 920, reward: 38.0, total length: 37, average length: 95.8 \n",
      "episode: 930, reward: 178.0, total length: 177, average length: 100.8 \n",
      "episode: 940, reward: 42.0, total length: 41, average length: 75.9 \n",
      "episode: 950, reward: 298.0, total length: 297, average length: 129.5 \n",
      "episode: 960, reward: 148.0, total length: 147, average length: 93.8 \n",
      "episode: 970, reward: 108.0, total length: 107, average length: 95.9 \n",
      "episode: 980, reward: 46.0, total length: 45, average length: 107.4 \n",
      "episode: 990, reward: 144.0, total length: 143, average length: 124.3 \n",
      "episode: 1000, reward: 108.0, total length: 107, average length: 109.8 \n",
      "episode: 1010, reward: 73.0, total length: 72, average length: 106.0 \n",
      "episode: 1020, reward: 147.0, total length: 146, average length: 149.8 \n",
      "episode: 1030, reward: 76.0, total length: 75, average length: 110.4 \n",
      "episode: 1040, reward: 108.0, total length: 107, average length: 119.1 \n",
      "episode: 1050, reward: 103.0, total length: 102, average length: 157.4 \n",
      "episode: 1060, reward: 229.0, total length: 228, average length: 152.2 \n",
      "episode: 1070, reward: 136.0, total length: 135, average length: 136.3 \n",
      "episode: 1080, reward: 138.0, total length: 137, average length: 136.6 \n",
      "episode: 1090, reward: 80.0, total length: 79, average length: 133.9 \n",
      "episode: 1100, reward: 97.0, total length: 96, average length: 118.1 \n",
      "episode: 1110, reward: 53.0, total length: 52, average length: 142.2 \n",
      "episode: 1120, reward: 119.0, total length: 118, average length: 129.2 \n",
      "episode: 1130, reward: 137.0, total length: 136, average length: 115.2 \n",
      "episode: 1140, reward: 43.0, total length: 42, average length: 138.5 \n",
      "episode: 1150, reward: 277.0, total length: 276, average length: 145.1 \n",
      "episode: 1160, reward: 199.0, total length: 198, average length: 172.3 \n",
      "episode: 1170, reward: 128.0, total length: 127, average length: 123.8 \n",
      "episode: 1180, reward: 192.0, total length: 191, average length: 137.1 \n",
      "episode: 1190, reward: 222.0, total length: 221, average length: 175.5 \n",
      "episode: 1200, reward: 61.0, total length: 60, average length: 123.2 \n",
      "episode: 1210, reward: 138.0, total length: 137, average length: 132.2 \n",
      "episode: 1220, reward: 199.0, total length: 198, average length: 166.0 \n",
      "episode: 1230, reward: 110.0, total length: 109, average length: 128.5 \n",
      "episode: 1240, reward: 148.0, total length: 147, average length: 151.6 \n",
      "episode: 1250, reward: 119.0, total length: 118, average length: 118.6 \n",
      "episode: 1260, reward: 296.0, total length: 295, average length: 173.3 \n",
      "episode: 1270, reward: 282.0, total length: 281, average length: 219.8 \n",
      "episode: 1280, reward: 128.0, total length: 127, average length: 171.2 \n",
      "episode: 1290, reward: 300.0, total length: 299, average length: 179.6 \n",
      "episode: 1300, reward: 300.0, total length: 299, average length: 201.1 \n",
      "episode: 1310, reward: 149.0, total length: 148, average length: 185.3 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000003?line=0'>1</a>\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mmake(\u001b[39m\"\u001b[39m\u001b[39mCartPole-v1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000003?line=1'>2</a>\u001b[0m a2c(env)\n",
      "\u001b[1;32m/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb Cell 5'\u001b[0m in \u001b[0;36ma2c\u001b[0;34m(env)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000005?line=17'>18</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000005?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m steps \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_steps):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000005?line=19'>20</a>\u001b[0m     value, policy_dist \u001b[39m=\u001b[39m actor_critic\u001b[39m.\u001b[39;49mforward(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000005?line=20'>21</a>\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000005?line=21'>22</a>\u001b[0m     dist \u001b[39m=\u001b[39m policy_dist\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy() \n",
      "\u001b[1;32m/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb Cell 4'\u001b[0m in \u001b[0;36mActorCritic.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000004?line=22'>23</a>\u001b[0m value \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_linear1(state))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000004?line=23'>24</a>\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_linear2(value)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000004?line=25'>26</a>\u001b[0m policy_dist \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor_linear1(state))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000004?line=26'>27</a>\u001b[0m policy_dist \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactor_linear2(policy_dist), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/mosesnah/Documents/projects/Q-learning-tutorial/actor_critic.ipynb#ch0000004?line=28'>29</a>\u001b[0m \u001b[39mreturn\u001b[39;00m value, policy_dist\n",
      "File \u001b[0;32m~/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/torch/nn/functional.py:1442\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/torch/nn/functional.py?line=1439'>1440</a>\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/torch/nn/functional.py?line=1440'>1441</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/torch/nn/functional.py?line=1441'>1442</a>\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39m)\n\u001b[1;32m   <a href='file:///Users/mosesnah/Documents/projects/Q-learning-tutorial/.venv/lib/python3.8/site-packages/torch/nn/functional.py?line=1442'>1443</a>\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "a2c(env)    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fd23c35c013382817769049cdac90792e6589b2481e2321ea0fa01482e4dfef6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
